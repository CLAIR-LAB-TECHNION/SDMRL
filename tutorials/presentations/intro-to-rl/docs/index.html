<!DOCTYPE html>
<html lang="en"><head>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/tabby.min.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-29e2c20b02301cfff04dc8050bf30c7e.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.37">

  <meta name="author" content="Itay Segev">
  <title>Introduction to Reinforcement Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="site_libs/revealjs/dist/theme/quarto-bbe7401fe57d4b791b917637bb662036.css">
  <link rel="stylesheet" href="style.css">
  <link href="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Introduction to Reinforcement Learning</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Itay Segev 
</div>
</div>
</div>

</section>
<section id="key-concepts" class="slide level2">
<h2>Key Concepts</h2>

<img data-src="https://spinningup.openai.com/en/latest/_images/rl_diagram_transparent_bg.png" class="r-stretch quarto-figure-center"><p class="caption">Agent-environment interaction loop</p></section>
<section id="rl-process-example" class="slide level2">
<h2>RL Process Example</h2>

<img data-src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg" class="r-stretch"></section>
<section id="rl-process---code-example" class="slide level2">
<h2>RL Process - code example</h2>
<div class="sourceCode" id="cb1" data-code-line-numbers="2-3|6|7|9-10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a><span class="im">import</span> gymnasium <span class="im">as</span> gym</span>
<span id="cb1-2"><a></a>env <span class="op">=</span> gym.make(<span class="st">"CartPole-v1"</span>)</span>
<span id="cb1-3"><a></a>observation, info <span class="op">=</span> env.reset()</span>
<span id="cb1-4"><a></a></span>
<span id="cb1-5"><a></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb1-6"><a></a>    action <span class="op">=</span> env.action_space.sample()  <span class="co"># agent policy that uses the observation and info</span></span>
<span id="cb1-7"><a></a>    observation, reward, terminated, truncated, info <span class="op">=</span> env.step(action)</span>
<span id="cb1-8"><a></a></span>
<span id="cb1-9"><a></a>    <span class="cf">if</span> terminated <span class="kw">or</span> truncated:</span>
<span id="cb1-10"><a></a>        observation, info <span class="op">=</span> env.reset()</span>
<span id="cb1-11"><a></a></span>
<span id="cb1-12"><a></a>env.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="observationsstates-space" class="slide level2">
<h2>Observations/States Space</h2>
<div>
<ul>
<li class="fragment">State <span class="math inline">\(s\)</span>: is a complete description of the state of the world (there is no hidden information). In a fully observed environment.</li>
<li class="fragment">Observation <span class="math inline">\(o\)</span>: is a partial description of the state. In a partially observed environment.</li>
</ul>
</div>
</section>
<section id="observationsstates-space-1" class="slide level2">
<h2>Observations/States Space</h2>
<p>There is a differentiation to make between observation and state:</p>
<p><img data-src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/chess.jpg" class="absolute" style="top: 200px; left: 30px; width: 450px; "></p>
<p><img data-src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/mario.jpg" class="absolute" style="top: 200px; right: 80px; width: 450px; "></p>
</section>
<section id="states-space---code-example" class="slide level2">
<h2>States Space - code example</h2>
<p>The state space of the CartPole-v1 environment is represented by a 4-dimensional vector:</p>
<div id="b340c743" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a></a><span class="im">import</span> gymnasium <span class="im">as</span> gym</span>
<span id="cb2-2"><a></a>env <span class="op">=</span> gym.make(<span class="st">"CartPole-v1"</span>)</span>
<span id="cb2-3"><a></a></span>
<span id="cb2-4"><a></a><span class="bu">print</span>(<span class="st">"The State Space is: "</span>, env.observation_space)</span>
<span id="cb2-5"><a></a><span class="bu">print</span>(<span class="st">"Sample observation"</span>, env.observation_space.sample()) <span class="co"># Get a random observation</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The State Space is:  Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)
Sample observation [ 2.4335556e+00 -7.9915350e+36 -2.4241246e-01 -2.1394906e+38]</code></pre>
</div>
</div>
</section>
<section id="action-space" class="slide level2">
<h2>Action Space</h2>
<p>Different environments allow different kinds of actions.</p>
<div>
<ul>
<li class="fragment">Discrete action spaces, where only a finite number of moves are available to the agent.</li>
<li class="fragment">Continuous spaces, where actions are real-valued vectors.</li>
</ul>
</div>
</section>
<section id="action-space---code-example" class="slide level2">
<h2>Action Space - code example</h2>
<p>The action space is discrete, consisting of two possible actions:</p>
<div id="31c00ce2" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a><span class="bu">print</span>(<span class="st">"The State Space is: "</span>, env.action_space)</span>
<span id="cb4-2"><a></a><span class="bu">print</span>(<span class="st">"Sample observation"</span>, env.action_space.sample()) <span class="co"># Get a randoma action</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The State Space is:  Discrete(2)
Sample observation 0</code></pre>
</div>
</div>
</section>
<section id="markov-property-in-rl" class="slide level2">
<h2>Markov Property in RL</h2>
<p>The Markov Property implies that our agent needs only the current state to decide what action to take and not the history of all the states and actions they took before.</p>
<span class="math display">\[\begin{align*}

P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, \dots, s_0, a_0) = P(s_{t+1} | s_t, a_t)

\end{align*}\]</span>
</section>
<section id="trajectories" class="slide level2">
<h2>Trajectories</h2>
<p>A trajectory <span class="math inline">\(\tau\)</span> is a sequence of states and actions in the world,</p>
<span class="math display">\[\begin{align*}

\tau = (s_0, a_0, s_1, a_1, \ldots)

\end{align*}\]</span>
<p>Trajectories are also frequently called episodes or rollouts.</p>
</section>
<section id="trajectories---code-example" class="slide level2">
<h2>Trajectories - code example</h2>
<div class="sourceCode" id="cb6" data-code-line-numbers="2-3|6|7|8|9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a></a>num_steps <span class="op">=</span> <span class="dv">10</span> </span>
<span id="cb6-2"><a></a>trajectory <span class="op">=</span> []</span>
<span id="cb6-3"><a></a>state, _ <span class="op">=</span> <span class="co"># Initial state</span></span>
<span id="cb6-4"><a></a><span class="co"># Sample trajectory</span></span>
<span id="cb6-5"><a></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_steps):</span>
<span id="cb6-6"><a></a>      <span class="co"># Sample a random action</span></span>
<span id="cb6-7"><a></a>      <span class="co"># Act in the enviroment</span></span>
<span id="cb6-8"><a></a>      <span class="co"># Store the transition</span></span>
<span id="cb6-9"><a></a>    state <span class="op">=</span> next_state  <span class="co"># Update the state</span></span>
<span id="cb6-10"><a></a>    <span class="cf">if</span> terminated <span class="kw">or</span> truncated:</span>
<span id="cb6-11"><a></a>        <span class="cf">break</span>  <span class="co"># Apply the action to the environment</span></span>
<span id="cb6-12"><a></a><span class="co"># Print the trajectory</span></span>
<span id="cb6-13"><a></a><span class="cf">for</span> t, transition <span class="kw">in</span> <span class="bu">enumerate</span>(trajectory):</span>
<span id="cb6-14"><a></a>    state, action, reward, next_state <span class="op">=</span> transition</span>
<span id="cb6-15"><a></a>    <span class="bu">print</span>(<span class="ss">f"Step </span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">: State: </span><span class="sc">{</span>state<span class="sc">}</span><span class="ss">, Action: </span><span class="sc">{</span>action<span class="sc">}</span><span class="ss">, Reward: </span><span class="sc">{</span>reward<span class="sc">}</span><span class="ss">, Next State: </span><span class="sc">{</span>next_state<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="trajectories---code-example-1" class="slide level2">
<h2>Trajectories - code example</h2>
<div id="607187a2" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a></a>num_steps <span class="op">=</span> <span class="dv">10</span> </span>
<span id="cb7-2"><a></a>trajectory <span class="op">=</span> []</span>
<span id="cb7-3"><a></a>state, _ <span class="op">=</span> env.reset()</span>
<span id="cb7-4"><a></a><span class="co"># Sample trajectory</span></span>
<span id="cb7-5"><a></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_steps):</span>
<span id="cb7-6"><a></a>    action <span class="op">=</span> env.action_space.sample()  <span class="co"># Sample a random action</span></span>
<span id="cb7-7"><a></a>    next_state, reward, terminated, truncated, info <span class="op">=</span> env.step(action)</span>
<span id="cb7-8"><a></a>    trajectory.append((state, action, reward, next_state))  <span class="co"># Store the transition</span></span>
<span id="cb7-9"><a></a>    state <span class="op">=</span> next_state  <span class="co"># Update the state</span></span>
<span id="cb7-10"><a></a>    <span class="cf">if</span> terminated <span class="kw">or</span> truncated:</span>
<span id="cb7-11"><a></a>        <span class="cf">break</span>  <span class="co"># Apply the action to the environment</span></span>
<span id="cb7-12"><a></a><span class="co"># Print the trajectory</span></span>
<span id="cb7-13"><a></a><span class="cf">for</span> t, transition <span class="kw">in</span> <span class="bu">enumerate</span>(trajectory):</span>
<span id="cb7-14"><a></a>    state, action, reward, next_state <span class="op">=</span> transition</span>
<span id="cb7-15"><a></a>    <span class="bu">print</span>(<span class="ss">f"Step </span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">: State: </span><span class="sc">{</span>state<span class="sc">}</span><span class="ss">, Action: </span><span class="sc">{</span>action<span class="sc">}</span><span class="ss">, Reward: </span><span class="sc">{</span>reward<span class="sc">}</span><span class="ss">, Next State: </span><span class="sc">{</span>next_state<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>

</section>
<section id="trajectories---code-example-1-output" class="slide level2 output-location-slide"><h2>Trajectories - code example</h2><div class="cell output-location-slide" data-execution_count="3">
<div class="cell-output cell-output-stdout">
<pre><code>Step 0: State: [ 0.01383663 -0.02250784 -0.03984438 -0.01259652], Action: 0, Reward: 1.0, Next State: [ 0.01338647 -0.2170364  -0.04009631  0.26725358]
Step 1: State: [ 0.01338647 -0.2170364  -0.04009631  0.26725358], Action: 0, Reward: 1.0, Next State: [ 0.00904574 -0.41156384 -0.03475124  0.5470251 ]
Step 2: State: [ 0.00904574 -0.41156384 -0.03475124  0.5470251 ], Action: 1, Reward: 1.0, Next State: [ 0.00081446 -0.21597134 -0.02381073  0.24359861]
Step 3: State: [ 0.00081446 -0.21597134 -0.02381073  0.24359861], Action: 1, Reward: 1.0, Next State: [-0.00350496 -0.02051751 -0.01893876 -0.0564987 ]
Step 4: State: [-0.00350496 -0.02051751 -0.01893876 -0.0564987 ], Action: 1, Reward: 1.0, Next State: [-0.00391531  0.1748708  -0.02006874 -0.35509628]
Step 5: State: [-0.00391531  0.1748708  -0.02006874 -0.35509628], Action: 1, Reward: 1.0, Next State: [-4.1789780e-04  3.7027225e-01 -2.7170662e-02 -6.5403926e-01]
Step 6: State: [-4.1789780e-04  3.7027225e-01 -2.7170662e-02 -6.5403926e-01], Action: 0, Reward: 1.0, Next State: [ 0.00698755  0.17553896 -0.04025145 -0.3700343 ]
Step 7: State: [ 0.00698755  0.17553896 -0.04025145 -0.3700343 ], Action: 0, Reward: 1.0, Next State: [ 0.01049833 -0.01898867 -0.04765213 -0.09030993]
Step 8: State: [ 0.01049833 -0.01898867 -0.04765213 -0.09030993], Action: 1, Reward: 1.0, Next State: [ 0.01011855  0.17678276 -0.04945833 -0.39763817]
Step 9: State: [ 0.01011855  0.17678276 -0.04945833 -0.39763817], Action: 0, Reward: 1.0, Next State: [ 0.01365421 -0.0176039  -0.0574111  -0.12094954]</code></pre>
</div>
</div></section><section id="policies" class="slide level2">
<h2>Policies</h2>
<p>A policy is a rule used by an agent to decide what actions to take.</p>
<p>In deep RL, we deal with parameterized policies: policies whose outputs are computable functions that depend on a set of parameters (eg the weights and biases of a neural network) which we can adjust to change the behavior via some optimization algorithm.</p>
<span class="math display">\[\begin{align*}
a_t \sim \pi(\cdot | s_t)
\end{align*}\]</span>
</section>
<section id="policies---code-example" class="slide level2">
<h2>Policies - code example</h2>
<div class="sourceCode" id="cb9" data-code-line-numbers="1-6|8-12|14-16|17-18|19-20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a></a><span class="kw">class</span> Policy(nn.Module):</span>
<span id="cb9-2"><a></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, s_size, a_size, h_size):</span>
<span id="cb9-3"><a></a>        <span class="bu">super</span>(Policy, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb9-4"><a></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(s_size, h_size)</span>
<span id="cb9-5"><a></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(h_size, a_size)</span>
<span id="cb9-6"><a></a>        <span class="va">self</span>.log_std <span class="op">=</span> nn.Parameter(torch.zeros(a_size))  <span class="co"># Log standard deviation</span></span>
<span id="cb9-7"><a></a></span>
<span id="cb9-8"><a></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb9-9"><a></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb9-10"><a></a>        mu <span class="op">=</span> <span class="va">self</span>.fc2(x)</span>
<span id="cb9-11"><a></a>        std <span class="op">=</span> torch.exp(<span class="va">self</span>.log_std)  <span class="co"># Standard deviation</span></span>
<span id="cb9-12"><a></a>        <span class="cf">return</span> mu, std</span>
<span id="cb9-13"><a></a></span>
<span id="cb9-14"><a></a>policy <span class="op">=</span> Policy(s_size, a_size, h_size)</span>
<span id="cb9-15"><a></a><span class="co"># Forward pass through the policy to get mean and standard deviation of action distribution</span></span>
<span id="cb9-16"><a></a>mu, std <span class="op">=</span> policy(state.unsqueeze(<span class="dv">0</span>))  <span class="co"># add batch dimension</span></span>
<span id="cb9-17"><a></a>m <span class="op">=</span> Normal(mu, std)</span>
<span id="cb9-18"><a></a>action <span class="op">=</span> m.sample()</span>
<span id="cb9-19"><a></a><span class="co"># Calculate the log probability of the sampled action</span></span>
<span id="cb9-20"><a></a>log_prob <span class="op">=</span> m.log_prob(action).<span class="bu">sum</span>(dim<span class="op">=-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="reward-and-return" class="slide level2">
<h2>Reward and Return</h2>
<p>The reward function R is critically important in reinforcement learning. It depends on the current state of the world, the action just taken, and the next state of the world:</p>
<span class="math display">\[\begin{align*}

r_t = R(s_t, a_t, s_{t+1})

\end{align*}\]</span>
<p>The reward is fundamental in RL because it’s the only feedback for the agent. Thanks to it, our agent knows if the action taken was good or not.</p>
</section>
<section id="finite-horizon-return" class="slide level2">
<h2>Finite-Horizon Return</h2>
<p>Sum of rewards obtained in a fixed window of steps:</p>

<img data-src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_1.jpg" class="r-stretch quarto-figure-center"><p class="caption">The cumulative reward equals the sum of all rewards in the sequence</p></section>
<section id="infinite-horizon-return" class="slide level2">
<h2>Infinite-Horizon Return</h2>
<p>Sum of all rewards ever obtained by the agent, but discounted by how far off in the future they’re obtained:</p>

<img data-src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_4.jpg" class="r-stretch quarto-figure-center"><p class="caption">Discounted expected cumulative reward</p></section>
<section id="the-discount-factor" class="slide level2">
<h2>The Discount Factor</h2>
<p>The discount factor <span class="math inline">\(\gamma \in (0,1)\)</span> is both intuitively appealing and mathematically convenient.</p>
<ul>
<li><p>On an intuitive level: cash now is better than cash later.</p></li>
<li><p>Mathematically: an infinite-horizon sum of rewards may not converge to a finite value, and is hard to deal with in equations. But with a discount factor and under reasonable conditions, the infinite sum converges.</p></li>
</ul>
</section>
<section id="the-rl-problem" class="slide level2">
<h2>The RL Problem</h2>
<p>The goal in RL is to select a policy which maximizes expected return when the agent acts according to it.</p>
<p>Let’s suppose that both the environment transitions and the policy are stochastic. In this case, the probability of a T -step trajectory is:</p>
<span class="math display">\[\begin{align*}

P(\tau|\pi) = \rho_0 (s_0) \prod_{t=0}^{T-1} P(s_{t+1} | s_t, a_t) \pi(a_t | s_t)

\end{align*}\]</span>
</section>
<section id="the-rl-problem-1" class="slide level2">
<h2>The RL Problem</h2>
<p>The expected return (for whichever measure), denoted by <span class="math inline">\(J(\pi)\)</span>, is then:</p>
<span class="math display">\[\begin{align*}

J(\pi) = \int_{\tau} P(\tau|\pi) R(\tau) = \mathbb{E}_{\tau\sim \pi}[R(\tau)]

\end{align*}\]</span>
</section>
<section id="the-rl-problem-2" class="slide level2">
<h2>The RL Problem</h2>
<p>The central optimization problem in RL can then be expressed by</p>
<span class="math display">\[\begin{align*}

\pi^* = \arg \max_{\pi} J(\pi)

\end{align*}\]</span>
<p>with <span class="math inline">\(\pi^*\)</span> being the optimal policy.</p>
</section>
<section id="episodic-task" class="slide level2">
<h2>Episodic task</h2>
<p>In this case, we have a starting point and an ending point (a terminal state). This creates an episode: a list of States, Actions, Rewards, and new States.</p>

<img data-src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/mario.jpg" class="r-stretch quarto-figure-center"><p class="caption">Beginning of a new episode.</p></section>
<section id="continuing-tasks" class="slide level2">
<h2>Continuing tasks</h2>
<p>These are tasks that continue forever (no terminal state). In this case, the agent must learn how to choose the best actions and simultaneously interact with the environment.</p>

<img data-src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/stock.jpg" class="r-stretch"></section>
<section id="the-anatomy-of-a-rl-algorithm" class="slide level2">
<h2>The Anatomy of a RL Algorithm</h2>

<img data-src="https://www.researchgate.net/profile/Jonathan-Boron-2/publication/344589417/figure/fig2/AS:945255583080448@1602377901230/Structure-of-a-Reinforcement-Learning-Algorithm-Source-10.png" class="r-stretch quarto-figure-center"><p class="caption">The anatomy of a reinforcement learning algorithm</p></section>
<section id="a-taxonomy-of-rl-algorithms" class="slide level2">
<h2>A Taxonomy of RL Algorithms</h2>

<img data-src="https://spinningup.openai.com/en/latest/_images/rl_algorithms_9_15.svg" class="r-stretch quarto-figure-center"><p class="caption">A non-exhaustive, but useful taxonomy of algorithms in modern RL</p></section>
<section id="model-free-vs-model-based-rl" class="slide level2">
<h2>Model-Free vs Model-Based RL</h2>
<ul>
<li><p>One of the most important branching points in an RL algorithm is the question of whether the agent has access to (or learns) a model of the environment.</p></li>
<li><p>By a model of the environment, we mean a function which predicts state transitions and rewards.</p></li>
<li><p>Algorithms which use a model are called model-based methods, and those that don’t are called model-free.</p></li>
</ul>
</section>
<section id="model-based-rl" class="slide level2">
<h2>Model-Based RL</h2>

<img data-src="https://www.kukuxiaai.com/images/blog/reinforcement_learning/CS285/introduction_to_rl_8.png" class="r-stretch"></section>
<section id="model-based-rl---improve-the-policy" class="slide level2">
<h2>Model-Based RL - Improve the Policy</h2>
<p>Pure Planning:</p>
<ul>
<li>The most basic approach never explicitly represents the policy, and instead, uses pure planning techniques</li>
</ul>
</section>
<section id="model-based-rl---improve-the-policy-1" class="slide level2">
<h2>Model-Based RL - Improve the Policy</h2>
<p>Expert Iteration:</p>
<ul>
<li><p>A straightforward follow-on to pure planning involves using and learning an explicit representation of the policy, <span class="math inline">\(\pi_{\theta}(a|s)\)</span>.</p></li>
<li><p>The agent uses a planning algorithm (like Monte Carlo Tree Search) in the model, generating candidate actions for the plan by sampling from its current policy.</p></li>
<li><p>The planning algorithm produces an action which is better than what the policy alone would have produced, hence it is an “expert” relative to the policy.</p></li>
</ul>
</section>
<section id="model-based-rl---improve-the-policy-2" class="slide level2">
<h2>Model-Based RL - Improve the Policy</h2>
<p>Data Augmentation for Model-Free Methods:</p>
<ul>
<li>Augment real experiences and use a model-free RL algorithm to train a policy.</li>
</ul>
</section>
<section id="model-free---policy-based" class="slide level2">
<h2>Model-Free - Policy Based</h2>

<img data-src="images/pg.png" class="r-stretch"></section>
<section id="model-free---value-based" class="slide level2">
<h2>Model-Free - Value Based</h2>

<img data-src="images/vb.png" class="r-stretch"></section>
<section id="explorationexploitation-trade-off" class="slide level2">
<h2>Exploration/Exploitation Trade-Off</h2>
<ul>
<li><p>Exploration is trying random actions in order to find more information about the environment.</p></li>
<li><p>Exploitation is exploiting known information to maximize the reward.</p></li>
</ul>

<img data-src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/exp_2.jpg" class="r-stretch"></section>
<section id="why-so-many-rl-algorithms" class="slide level2">
<h2>Why so many RL algorithms?</h2>
<ul>
<li><p>Different tradeoffs - Sample efficiency, Stability &amp; ease of use</p></li>
<li><p>Different assumptions - Stochastic or deterministic? Continuous or discrete? Episodic or infinite horizon?</p></li>
<li><p>Different things are easy or hard in different settings - Easier to represent the policy? Easier to represent the model?</p></li>
</ul>
</section>
<section id="sample-efficiency" class="slide level2">
<h2>Sample Efficiency</h2>
<p>How many samples do we need to get a good policy?</p>
<ul>
<li><p>Off policy: able to improve the policy without generating new samples from that policy</p></li>
<li><p>On policy: each time the policy is changed, even a little bit, we need to generate new samples</p></li>
</ul>

<img data-src="https://braindump.jethro.dev/ox-hugo/screenshot2019-12-16_01-35-50_.png" class="r-stretch"></section>
<section id="real-life-applications" class="slide level2">
<h2>Real-Life Applications</h2>



<img data-src="https://cdn.prod.website-files.com/5d7b77b063a9066d83e1209c/626225c5a5602e1b840a31f0_NxKq7jCHZUXiteWRVby-UO1NTxb7A9qPCeoo1_8licAkii03hzieJdWreERWoVpf-gxw1xsM5xaHmDwOmMD6iyeTflSfx2426CjgoTWCEAhVSme7qqKMVdln-sR0CoS1q3vNx8RV.png" class="r-stretch quarto-figure-center"><p class="caption">Deep Reinforcement Learning applications</p></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/CLAIR_logo.png?raw=true" class="slide-logo"></p>
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="site_libs/revealjs/plugin/search/search.js"></script>
  <script src="site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":false},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp("https:\/\/itaysegev\.quarto\.pub\/introduction-to-rl");
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>
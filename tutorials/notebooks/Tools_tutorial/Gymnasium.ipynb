{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6kDXxMb93dI"
   },
   "source": [
    "# Gymnasium\n",
    "\n",
    "Gymnasium is a project that provides an API for all single agent reinforcement learning environments. We will outline the basics of how to use Gymnasium including its four key functions: `make`, `Env.reset`, `Env.step` and `Env.render`.\n",
    "\n",
    "At the core of Gymnasium is `Env`, a high-level python class representing a markov decision process (MDP). The class provides users the ability generate an initial state, transition / move to new states given an action and the visualise the environment. Alongside `Env`, `Wrapper` are provided to help augment / modify the environment, in particular, the agent observations, rewards and actions taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLsGr9DY93dK"
   },
   "outputs": [],
   "source": [
    "!pip install -q swig\n",
    "!pip install -q mediapy\n",
    "!pip install -q gymnasium\n",
    "!pip install -q gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-72VDFf93dL"
   },
   "source": [
    "## Basic Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sbe2-FJa93dM"
   },
   "source": [
    "Initializing environments is very easy in Gymnasium and can be done via the `make` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TC9XHCM93dM"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYpslmam93dM"
   },
   "source": [
    "This function will return an `Env` for users to interact with.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8skV5EDd93dN"
   },
   "source": [
    "The classic “agent-environment loop” pictured below is simplified representation of reinforcement learning that Gymnasium implements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CX2nq8g93dN"
   },
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src='https://gymnasium.farama.org/_images/AE_loop.png' />\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlesUFlt93dN"
   },
   "source": [
    "### Typical workflow\n",
    "\n",
    "First, an environment is created using `make` with an additional keyword ``\"render_mode\"`` that specifies how the environment should be visualised.\n",
    "\n",
    "After initializing the environment, we `Env.reset` the environment to get the first observation of the environment along with an additional information. For initializing the environment with a particular random seed or options (see the environment documentation for possible values) use the ``seed`` or ``options`` parameters with `reset`.\n",
    "\n",
    "As we wish to continue the agent-environment loop until the environment ends, which is in an unknown number of timesteps, we define ``episode_over`` as a variable to know when to stop interacting with the environment along with a while loop that uses it.\n",
    "\n",
    "Next, the agent performs an action in the environment, `Env.step` executes the select actions to update the environment. As a result, the agent receives a new observation from the updated environment along with a reward for taking the action. One such action-observation exchange is referred to as a **timestep**.\n",
    "\n",
    "However, after some timesteps, the environment may end, this is called the terminal state. In gymnasium, if the environment has terminated, this is returned by `step` as the third variable, ``terminated``.\n",
    "Similarly, we may also want the environment to end after a fixed number of timesteps, in this case, the environment issues a truncated signal. If either of ``terminated`` or ``truncated`` are ``True`` then we end the episode but in most cases users might wish to restart the environment, this can be done with `env.reset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "3o1Ty3oT_hlj",
    "outputId": "ad33a46e-36d9-410b-bd38-454dfdf66f11"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import mediapy as media\n",
    "import time\n",
    "\n",
    "# Initialize the environment with the appropriate render mode\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "frames = []  # List to hold all frames\n",
    "\n",
    "for _ in range(1_000):\n",
    "    action = env.action_space.sample()  # Randomly sample an action\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    frame = env.render()  # Get the frame\n",
    "    frames.append(frame)  # Append frame to the list\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()  # Reset if the episode ends\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Show the collected frames as a video\n",
    "media.show_video(frames, fps=60)  # You can adjust FPS as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udXJPZ0893dP"
   },
   "source": [
    "## Action and observation spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3M287FLV93dP"
   },
   "source": [
    "Every environment specifies the format of valid actions and observations with the `action_space` and `observation_space` attributes. This is helpful for both knowing the expected input and output of the environment as all valid actions and observation should be contained with their respective space.\n",
    "\n",
    "Importantly, `Env.action_space` and `Env.observation_space` are instances of `Space`, a high-level python class that provides the key functions: `Space.contains` and `Space.sample`. Gymnasium has support for a wide range of spaces that users might need:\n",
    "\n",
    "\n",
    "\n",
    "- `Box`: describes bounded space with upper and lower limits of any n-dimensional shape.\n",
    "- `Discrete`: describes a discrete space where ``{0, 1, ..., n-1}`` are the possible values our observation or action can take.\n",
    "- `MultiBinary`: describes a binary space of any n-dimensional shape.\n",
    "- `MultiDiscrete`: consists of a series of `Discrete` action spaces with a different number of actions in each element.\n",
    "- `Text`: describes a string space with a minimum and maximum length\n",
    "- `Dict`: describes a dictionary of simpler spaces.\n",
    "- `Tuple`: describes a tuple of simple spaces.\n",
    "- `Graph`: describes a mathematical graph (network) with interlinking nodes and edges\n",
    "- `Sequence`: describes a variable length of simpler space elements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fU8r5i5W93dQ",
    "outputId": "af2ed180-4f79-4243-ad61-bdcc2eb38e33"
   },
   "outputs": [],
   "source": [
    "from gymnasium.spaces import Box, Discrete, Tuple\n",
    "import numpy as np\n",
    "\n",
    "Box(low=np.array([-1.0, -2.0]), high=np.array([2.0, 4.0]), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DCkqNPPL93dQ",
    "outputId": "24c8f9ca-daec-4732-97c6-1730e8dd4274"
   },
   "outputs": [],
   "source": [
    "observation_space = Discrete(3, start=-1, seed=42)  # {-1, 0, 1}\n",
    "observation_space.sample() # Generates a single random sample from this space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DAUgwLq893dQ",
    "outputId": "87a4e160-9873-4b2d-d2a5-cb45ba89ff72"
   },
   "outputs": [],
   "source": [
    "observation_space = Tuple((Discrete(2), Box(-1, 1, shape=(2,))), seed=42)\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y51GZ0D_93dQ"
   },
   "source": [
    "For more example usage of spaces, see their [documentation](https://gymnasium.farama.org/api/spaces/) along with utility functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0WeAGhW93dQ"
   },
   "source": [
    "## Modifying the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9br8bJG93dR"
   },
   "source": [
    "Wrappers are a convenient way to modify an existing environment without having to alter the underlying code directly.\n",
    "\n",
    "Gymnasium already provides many commonly used wrappers. Some examples:\n",
    "\n",
    "- `TimeLimit`: Issues a truncated signal if a maximum number of timesteps has been exceeded (or the base environment has issued a truncated signal).\n",
    "- `ClipAction`: Clips any action passed to ``step`` such that it lies in the base environment's action space.\n",
    "- `RescaleAction`: Applies an affine transformation to the action to linearly scale for a new low and high bound on the environment.\n",
    "- `TimeAwareObservation`: Add information about the index of timestep to observation.\n",
    "\n",
    "Most environments that are generated via `gymnasium.make` will already be wrapped by default using the `TimeLimit`, `OrderEnforcing` and `PassiveEnvChecker`.\n",
    "\n",
    "In order to wrap an environment, you must first initialize a base environment. Then you can pass this environment along with (possibly optional) parameters to the wrapper's constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dNfhO3Z893dR",
    "outputId": "7d435e35-b17a-4338-ea61-b50e118dea7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.  0.  0.], 1.0, (3,), float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gymnasium.wrappers import FlattenObservation, RescaleAction, TimeAwareObservation\n",
    "base_env = gym.make(\"CarRacing-v3\")\n",
    "base_env.action_space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jMPNXkyk93dR",
    "outputId": "9ad4f0dd-ed0c-4639-fbee-840b9e7ad48d"
   },
   "outputs": [],
   "source": [
    "wrapped_env = RescaleAction(base_env, min_action=0, max_action=1)\n",
    "wrapped_env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qhHesQwn93dR",
    "outputId": "31f20cb4-6069-433a-fdfb-610d50a25d12"
   },
   "outputs": [],
   "source": [
    "base_env = gym.make(\"CartPole-v1\")\n",
    "base_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oDe6ycSs93dR",
    "outputId": "2cb0dd28-440a-455c-b036-b078270f92c8"
   },
   "outputs": [],
   "source": [
    "env = TimeAwareObservation(base_env)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mgo334h793dR",
    "outputId": "366f52a5-1380-47f9-aaa0-b51ad06393a9"
   },
   "outputs": [],
   "source": [
    "env.step(env.action_space.sample())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h7UhwVn193dS",
    "outputId": "f304250a-2d8f-44c1-b7f7-b1e7fc3ab7b5"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v3\")\n",
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jKFXXZLy93dS",
    "outputId": "335175e8-1c40-4483-b07a-ff622e6da046"
   },
   "outputs": [],
   "source": [
    "env = FlattenObservation(env)\n",
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MX55eRIK93dS"
   },
   "source": [
    "You can access the environment underneath the **first** wrapper by using the `gymnasium.Wrapper.env` attribute. As the `gymnasium.Wrapper` class inherits from `gymnasium.Env` then `gymnasium.Wrapper.env` can be another wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6m_DUCm93dS",
    "outputId": "446a84ab-5b73-4ae2-a948-af7b984c9d93"
   },
   "outputs": [],
   "source": [
    "wrapped_env.env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxOwlLjP93dS"
   },
   "source": [
    "If you want to get to the environment underneath **all** of the layers of wrappers, you can use the `gymnasium.Wrapper.unwrapped` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bFqplFUr93dS",
    "outputId": "e38bfa44-96ce-4121-c1bf-cf9fc8265746"
   },
   "outputs": [],
   "source": [
    "wrapped_env.unwrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3eC5N6w93dS"
   },
   "source": [
    "There are three common things you might want a wrapper to do:\n",
    "\n",
    "- Transform actions before applying them to the base environment\n",
    "\n",
    "- Transform observations that are returned by the base environment\n",
    "\n",
    "- Transform rewards that are returned by the base environment\n",
    "\n",
    "Such wrappers can be easily implemented by inheriting from `gymnasium.ActionWrapper`,` gymnasium.ObservationWrapper`, or `gymnasium.RewardWrapper` and implementing the respective transformation. If you need a wrapper to do more complicated tasks, you can inherit from the `gymnasium.Wrapper` class directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csmOLvxh93dS"
   },
   "source": [
    "### Example: normalize actions\n",
    "\n",
    "It is usually a good idea to normalize observations and actions before giving it to the agent, this prevents this [hard to debug issue](https://github.com/hill-a/stable-baselines/issues/473).\n",
    "\n",
    "In this example, we are going to normalize the action space of *Pendulum-v1* so it lies in [-1, 1] instead of [-2, 2].\n",
    "\n",
    "Note: here we are dealing with continuous actions, hence the `gym.Box` space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-LNBmizq93dT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NormalizeActionWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    :param env: (gym.Env) Gym environment that will be wrapped\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        # Retrieve the action space\n",
    "        action_space = env.action_space\n",
    "        assert isinstance(\n",
    "            action_space, gym.spaces.Box\n",
    "        ), \"This wrapper only works with continuous action space (spaces.Box)\"\n",
    "        # Retrieve the max/min values\n",
    "        self.low, self.high = action_space.low, action_space.high\n",
    "\n",
    "        # We modify the action space, so all actions will lie in [-1, 1]\n",
    "        env.action_space = gym.spaces.Box(\n",
    "            low=-1, high=1, shape=action_space.shape, dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Call the parent constructor, so we can access self.env later\n",
    "        super(NormalizeActionWrapper, self).__init__(env)\n",
    "\n",
    "    def rescale_action(self, scaled_action):\n",
    "        \"\"\"\n",
    "        Rescale the action from [-1, 1] to [low, high]\n",
    "        (no need for symmetric action space)\n",
    "        :param scaled_action: (np.ndarray)\n",
    "        :return: (np.ndarray)\n",
    "        \"\"\"\n",
    "        return self.low + (0.5 * (scaled_action + 1.0) * (self.high - self.low))\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Reset the environment\n",
    "        \"\"\"\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        :param action: ([float] or int) Action taken by the agent\n",
    "        :return: (np.ndarray, float,bool, bool, dict) observation, reward, final state? truncated?, additional informations\n",
    "        \"\"\"\n",
    "        # Rescale action from [-1, 1] to original [low, high] interval\n",
    "        rescaled_action = self.rescale_action(action)\n",
    "        obs, reward, terminated, truncated, info = self.env.step(rescaled_action)\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDZqYtRm93dT"
   },
   "source": [
    "#### Test before rescaling actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_OuGRDIJ93dT",
    "outputId": "219a45ff-c583-4500-e9ba-a983ff077f33"
   },
   "outputs": [],
   "source": [
    "original_env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "print(original_env.action_space.low)\n",
    "for _ in range(10):\n",
    "    print(original_env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29mEHfPE93dT"
   },
   "source": [
    "#### Test the NormalizeAction wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JBx0F7Mw93dT",
    "outputId": "830f4bdb-56f0-4bbb-a36b-ec3cf076f7b1"
   },
   "outputs": [],
   "source": [
    "env = NormalizeActionWrapper(gym.make(\"Pendulum-v1\"))\n",
    "\n",
    "print(env.action_space.low)\n",
    "\n",
    "for _ in range(10):\n",
    "    print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRhnZbh693dT"
   },
   "source": [
    "## Handling Time Limits\n",
    "\n",
    "- **Termination** refers to the episode ending after reaching a terminal state that is defined as part of the environment definition.\n",
    "Examples are - task success, task failure, robot falling down etc. Notably, this also includes episodes ending in finite-horizon environments due to a time-limit\n",
    "\n",
    "- **Truncation** - Truncation refers to the episode ending after an externally defined condition (that is outside the scope of the Markov Decision Process). This could be a time-limit, a robot going out of bounds etc. This is different from time-limits in finite horizon environments as the agent in this case has no idea about this time-limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jy3xQ_ZY93dT"
   },
   "source": [
    "### Why the distinction between termination and truncation is important\n",
    "\n",
    "**When an episode ends due to termination we don’t bootstrap, when it ends due to truncation, we bootstrap.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ys3jRuVp93dU"
   },
   "source": [
    "Here is an illustrative example and not part of any specific algorithm:\n",
    "\n",
    "```python\n",
    "# INCORRECT\n",
    "vf_target = rew + gamma * (1 - done) * vf_next_state\n",
    "\n",
    "# CORRCET\n",
    "vf_target = rew + gamma * (1 - terminated) * vf_next_state\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Bcuk7Y793dU"
   },
   "source": [
    "From v0.26 onwards, Gymnasium’s `env.step` API returns both termination and truncation information explicitly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJ-a6uGC93dU"
   },
   "source": [
    "## Make your own custom environment\n",
    "\n",
    "In practice this is how a gym environment looks like. Here, we have implemented a simple grid world were the agent must learn to go always left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGn-JqSK93dU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "\n",
    "class GoLeftEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Environment that follows gym interface.\n",
    "    This is a simple env where the agent must learn to go always left.\n",
    "    \"\"\"\n",
    "\n",
    "    # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
    "    metadata = {\"render_modes\": [\"console\"]}\n",
    "\n",
    "    # Define constants for clearer code\n",
    "    LEFT = 0\n",
    "    RIGHT = 1\n",
    "\n",
    "    def __init__(self, grid_size=10, render_mode=\"console\"):\n",
    "        super(GoLeftEnv, self).__init__()\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Size of the 1D-grid\n",
    "        self.grid_size = grid_size\n",
    "        # Initialize the agent at the right of the grid\n",
    "        self.agent_pos = grid_size - 1\n",
    "\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions, we have two: left and right\n",
    "        n_actions = 2\n",
    "        self.action_space = spaces.Discrete(n_actions)\n",
    "        # The observation will be the coordinate of the agent\n",
    "        # this can be described both by Discrete and Box space\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=self.grid_size, shape=(1,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Important: the observation must be a numpy array\n",
    "        :return: (np.array)\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed, options=options)\n",
    "        # Initialize the agent at the right of the grid\n",
    "        self.agent_pos = self.grid_size - 1\n",
    "        # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
    "        return np.array([self.agent_pos]).astype(np.float32), {}  # empty info dict\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == self.LEFT:\n",
    "            self.agent_pos -= 1\n",
    "        elif action == self.RIGHT:\n",
    "            self.agent_pos += 1\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Received invalid action={action} which is not part of the action space\"\n",
    "            )\n",
    "\n",
    "        # Account for the boundaries of the grid\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
    "\n",
    "        # Are we at the left of the grid?\n",
    "        terminated = bool(self.agent_pos == 0)\n",
    "        truncated = False  # we do not limit the number of steps here\n",
    "\n",
    "        # Null reward everywhere except when reaching the goal (left of the grid)\n",
    "        reward = 1 if self.agent_pos == 0 else 0\n",
    "\n",
    "        # Optionally we can pass additional info, we are not using that for now\n",
    "        info = {}\n",
    "\n",
    "        return (\n",
    "            np.array([self.agent_pos]).astype(np.float32),\n",
    "            reward,\n",
    "            terminated,\n",
    "            truncated,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    def render(self):\n",
    "        # agent is represented as a cross, rest as a dot\n",
    "        if self.render_mode == \"console\":\n",
    "            print(\".\" * self.agent_pos, end=\"\")\n",
    "            print(\"x\", end=\"\")\n",
    "            print(\".\" * (self.grid_size - self.agent_pos))\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJsCzma593dU"
   },
   "source": [
    "### Validate the environment\n",
    "\n",
    "Gymnasium provides a [helper](https://gymnasium.farama.org/api/utils/#gymnasium.utils.env_checker.check_env) to check that your environment follows the Gym interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7-2ClpSA93dU",
    "outputId": "49f8f4ae-5768-4eba-b725-40b9192a6c16"
   },
   "outputs": [],
   "source": [
    "from gymnasium.utils.env_checker import check_env\n",
    "env = GoLeftEnv()\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True, skip_render_check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWCwrTeK93dV"
   },
   "source": [
    "### Testing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oHkLTN4u93dV",
    "outputId": "ea40584c-d95e-4efc-87d3-6d4c11f0e52c"
   },
   "outputs": [],
   "source": [
    "env = GoLeftEnv(grid_size=10, render_mode=\"console\")\n",
    "\n",
    "obs, _ = env.reset()\n",
    "env.render()\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(env.action_space.sample())\n",
    "\n",
    "GO_LEFT = 0\n",
    "# Hardcoded best agent: always go left!\n",
    "n_steps = 20\n",
    "for step in range(n_steps):\n",
    "    print(f\"Step {step + 1}\")\n",
    "    obs, reward, terminated, truncated, info = env.step(GO_LEFT)\n",
    "    done = terminated or truncated\n",
    "    print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(\"Goal reached!\", \"reward=\", reward)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "SDMRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

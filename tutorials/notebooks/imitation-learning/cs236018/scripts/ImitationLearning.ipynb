{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c79b2ed6",
   "metadata": {
    "id": "c79b2ed6"
   },
   "source": [
    "<div style=\"text-align: left\">\n",
    "    <img src='https://avatars.githubusercontent.com/u/101578736?v=4' width=100/>  \n",
    "</div>\n",
    "\n",
    "Author: Itay Segev\n",
    "\n",
    "E-mail: [itaysegev@campus.technion.ac.il](mailto:itaysegev@campus.technion.ac.il)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c04ad1-e844-4836-9de1-206dc454d870",
   "metadata": {
    "id": "e8c04ad1-e844-4836-9de1-206dc454d870",
    "tags": []
   },
   "source": [
    "#  Imitation Learning\n",
    "\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/1/1f/Makak_neonatal_imitation.png?1648499532601' width=1000/>\n",
    "\n",
    "<a id=\"section:intro\"></a>\n",
    "\n",
    "# <img src=\"https://img.icons8.com/?size=50&id=55412&format=png&color=000000\" style=\"height:50px;display:inline\"> Introduction\n",
    "---\n",
    "\n",
    "Imitation Learning (IL) is a technique for learning a policy from demonstrations produced by an \"expert\" (in most cases, a human). There are several types of imitation learning methods, but the simplest approach is called Behavior Cloning (BC). In BC, we attempt to learn a classifier (or regressor if actions are continuous) where the feature space $\\mathcal{X}$ is some representation of the state and the label set $\\mathcal{Y}$ is the set of actions. The expert provides a \"correct\" action for a sample set of states by running in the environment and recording the actions taken at each state. This data is used to learn a classifier that predicts what action the expert would have taken at each state.\n",
    "\n",
    "In this notebook, we present the BC method and implement an example on the [gym taxi environment](https://gymnasium.farama.org/environments/toy_text/taxi/). Our expert will be an A* algorithm with an admissible heuristic (ensuring optimality). We will then learn from the collected expert data using a multi-layer perceptron neural network, implemented in pytorch.\n",
    "\n",
    "Recommended Lecture on IL: [Part 1](https://www.youtube.com/watch?v=kGc8jOy5_zY), [Part 2](https://www.youtube.com/watch?v=06uB13C5pxw), [Part 3](https://www.youtube.com/watch?v=a5wkzPa4fO4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-oid7uNNGKvv",
   "metadata": {
    "id": "-oid7uNNGKvv"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/?size=50&id=43171&format=png&color=000000\" style=\"height:30px;display:inline\"> Setup\n",
    "\n",
    "\n",
    "You will need to make a copy of this notebook in your Google Drive before you can edit the notebook. You can do so with **File &rarr; Save a copy in Drive**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DSNObDRJgu8b",
   "metadata": {
    "cellView": "form",
    "id": "DSNObDRJgu8b"
   },
   "outputs": [],
   "source": [
    "#@title mount your Google Drive\n",
    "import os\n",
    "connect_drive = False #@param {type: \"boolean\"}\n",
    "if connect_drive:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive', force_remount=True)\n",
    "\n",
    "  # set up mount symlink\n",
    "  DRIVE_PATH = '/content/gdrive/My\\ Drive/cs236018_w24'\n",
    "  DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
    "  if not os.path.exists(DRIVE_PYTHON_PATH):\n",
    "    %mkdir $DRIVE_PATH\n",
    "\n",
    "## the space in `My Drive` causes some issues,\n",
    "## make a symlink to avoid this\n",
    "SYM_PATH = '/content/cs236018_w24'\n",
    "if not os.path.exists(SYM_PATH) and connect_drive:\n",
    "  !ln -s $DRIVE_PATH $SYM_PATH\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ft-NJZ29L_p9",
   "metadata": {
    "cellView": "form",
    "id": "Ft-NJZ29L_p9"
   },
   "outputs": [],
   "source": [
    "#@title apt install requirements\n",
    "\n",
    "#@markdown Run each section with Shift+Enter\n",
    "\n",
    "#@markdown Double-click on section headers to show code.\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "!apt update -qq\n",
    "!apt install -y -qq --no-install-recommends \\\n",
    "        build-essential \\\n",
    "        curl \\\n",
    "        git \\\n",
    "        gnupg2 \\\n",
    "        make \\\n",
    "        cmake \\\n",
    "        ffmpeg \\\n",
    "        swig \\\n",
    "        libz-dev \\\n",
    "        unzip \\\n",
    "        zlib1g-dev \\\n",
    "        libglfw3 \\\n",
    "        libglfw3-dev \\\n",
    "        libxrandr2 \\\n",
    "        libxinerama-dev \\\n",
    "        libxi6 \\\n",
    "        libxcursor-dev \\\n",
    "        libgl1-mesa-dev \\\n",
    "        libgl1-mesa-glx \\\n",
    "        libglew-dev \\\n",
    "        libosmesa6-dev \\\n",
    "        lsb-release \\\n",
    "        ack-grep \\\n",
    "        patchelf \\\n",
    "        wget \\\n",
    "        xpra \\\n",
    "        xserver-xorg-dev \\\n",
    "        ffmpeg\n",
    "!apt-get install python-opengl -y -qq\n",
    "!apt install xvfb -y -qq\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ApXAKq1CNbCs",
   "metadata": {
    "id": "ApXAKq1CNbCs"
   },
   "outputs": [],
   "source": [
    "#@title clone course repo\n",
    "\n",
    "%cd $SYM_PATH\n",
    "# !git clone {repo_url}\n",
    "!git clone --single-branch --branch main https://github.com/CLAIR-LAB-TECHNION/SDMRL.git\n",
    "%cd SDMRL/tutorials/notebooks/imitation-learning\n",
    "\n",
    "%pip install -r requirements_colab.txt\n",
    "\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B8_R2qRiG2xA",
   "metadata": {
    "cellView": "form",
    "id": "B8_R2qRiG2xA"
   },
   "outputs": [],
   "source": [
    "#@title set up virtual display\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lT-vzHwjG3fA",
   "metadata": {
    "cellView": "form",
    "id": "lT-vzHwjG3fA"
   },
   "outputs": [],
   "source": [
    "#@title test virtual display\n",
    "\n",
    "#@markdown If you see a video of a taxi moving randomly in a grid, setup is complete!\n",
    "\n",
    "import gym\n",
    "from cs236018.infrastructure.colab_utils import (\n",
    "    wrap_env,\n",
    "    show_video\n",
    ")\n",
    "\n",
    "env = wrap_env(gym.make(\"Taxi-v3\", render_mode='rgb_array'))\n",
    "\n",
    "observation = env.reset()\n",
    "for i in range(100):\n",
    "    env.render()\n",
    "    obs, rew, term, _ = env.step(env.action_space.sample() )\n",
    "    if term:\n",
    "      break;\n",
    "\n",
    "env.close()\n",
    "print('Loading video...')\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcb5e2f-b9e0-4354-a24a-2f893b1313e6",
   "metadata": {
    "id": "2bcb5e2f-b9e0-4354-a24a-2f893b1313e6"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import itertools\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from aidm.Environments.gym_problem import GymProblem\n",
    "from aidm.Search.best_first_search import a_star\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from cs236018.infrastructure.dataset import ImitationLearningDataset\n",
    "from cs236018.infrastructure.pytorch_util import train_torch_model_sgd\n",
    "from cs236018.infrastructure.utils import evaluate_policy\n",
    "from cs236018.infrastructure.colab_utils import animate_policy\n",
    "\n",
    "\n",
    "\n",
    "# initialize taxi env\n",
    "taxi_env = wrap_env(gym.make(\"Taxi-v3\", render_mode='rgb_array'))\n",
    "taxi_env.reset()\n",
    "\n",
    "# constants for taxi env planning\n",
    "PASSENGER_IN_TAXI = 4  # passenger idx when in taxi\n",
    "LOCS = taxi_env.unwrapped.locs  # environment locations\n",
    "\n",
    "# random seed\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998cefdd-fc4a-458b-b33d-d87b8bde0943",
   "metadata": {
    "id": "998cefdd-fc4a-458b-b33d-d87b8bde0943"
   },
   "source": [
    "---\n",
    "\n",
    "We model the world as a deterministic planning model where the state space is the set of all possible environment configurations, and the action space is the list of actions the taxi agent can execute within the environment. A possible configuration is any combination of taxi location, passenger location (including \"in_taxi\" indication), and the destination location. In our example the domain map is 5x5 grid, the passenger and destination can be one of 4 possible locations, and the passenger can be either in the taxi or at the initial destination, adding up to a total of $5\\cdot 5\\cdot 5\\cdot 4 = 500$ states. This makes this simplified taxi environment learnable via deterministic planning and state-value function estimation algorithms. However, for our purposes, we use the model 'under the hood' for simulation. While we acknowledge the model, we focus on using its notations to collect data using an expert policy, rather than directly employing the model framework for planning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815e3964-f4f0-4408-af68-6dcfd0e37e4a",
   "metadata": {
    "id": "815e3964-f4f0-4408-af68-6dcfd0e37e4a"
   },
   "source": [
    "# <img src=\"https://img.icons8.com/?size=50&id=43254&format=png&color=000000\" style=\"height:50px;display:inline\"> The expert\n",
    "---\n",
    "\n",
    "As mentioned in the [introduction](#section:intro), the expert is an implementation of the A* algorithm for this environment. [aidm](https://github.com/CLAIR-LAB-TECHNION/aidm) is a library that provides generalized search algorithms and supports the taxi environment. The environment is wrapped as a custom `Problem` object that can be solved using A*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9f3b5d-0fc3-41f3-97ca-89c8526d9288",
   "metadata": {
    "id": "9c9f3b5d-0fc3-41f3-97ca-89c8526d9288"
   },
   "outputs": [],
   "source": [
    "taxi_problem = GymProblem(taxi_env, taxi_env.unwrapped.s)\n",
    "taxi_problem.__class__.__bases__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nVKbIc8cvyvd",
   "metadata": {
    "id": "nVKbIc8cvyvd"
   },
   "source": [
    "A* requires an admissible heuristic if we want to guarantee optimality. Below we define the Manhatten distance heuristic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Tu38KdmKv7C0",
   "metadata": {
    "id": "Tu38KdmKv7C0"
   },
   "outputs": [],
   "source": [
    "def manhatten_dist(r1, c1, r2, c2):\n",
    "    # calssic manhatten dist |row1 - row2| + |col1 - col2|\n",
    "    return abs(r1 - r2) + abs(c1 - c2)\n",
    "\n",
    "def taxi_heuristic(node):\n",
    "    # decode state integer to interpretable values\n",
    "    taxi_row, taxi_col, passenger_idx, dest_idx = taxi_env.decode(node.state.get_key())\n",
    "\n",
    "    # dist from the taxi to the destination\n",
    "    return manhatten_dist(taxi_row, taxi_col, *LOCS[dest_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8l0ZwvcNwNZA",
   "metadata": {
    "id": "8l0ZwvcNwNZA"
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/?size=50&id=ndnNDCLXM-H6&format=png&color=000000\" style=\"height:50px;display:inline\"> Task 1:  Create your own heuristic\n",
    "\n",
    "Design a new admissible heuristic for the taxi problem. Fill in the following code with your implementation and test your heuristic by running the next two code cells with your heuristic. Compare the time difference it takes to return a solution. Is it an optimal solution?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Clzvxy7uxKDy",
   "metadata": {
    "id": "Clzvxy7uxKDy"
   },
   "outputs": [],
   "source": [
    "# Your new heuristic function\n",
    "def your_new_heuristic(node):\n",
    "    # decode state integer to interpretable values\n",
    "    taxi_row, taxi_col, passenger_idx, dest_idx = taxi_env.decode(node.state.get_key())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4335b249-1802-4f92-9c36-a2cde3f5ab14",
   "metadata": {
    "cellView": "form",
    "id": "4335b249-1802-4f92-9c36-a2cde3f5ab14"
   },
   "outputs": [],
   "source": [
    "#@title Admissible heuristic\n",
    "\n",
    "#@markdown In this hidden cell, you can find a heuristic that uses domain knowledge, which you can explore as well.\n",
    "#@markdown Below we define the following heuristic:\n",
    "#@markdown * if the passenger is in the taxi, calculate the Manhatten distance between the taxi and the destination and add 1 for the dropoff action\n",
    "\n",
    "#@markdown * if the passenger is not in the taxi, calculate the Manhatten distances between the taxi and the passenger, and between the passenger and the destination. Add 2 for the pickup and dropoff actions.\n",
    "\n",
    "#@markdown Double-click on section headers to show code.\n",
    "\n",
    "def manhatten_dist(r1, c1, r2, c2):\n",
    "    # calssic manhatten dist |row1 - row2| + |col1 - col2|\n",
    "    return abs(r1 - r2) + abs(c1 - c2)\n",
    "\n",
    "def taxi_heuristic(node):\n",
    "    # decode state integer to interpretable values\n",
    "    taxi_row, taxi_col, passenger_idx, dest_idx = taxi_env.decode(node.state.get_key())\n",
    "\n",
    "    # split to 2 cases where the passenger is in the taxi and not in the taxi.\n",
    "    if passenger_idx == PASSENGER_IN_TAXI:\n",
    "        # dist from the taxi to the destination\n",
    "        return manhatten_dist(taxi_row, taxi_col, *LOCS[dest_idx]) + 1  # include dropoff\n",
    "    elif passenger_idx == dest_idx:\n",
    "        # passenger has reached the destination. this is a goal state\n",
    "        return 0\n",
    "    else:\n",
    "        # dist from the taxi to the passenger and from the passenger to the destination\n",
    "        passenger_dist = manhatten_dist(taxi_row, taxi_col, *LOCS[passenger_idx])\n",
    "        dest_dist = manhatten_dist(*LOCS[passenger_idx], *LOCS[dest_idx])\n",
    "        return passenger_dist + dest_dist + 2  # include pickup and dropoff actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gFZAkjAjSlM6",
   "metadata": {
    "id": "gFZAkjAjSlM6"
   },
   "source": [
    "A policy takes an observation and creates a plan. While there are still acitons in that plan, it performs the next action. Otherwise, it starts a new plan.\n",
    "\n",
    "This concept is implemented in the `TaxiAStarPolicy` class.\n",
    "The main method of the class is `__call__`, which takes an observation (`obs`) as input. The method works as follows:\n",
    "\n",
    "1. **Check Current Plan**:\n",
    "   - If there are no actions left in the current plan, or if the observation does not match the expected observation in the current plan, a new plan is created.\n",
    "\n",
    "2. **Create New Plan**:\n",
    "   - The problem is refreshed with a new initial state from the environment.\n",
    "   - The A* algorithm is used to find a solution from the current state to the goal. This solution includes a series of states (`state_lst`) and corresponding actions (`sol`).\n",
    "\n",
    "3. **Save the Plan**:\n",
    "   - The expected states and actions are combined into tuples and stored in `cur_plan` for later use. Each tuple contains an expected observation and the corresponding expert action.\n",
    "\n",
    "4. **Execute Next Action**:\n",
    "   - The next action in the current plan is performed by popping it from the `cur_plan` deque.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e1c657-3457-45b2-8d19-6cd659ad18fc",
   "metadata": {
    "id": "d9e1c657-3457-45b2-8d19-6cd659ad18fc"
   },
   "outputs": [],
   "source": [
    "class TaxiAStarPolicy:\n",
    "    def __init__(self, heuristic):\n",
    "        self.heuristic = heuristic\n",
    "\n",
    "        # a container for the plan actions.\n",
    "        self.cur_plan = deque()\n",
    "\n",
    "    def __call__(self, obs):\n",
    "        # if out of actions (finished previous plan), or if observation is not in current plan,\n",
    "        # create a new plan.\n",
    "        if not self.cur_plan or self.cur_plan[0][0] != obs:\n",
    "            # refresh the problem with a new initial state\n",
    "            taxi_prob = GymProblem(taxi_env, taxi_env.unwrapped.s)\n",
    "\n",
    "            # find the solution with the A* algorithm\n",
    "            _, node, sol, _, _ = a_star(taxi_prob, heuristic_func=self.heuristic)\n",
    "\n",
    "            # get a list of expected states\n",
    "            state_lst = []\n",
    "            while node.parent:\n",
    "                node = node.parent\n",
    "                state_lst.append(node.state.key)\n",
    "            state_lst = reversed(state_lst)\n",
    "\n",
    "            # save the plan for later extraction\n",
    "            # a plan is a tuple of expected observations and the corresponding expert action\n",
    "            self.cur_plan = deque(list(zip(state_lst, map(int, sol))))\n",
    "\n",
    "        # pop the next action\n",
    "        return self.cur_plan.popleft()[1]\n",
    "\n",
    "taxi_expert = TaxiAStarPolicy(taxi_heuristic) #TODO: Change to your_new_heuristic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cafba03-3c03-4824-90bf-a37b53826adf",
   "metadata": {
    "id": "9cafba03-3c03-4824-90bf-a37b53826adf"
   },
   "source": [
    "Let's see how our policy performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab5a7cc-3a3e-4233-a326-e5b5a028c87f",
   "metadata": {
    "id": "6ab5a7cc-3a3e-4233-a326-e5b5a028c87f"
   },
   "outputs": [],
   "source": [
    "# This code can be terminated early with an interruption\n",
    "animate_policy(taxi_env, taxi_expert, episode_limit=10)\n",
    "print('Loading video...')\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b1226a-918f-4699-8935-48ecdf055375",
   "metadata": {
    "id": "f2b1226a-918f-4699-8935-48ecdf055375"
   },
   "source": [
    "As we can see, we can use planning to define an optimal policy for this domain that solves the problem in real time. This will be our expert from which we will collect our demonstration data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5b6d3a-010f-44e6-b7db-32b6d5bcf22c",
   "metadata": {
    "id": "0a5b6d3a-010f-44e6-b7db-32b6d5bcf22c"
   },
   "source": [
    "# <img src=\"https://img.icons8.com/?size=50&id=104319&format=png&color=000000\" style=\"height:50px;display:inline\"> Trajectories\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Given an initial state $s$ and a policy $\\pi$, a **trajectory** (in a deterministic environment) is a collection of state action pairs $((s_1, a_1), ..., (s_n, a_n))$ where $s_1=s$, $a_i = \\pi(s_i)$, and since we are in a deterministic environment, $s_{i+1}=P(s_i, a_i)$. In other words, a trajectory is an ordered collection of states and actions as experienced by running policy $\\pi$ starting from state $s$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ccde5c-c748-4b6f-9f10-abc15bb64a4a",
   "metadata": {
    "id": "60ccde5c-c748-4b6f-9f10-abc15bb64a4a"
   },
   "outputs": [],
   "source": [
    "# trajectory struct\n",
    "class Trajectory:\n",
    "    def __init__(self, observations=None, actions=None):\n",
    "        self.observations = observations or []\n",
    "        self.actions = actions or []\n",
    "\n",
    "    def add_step(self, observation, action):\n",
    "        self.observations.append(observation)\n",
    "        self.actions.append(action)\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'trajectory: ' + str(list(zip(self.observations, self.actions)))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21130258-da1b-4a09-aa3c-9d7b73790f48",
   "metadata": {
    "id": "21130258-da1b-4a09-aa3c-9d7b73790f48"
   },
   "source": [
    "We can collect trajectories with our expert by simply recording the observations and the actions taken at these observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d582bd5c-27e4-4a5c-bc15-19de5e1be943",
   "metadata": {
    "id": "d582bd5c-27e4-4a5c-bc15-19de5e1be943"
   },
   "outputs": [],
   "source": [
    "def get_trajectory(env, policy, max_trajectory_length=float('inf')):\n",
    "    # init trajectory object\n",
    "    trajectory = Trajectory()\n",
    "\n",
    "    # get first observation\n",
    "    obs = env.reset()\n",
    "\n",
    "    # iterate and step in environment.\n",
    "    # limit num actions for incomplete policies\n",
    "    for i in itertools.count(start=1):\n",
    "        action = policy(obs)\n",
    "        trajectory.add_step(obs, action)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        if done or i >= max_trajectory_length:\n",
    "            break\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "trajectory = get_trajectory(taxi_env, taxi_expert)\n",
    "trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58454e6-b7ee-4059-98aa-f9a9eebfa8cf",
   "metadata": {
    "id": "e58454e6-b7ee-4059-98aa-f9a9eebfa8cf"
   },
   "source": [
    "# <img src=\"https://img.icons8.com/?size=50&id=pkrAODkotBly&format=png&color=000000\" style=\"height:50px;display:inline\"> Data collection and preparation\n",
    "---\n",
    "\n",
    "We will now collect the data with which we will train by collecting multiple trajectories. As with most supervised learning settings, we will collect 3 datasets: training, validation, and testing. Since this is a relatively simple environment with a small number of states, we will collect a small number of trajectories so we do not encounter the entire state space in training. This way, we can see if our model is generalizing to new, unseen states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f4754-b820-4bde-8642-d928141dc143",
   "metadata": {
    "id": "cd9f4754-b820-4bde-8642-d928141dc143"
   },
   "outputs": [],
   "source": [
    "def collect_data(env, policy, num_trajectories, max_trajectory_length=float('inf')):\n",
    "    trajectories = []\n",
    "    for _ in tqdm(range(num_trajectories)):\n",
    "        trajectories.append(get_trajectory(env, policy, max_trajectory_length))\n",
    "\n",
    "    return trajectories\n",
    "\n",
    "# get the same trajectories every time!\n",
    "taxi_env.seed(SEED)\n",
    "\n",
    "taxi_raw_train_data = collect_data(taxi_env, taxi_expert, num_trajectories=400)\n",
    "taxi_raw_val_data = collect_data(taxi_env, taxi_expert, num_trajectories=250)\n",
    "taxi_raw_test_data = collect_data(taxi_env, taxi_expert, num_trajectories=250)\n",
    "\n",
    "# show the first 5 training trajectories\n",
    "taxi_raw_train_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f5c7c-a7c0-432a-a137-101737396243",
   "metadata": {
    "id": "144f5c7c-a7c0-432a-a137-101737396243"
   },
   "source": [
    "In the taxi environment, states are represented as integers. However, this type of input is not very informative for supervised learning algorithms. To enhance the representation, we can decompose the integer into its state attributes. This preprocessing function represents the state as a vector of features $\\mathcal{X}$ consisting of the taxi location, the passenger location, the destination location, and an indicator of whether the passenger is in the taxi. Using this structured representation allows for more effective learning by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6484896a-0195-49fb-8159-2d7d15c69e1d",
   "metadata": {
    "id": "6484896a-0195-49fb-8159-2d7d15c69e1d"
   },
   "outputs": [],
   "source": [
    "def prep_taxi_state(state):\n",
    "    # decompose state bits\n",
    "    taxi_row, taxi_col, passenger_idx, destination_idx = taxi_env.decode(state)\n",
    "\n",
    "    # get destination true location coordinates\n",
    "    destination_row, destination_col = LOCS[destination_idx]\n",
    "\n",
    "    # get passenger true location coordinates\n",
    "    # add `in_taxi` indicator bit\n",
    "    if passenger_idx == PASSENGER_IN_TAXI:\n",
    "        passenger_row, passenger_col = taxi_row, taxi_col\n",
    "        passenger_in_taxi = 1\n",
    "    else:\n",
    "        passenger_row, passenger_col = LOCS[passenger_idx]\n",
    "        passenger_in_taxi = 0\n",
    "\n",
    "    # return all data as a flat Tensor object for pytorch compatibility\n",
    "    return torch.Tensor([taxi_row,\n",
    "                         taxi_col,\n",
    "                         passenger_row,\n",
    "                         passenger_col,\n",
    "                         passenger_in_taxi,\n",
    "                         destination_row,\n",
    "                         destination_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3de395-9683-4cf5-9b1e-a617254ab5e5",
   "metadata": {
    "id": "ee3de395-9683-4cf5-9b1e-a617254ab5e5"
   },
   "source": [
    "We build the `ImitationLearningDataset` class to package trajectories and preprocessing functions into a format compatible with PyTorch. This class unwraps the trajectories to extract state-action pairs required for supervised learning. Note that we do **NOT** remove duplicates. This is to maintain the true trajectory sample distribution of the expert policy. The dataset is designed to work seamlessly with PyTorch DataLoaders for efficient batching during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261b1d4f-8826-4729-94c7-a242967413dc",
   "metadata": {
    "id": "261b1d4f-8826-4729-94c7-a242967413dc"
   },
   "outputs": [],
   "source": [
    "taxi_ds_train = ImitationLearningDataset(taxi_raw_train_data, prep_obs=prep_taxi_state)\n",
    "taxi_ds_val = ImitationLearningDataset(taxi_raw_val_data, prep_obs=prep_taxi_state)\n",
    "taxi_ds_test = ImitationLearningDataset(taxi_raw_test_data, prep_obs=prep_taxi_state)\n",
    "\n",
    "taxi_ds_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e8b14e-c05f-4316-8b06-2a6e9e6ddcaa",
   "metadata": {
    "id": "75e8b14e-c05f-4316-8b06-2a6e9e6ddcaa"
   },
   "source": [
    "# <img src=\"https://img.icons8.com/?size=50&id=46802&format=png&color=000000\" style=\"height:50px;display:inline\"> Learning model\n",
    "---\n",
    "\n",
    "The learning model we will be using is the [multi-layer perceptron](https://www.sciencedirect.com/topics/computer-science/multilayer-perceptron#:~:text=Multi%20layer%20perceptron%20(MLP)%20is,input%20signal%20to%20be%20processed.) (MLP). This was one of the first neural network architectures. It uses multiple fully connected linear layers separated by non-linear activation functions ([ReLU](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/) in our case). MLP's excell at finding statistical correlations in vector data (data represented as arrays or lists of numbers, where each element represents a different feature) that is strongly ordered (sequence of elements in the vector matters and is consistent) and real valued, especially if these corelations are continuous or near-continuous. Given that our state representation matches this description, the MLP model is well-suited to handle this task efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb3d0fd",
   "metadata": {
    "id": "7cb3d0fd"
   },
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:800/1*-IPQlOd46dlsutIbUq1Zcw.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c08529f-70dd-4e4e-89b5-7165641413f1",
   "metadata": {
    "id": "6c08529f-70dd-4e4e-89b5-7165641413f1"
   },
   "outputs": [],
   "source": [
    "from cs236018.policies.MLP_policy import build_mlp\n",
    "# get the input vector length from a training example\n",
    "in_features = len(taxi_ds_train[0][0])\n",
    "\n",
    "# The output vector length is the number of actions\n",
    "num_actions = taxi_env.action_space.n\n",
    "\n",
    "\n",
    "# create MLP model with 3 hidden layers\n",
    "mlp_taxi = build_mlp(input_size=in_features, output_size=num_actions, n_layers=3, size=32)\n",
    "\n",
    "\n",
    "# TODO: Change the hyperparameters (n_layers, size) to see their effect on the learning process\n",
    "# Try different values for n_layers and size and observe how they impact the performance of the model.\n",
    "\n",
    "mlp_taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bi8qYwwWYzmC",
   "metadata": {
    "id": "Bi8qYwwWYzmC"
   },
   "source": [
    "We selected the hyperparameters (`n_layers`, `size`) following the common practice of tuning these values to find the optimal configuration for our task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GufaMc7p1xlp",
   "metadata": {
    "id": "GufaMc7p1xlp"
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/?size=50&id=ndnNDCLXM-H6&format=png&color=000000\" style=\"height:50px;display:inline\"> Task 2: Explore different hyperparameters\n",
    "\n",
    "Try different values for n_layers and size and observe how they impact the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v62xqhtA2GAO",
   "metadata": {
    "id": "v62xqhtA2GAO"
   },
   "outputs": [],
   "source": [
    "# When you fill code in cells for tasks, make sure you delete the following line\n",
    "%%script true\n",
    "\n",
    "\n",
    "# TODO: Change the hyperparameters (n_layers, size) to see their effect on the learning process\n",
    "# Try different values for n_layers and size and observe how they impact the performance of the model.\n",
    "n_layers = _\n",
    "size = _\n",
    "\n",
    "\n",
    "# create MLP model with 3 hidden layers\n",
    "mlp_taxi = build_mlp(in_features, num_actions, n_layers, size)\n",
    "\n",
    "mlp_taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa726b59-d185-4584-9f4c-3158acc50c02",
   "metadata": {
    "id": "aa726b59-d185-4584-9f4c-3158acc50c02"
   },
   "source": [
    "# <img src=\"https://img.icons8.com/?size=50&id=104328&format=png&color=000000\" style=\"height:50px;display:inline\"> Training\n",
    "---\n",
    "\n",
    "\n",
    "We will train the classifier with [stochastic gradient descent](https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31) optimization. We train using the Cross-Entropy loss which punishes the classifier for giving low scores to the true classes and higher scores to wrong classes (reminder, in this case a class is an action)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4419959e-1e16-4a69-b7b1-a4f37e39fecb",
   "metadata": {
    "id": "4419959e-1e16-4a69-b7b1-a4f37e39fecb"
   },
   "outputs": [],
   "source": [
    "(train_losses,\n",
    " val_losses,\n",
    " train_accs,\n",
    " val_accs) = train_torch_model_sgd(\n",
    "    model=mlp_taxi,                  # The neural network model to train\n",
    "    ds_train=taxi_ds_train,          # The training dataset\n",
    "    ds_val=taxi_ds_val,              # The validation dataset\n",
    "    loss_fn=torch.nn.CrossEntropyLoss(), # The loss function used for training\n",
    "    batch_size=16,                   # Number of samples per batch\n",
    "    shuffle_data=True,               # Whether to shuffle the data before each epoch\n",
    "    num_epochs=200,                  # Number of epochs to train the model\n",
    "    learning_rate=1e-2,              # Learning rate for the optimizer\n",
    "    weight_decay=1e-5,               # Weight decay (L2 regularization) for the optimizer\n",
    "    print_every=10,                  # Frequency of printing training progress\n",
    "    include_accs=True,               # Whether to calculate and return accuracies\n",
    "    seed=SEED                        # Random seed for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2716d33a-2d06-4ed9-8247-58485e509bd5",
   "metadata": {
    "id": "2716d33a-2d06-4ed9-8247-58485e509bd5"
   },
   "source": [
    "Now let us visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0Rcr_4-KRq86",
   "metadata": {
    "id": "0Rcr_4-KRq86"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create 1x2 figure grid\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "# Plot losses\n",
    "sns.lineplot(x=range(len(train_losses)), y=train_losses, ax=ax1, label='train')\n",
    "sns.lineplot(x=range(len(val_losses)), y=val_losses, ax=ax1, label='validation')\n",
    "ax1.set_title('Cross-Entropy Loss per Epoch')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Cross-Entropy Loss')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot accuracies\n",
    "sns.lineplot(x=range(len(train_accs)), y=train_accs, ax=ax2, label='train')\n",
    "sns.lineplot(x=range(len(val_accs)), y=val_accs, ax=ax2, label='validation')\n",
    "ax2.set_title('Accuracy per Epoch')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d43509-2f4f-46ba-b644-936c65eca555",
   "metadata": {
    "id": "66d43509-2f4f-46ba-b644-936c65eca555"
   },
   "source": [
    "We see a relatiely short and stable training process. Seemingly, our learning algorithm is able to clone the expert's behavior almost perfectly. Below are the final accuracies of our model on our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f84f6c-597a-48f1-b420-947cc0607946",
   "metadata": {
    "id": "85f84f6c-597a-48f1-b420-947cc0607946"
   },
   "outputs": [],
   "source": [
    "def ds_acc(ds, model):\n",
    "    # get the entire dataset at once.\n",
    "    # this is will cause memory issues with large datasets\n",
    "    ds_data, ds_labels = next(iter(DataLoader(ds, batch_size=len(ds))))\n",
    "\n",
    "    # run get model predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ds_preds = model(ds_data)\n",
    "\n",
    "    # calculate prediction accuracies\n",
    "    return torch.mean((torch.argmax(ds_preds, dim=-1) == ds_labels).float()).item()\n",
    "\n",
    "# print test set accuracy\n",
    "print(f'train acc      = {ds_acc(taxi_ds_train, mlp_taxi)}')\n",
    "print(f'validation acc = {ds_acc(taxi_ds_val, mlp_taxi)}')\n",
    "print(f'test acc       = {ds_acc(taxi_ds_test, mlp_taxi)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53effb1b-18fc-478c-88ae-b65d95bb123f",
   "metadata": {
    "id": "53effb1b-18fc-478c-88ae-b65d95bb123f"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/?size=50&id=103792&format=png&color=000000\" style=\"height:50px;display:inline\"> **Discussion**\n",
    "\n",
    "\n",
    "The above reults are considered to be excellent results in classification tasks, surpassing human annotators. Does this mean we have an excellent policy? What happens when a predicted action is not optimal? Can our agent recover from such mistakes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb418e3-7d83-4e21-b19e-40ed29e6d127",
   "metadata": {
    "id": "3fb418e3-7d83-4e21-b19e-40ed29e6d127"
   },
   "source": [
    "# <img src=\"https://img.icons8.com/?size=50&id=55069&format=png&color=000000\" style=\"height:50px;display:inline\"> Evaluating the policy\n",
    "---\n",
    "\n",
    "\n",
    "Our newly trained classifier can be used as a policy for the taxi environment. With every input observation, the classifier gives each action a score. The action that was scored the highest is the one returned by the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eefb67-0387-49cc-9317-f703551aa54d",
   "metadata": {
    "id": "a6eefb67-0387-49cc-9317-f703551aa54d"
   },
   "outputs": [],
   "source": [
    "class ClassifierPolicy:\n",
    "    def __init__(self, model, prep_fn=None):\n",
    "        self.model = model\n",
    "\n",
    "        # if no preprocessing function is given, use the identity function\n",
    "        if prep_fn is None:\n",
    "            self.prep_fn = lambda x: x\n",
    "        else:\n",
    "            self.prep_fn = prep_fn\n",
    "\n",
    "    def __call__(self, observation):\n",
    "        # preprocess observation\n",
    "        prepped_obs = self.prep_fn(observation)\n",
    "        one_obs_batch = prepped_obs[None]  # convert to batch of size 1\n",
    "\n",
    "        # run model to get action scores\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_scores = self.model(one_obs_batch)\n",
    "\n",
    "        # get scores for single observation\n",
    "        obs_score = batch_scores[0]\n",
    "\n",
    "        # choose the action with the highest score\n",
    "        return torch.argmax(obs_score).item()\n",
    "\n",
    "# create a policy driven by the MLP model that uses the same observation preprocessing\n",
    "# function as in training\n",
    "taxi_il_policy = ClassifierPolicy(mlp_taxi, prep_fn=taxi_ds_train.prep_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14618f4-a050-4682-b0be-4ba1a27171fd",
   "metadata": {
    "id": "c14618f4-a050-4682-b0be-4ba1a27171fd"
   },
   "source": [
    "## How good can an imitation policy be?\n",
    "\n",
    "Let us compare the performance of the expert policy and the classifier policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E1mQMHnLT5bs",
   "metadata": {
    "id": "E1mQMHnLT5bs"
   },
   "outputs": [],
   "source": [
    "taxi_env = gym.make(\"Taxi-v3\").env\n",
    "taxi_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35862a6-7a16-401a-a208-ed59ca2705e1",
   "metadata": {
    "id": "c35862a6-7a16-401a-a208-ed59ca2705e1"
   },
   "outputs": [],
   "source": [
    "total_reward, mean_reward = evaluate_policy(taxi_env, taxi_expert, num_episodes=10_000,\n",
    "                                            seed=SEED)\n",
    "print('A* Policy')\n",
    "print('---------')\n",
    "print(f'total reward over all episodes: {total_reward}')\n",
    "print(f'mean reward per episode:        {mean_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efa2eb1-851b-4eed-b7be-6b9b837bf2ea",
   "metadata": {
    "id": "9efa2eb1-851b-4eed-b7be-6b9b837bf2ea"
   },
   "outputs": [],
   "source": [
    "total_reward, mean_reward = evaluate_policy(taxi_env, taxi_il_policy, num_episodes=10_000, seed=SEED)\n",
    "print('Classifier Policy')\n",
    "print('-----------------')\n",
    "print(f'total reward over all episodes: {total_reward}')\n",
    "print(f'mean reward per episode:        {mean_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EEDdexgaS2Sk",
   "metadata": {
    "id": "EEDdexgaS2Sk"
   },
   "source": [
    "We note that, in expectation, our imitation policy performs almost as well as the expert. Sadly, this is not the only measure of a policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd910fc-6e4e-4f7c-ba6e-aae94da2dac7",
   "metadata": {
    "id": "abd910fc-6e4e-4f7c-ba6e-aae94da2dac7"
   },
   "source": [
    "## Imitation learning failures\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb4a4ab-2a23-4c5a-9a25-eb3283b8d6d6",
   "metadata": {
    "id": "4fb4a4ab-2a23-4c5a-9a25-eb3283b8d6d6"
   },
   "source": [
    "In the vast majority of cases, our policy acts identically to the expert. However, during the above animation, you may have seen the policy fail miserably in one or more starting position. Let us recreate one such a scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b4b943-41f6-47bd-afa9-2ae19014e2a6",
   "metadata": {
    "id": "19b4b943-41f6-47bd-afa9-2ae19014e2a6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pygame\n",
    "from renderlab import RenderFrame\n",
    "# Ensure all Pygame instances are closed\n",
    "pygame.quit()\n",
    "# This code can be terminated early with an interruption\n",
    "taxi_env = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "# reset env\n",
    "taxi_env.reset()\n",
    "\n",
    "failure_obs = taxi_env.encode(3, 2, 3, 2)\n",
    "taxi_env.unwrapped.s = failure_obs\n",
    "\n",
    "# step using\n",
    "failure_action = taxi_il_policy(failure_obs)\n",
    "taxi_env.step(failure_action)\n",
    "\n",
    "# render the environment\n",
    "env_render = taxi_env.render(mode='ansi')\n",
    "\n",
    "# Display the render result\n",
    "print(env_render)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6VAgAr2SWdkb",
   "metadata": {
    "id": "6VAgAr2SWdkb"
   },
   "source": [
    "As we can see, the taxi should go North on its way to location B to pick up the passenger. However, our classifier policy chooses to go East, thus crashing into the wall. Worse yet, as long as the episode is live, the state will remain the same and our policy will choose the same action over and over. In a real-world situation, we could not afford to repeatedly crash a car into the wall. Obviously, our expert can solve this problem with ease, but our clasifier is unable to generalize. This kind of failure is caused by two phenomena, discussed below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TdzhH8sje-EW",
   "metadata": {
    "id": "TdzhH8sje-EW"
   },
   "source": [
    "# <img src=\"https://img.icons8.com/?size=50&id=yg0Xl3Bazd07&format=png&color=000000\" style=\"height:50px;display:inline\"> Challenges in Behavioral Cloning\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_VFgrE6kdTyO",
   "metadata": {
    "id": "_VFgrE6kdTyO"
   },
   "source": [
    "### Distributional shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QlN7MKjhX8L7",
   "metadata": {
    "id": "QlN7MKjhX8L7"
   },
   "source": [
    "\n",
    "\n",
    "<img src=\"https://futureoflife.org/wp-content/uploads/2019/06/distributional-shift.png\"/>\n",
    "\n",
    "In the classification problem, we assume our finite sample set $S\\subset \\mathcal{X}\\times \\mathcal{Y}$ was sampled I.I.D. from some distribution $D$. In reality, the sample set is a collection of trajectories sampled from distribution $D_{\\pi_{\\text{expert}}}$ that is dependent on the expert policy. When deploying our algorithm, we sample data from $D_{\\pi}$ that is dependant on our policy $\\pi$. However, unless  $\\pi_{\\text{expert}} = \\pi$, then $D_{\\pi_{\\text{expert}}} \\neq D_{\\pi}$, and so incoming data is sampled from outside the expected distribution.\n",
    "\n",
    "When the agent observes a previously unseen state, it may act differently from the expert. When this happens, the next observation is sampled from $D_\\pi$ and not $D_{\\pi_{\\text{expert}}}$, on which our agent is even more likely to make a mistake. This issue compounds as the distribution of the samples \"shifts\" from $D_{\\pi_{\\text{expert}}}$ to $D_\\pi$.\n",
    "\n",
    "Distributional shift is hard to demonstrate on the single taxi domain due to its simplicity. In the above example, the taxi begins from an unseen state (we know it is unseen because the training accuracy is 1). Since the policy's failure leaves the state unchanged, the distribution has nowhere to shift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whONpfPQYBQS",
   "metadata": {
    "id": "whONpfPQYBQS"
   },
   "source": [
    "### Accumulating Errors Due to Imperfect Data\n",
    "Behavioral cloning is highly sensitive to the quality of the data. If the training data is perfect, meaning it contains the optimal actions for every possible state, the policy can learn effectively. However, in real-world scenarios, perfect data is rarely available. Even small mistakes in the data can lead to significant issues because of the compounding nature of errors. When the policy makes a mistake, it moves the agent into states that are not well-represented in the training data, leading to further mistakes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0uqi4RU-bhXc",
   "metadata": {
    "id": "0uqi4RU-bhXc"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/?size=50&id=103792&format=png&color=000000\" style=\"height:50px;display:inline\"> **Discussion**\n",
    "\n",
    "\n",
    "In the real world, many problems involve continuous state spaces, where the states are not discrete and can take any value within a range.\n",
    "How can we design sampling strategies that efficiently cover the continuous state space? What approaches can be employed to enhance the model's ability to generalize from a finite set of training samples to the entire continuous state space? How might distributional shift affect a policy operating in a continuous state space differently than in a discrete one?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zDAGvwjoqnok",
   "metadata": {
    "id": "zDAGvwjoqnok"
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/cute-clipart/64/000000/warning-shield.png\" style=\"height:30px;display:inline\"> Advanced Topics in Imitation Learning\n",
    "\n",
    "In the following cells, we are going to explore some advanced topics in imitation learning. We will describe them briefly without getting into too much depth. If you're curious and want to learn more, feel free to check out the further reading section or reach out to the course team.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h0LdBobWe97c",
   "metadata": {
    "id": "h0LdBobWe97c"
   },
   "source": [
    "## Data Collection and Augmentation Techniques\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rate5qmuZIB9",
   "metadata": {
    "id": "rate5qmuZIB9"
   },
   "source": [
    "### Importance of Corrections in Training Data\n",
    "One way to mitigate the issue of accumulating errors is by incorporating corrections in the training data. If the dataset contains examples of mistakes and the corresponding corrective actions, the policy can learn how to recover from errors. This makes the policy more robust and capable of handling a wider range of scenarios. For instance, if the training data includes states resulting from both optimal actions and mistakes, the policy can learn to navigate from erroneous situations back to optimal paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83HFpYdzY4u_",
   "metadata": {
    "id": "83HFpYdzY4u_"
   },
   "source": [
    "### Data Augmentation Strategies\n",
    "Data augmentation involves creating additional training data by modifying existing data. This technique can help simulate various states that the policy might encounter, even if they were not present in the original dataset. For example, in the context of self-driving cars, side-facing cameras can provide alternative perspectives, simulating off-center positions that the car might need to correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PmVe3Gj0ZLiu",
   "metadata": {
    "cellView": "form",
    "id": "PmVe3Gj0ZLiu"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "#@title Example: Drone Flying Through Forests Using Camera Data Augmentation\n",
    "video_id = \"umRdt3zGgpU\"\n",
    "html_code = f\"\"\"\n",
    "<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/{video_id}\" frameborder=\"0\" allowfullscreen></iframe>\n",
    "\"\"\"\n",
    "display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58OSwBOYZ6-G",
   "metadata": {
    "id": "58OSwBOYZ6-G"
   },
   "source": [
    "An illustrative example of data augmentation comes from a study where drones were trained to navigate through forests. Instead of using just the forward-facing camera, the researchers mounted cameras on a hat worn by a person walking through the forest. These cameras faced forward, left, and right. The left-facing camera was labeled with the action to go right, the right-facing camera with the action to go left, and the forward-facing camera with the action to go straight. This simple augmentation provided diverse training data, enabling the drone to learn corrective actions more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elUX7VUqbcSo",
   "metadata": {
    "id": "elUX7VUqbcSo"
   },
   "source": [
    "## Non-Markovian Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z9sG5h7Me9zE",
   "metadata": {
    "id": "z9sG5h7Me9zE"
   },
   "source": [
    "We previously noticed that in the above example, our taxi continuously chooses the East action, casuing it to hit the wall and remain in place. Why does the agent not realize this action is not helping it advance toward the goal? This is due to the ***Markovian assumption***. Under it, the next state is determined only by the current state and action, regardless of any previous states visited or actions taken. In other words, our model does account for any memory of the past. Specifically, the agent has no recollection of hitting the wall, and so it has no access to information that could hint to East being a bad action.\n",
    "\n",
    "Remember that after training, neural networks are nothing more than functions. In our case, we have a deterministic model, i.e., for any observation $s$, $\\pi(s)$ will always yield the same output. Since our model parameters will no longer change, we cannot hope to surpass this problematic situation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AxAACryjbmxb",
   "metadata": {
    "id": "AxAACryjbmxb"
   },
   "source": [
    "### Using Sequence Models to Incorporate Temporal Context\n",
    "To address non-Markovian behavior, policies can be augmented with sequence models that incorporate a history of observations. Models like Long Short-Term Memory (LSTM) networks or Transformers can process sequences of observations and learn to make decisions based on the entire sequence. This allows the policy to account for temporal dependencies and make more informed decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sElwenQmc2Dr",
   "metadata": {
    "cellView": "form",
    "id": "sElwenQmc2Dr"
   },
   "outputs": [],
   "source": [
    "#@title Example: Imitation with Transformers\n",
    "video_id = \"UuKAp9a6wMs\"\n",
    "html_code = f\"\"\"\n",
    "<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/{video_id}\" frameborder=\"0\" allowfullscreen></iframe>\n",
    "\"\"\"\n",
    "display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aNYrnFeLbpEd",
   "metadata": {
    "id": "aNYrnFeLbpEd"
   },
   "source": [
    "### Potential Pitfalls and Causal Confusion\n",
    "While using sequence models can help address non-Markovian behavior, it can also introduce new challenges. One potential issue is causal confusion, where the policy learns spurious correlations rather than true causal relationships. For example, if the policy associates the activation of a brake light with the action of braking, it might not learn the underlying reason for braking (e.g., an obstacle ahead). Ensuring the policy focuses on the correct causal factors is crucial for effective learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gLR0DSUycH4K",
   "metadata": {
    "id": "gLR0DSUycH4K"
   },
   "source": [
    "## Multimodal Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oYMVBRv1cNek",
   "metadata": {
    "id": "oYMVBRv1cNek"
   },
   "source": [
    "### Definition and Challenges\n",
    "\n",
    "Multimodal behavior occurs when the expert's actions for a given state can follow multiple valid paths, leading to a complex distribution of possible actions. For instance, when faced with an obstacle, the expert might go left or right, both of which are correct. This poses a challenge for the policy, which might struggle to learn a single coherent action strategy if the training data contains such multimodal distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lJdVJWvklHUw",
   "metadata": {
    "id": "lJdVJWvklHUw"
   },
   "source": [
    "### Solutions: Mixture of Gaussians, Latent Variable Models, Diffusion Models\n",
    "Several advanced techniques can address the challenges of multimodal behavior:\n",
    "\n",
    "**Mixture of Gaussians:**\n",
    "A simple yet effective approach is to use a mixture of Gaussians. This method involves modeling the action distribution as a combination of multiple Gaussian distributions, each representing a different mode. The neural network outputs multiple means, variances, and weights for these Gaussians, allowing it to capture the multimodal nature of the actions.\n",
    "\n",
    "**Latent Variable Models:**\n",
    "Latent variable models introduce an additional latent variable that captures the underlying structure of the action distribution. Conditional variational autoencoders (CVAEs) are a popular choice, where the network learns to generate different modes by conditioning on this latent variable. During training, the latent variables are assigned to specific modes, helping the network distinguish between them.\n",
    "\n",
    "**Diffusion Models:**\n",
    "Diffusion models are gaining popularity due to their effectiveness in generating complex distributions. These models start with a highly noisy version of the action and iteratively denoise it. The neural network learns to reverse the noise addition process, effectively modeling the multimodal action distribution.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "luccIEZOc2bx",
   "metadata": {
    "cellView": "form",
    "id": "luccIEZOc2bx"
   },
   "outputs": [],
   "source": [
    "#@title Example: Imitation with latent variables\n",
    "video_id = \"w-CGSQAO5-Q\"\n",
    "html_code = f\"\"\"\n",
    "<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/{video_id}\" frameborder=\"0\" allowfullscreen></iframe>\n",
    "\"\"\"\n",
    "display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iCtvVpOLdrie",
   "metadata": {
    "id": "iCtvVpOLdrie"
   },
   "source": [
    "## Multitask Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C5lgMTNBlHO7",
   "metadata": {
    "id": "C5lgMTNBlHO7"
   },
   "source": [
    "Multitask learning involves training a policy to perform multiple tasks simultaneously. This approach can sometimes make imitation learning easier by providing more diverse training data and better state coverage.\n",
    "\n",
    "### Training with Multiple Goals\n",
    "\n",
    "**Example: Driving to Multiple Locations**\n",
    "Instead of training an agent to drive to a single location (P1) with many demonstrations for that location, you can train a policy to drive to multiple locations. This involves conditioning the policy on the desired location, which can be provided as an input along with the state.\n",
    "\n",
    "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/tut03_multi_tasks_learning.png?raw=true'>\n",
    "\n",
    "**Benefits:**\n",
    "- **Diverse State Coverage:** The expert will visit many different states when attempting to reach various locations, providing more comprehensive training data.\n",
    "- **Robustness to Errors:** The policy learns to handle a wider variety of states, including those resulting from suboptimal behavior.\n",
    "\n",
    "### Goal-Conditioned Behavioral Cloning\n",
    "\n",
    "In goal-conditioned behavioral cloning, the policy is trained using trajectories where the final state (goal) is provided as an additional input. This approach assumes that each demonstration is a good example for reaching the final state observed in the trajectory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2J_ostcxeMhk",
   "metadata": {
    "id": "2J_ostcxeMhk"
   },
   "source": [
    "# <img src=\"https://img.icons8.com/?size=50&id=46678&format=png&color=000000\" style=\"height:50px;display:inline\"> DAGGER Algorithm\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LvjZWVArlHI_",
   "metadata": {
    "id": "LvjZWVArlHI_"
   },
   "source": [
    "\n",
    "### Introduction to DAGGER\n",
    "One interesting solution to distributional shift is [DAGGER](https://www.cs.cmu.edu/~sross1/publications/Ross-AIStats11-NoRegret.pdf) (Dataset Aggregation). In DAGGER, the aim is to try to converge $\\pi_{\\text{expert}}$ to $\\pi$ via an iterative algorithm. The working assumption is that if $\\pi_{\\text{expert}} \\sim \\pi$ then $D_{\\pi_{\\text{expert}}} \\sim D_{\\pi}$. The idea is to perform behavior cloning, deploy the policy to collect more data, and then have an expert annotate the observations with the correct action. The new data is added to the old data and the process starts over. The most glaring issue with this technique is the need for a human annotator, which can be very expensive or simply unsafe. This kind of imitation learning is called policy aggregation.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Zn3e8ZnjfSf9",
   "metadata": {
    "id": "Zn3e8ZnjfSf9"
   },
   "source": [
    "### How DAGGER Works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RHEq1BfBep0d",
   "metadata": {
    "id": "RHEq1BfBep0d"
   },
   "source": [
    "\n",
    "\n",
    "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/tut03_DAgger.png?raw=true'>\n",
    "\n",
    "#### Step-by-Step Algorithm:\n",
    "1. **Initial Policy Training:** Train the initial policy on the expert demonstrations.\n",
    "2. **Policy Execution:** Execute the policy in the real environment to collect observations.\n",
    "3. **Human Labeling:** Ask human experts to label the collected observations with the correct actions.\n",
    "4. **Data Aggregation:** Combine the new labeled data with the original training data.\n",
    "5. **Policy Retraining:** Retrain the policy on the aggregated dataset.\n",
    "6. **Repeat:** Iterate steps 2-5 until the policy converges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Rw4OUg0Uf8ht",
   "metadata": {
    "id": "Rw4OUg0Uf8ht"
   },
   "source": [
    "## Run DAGGER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nzinR-n0dwcv",
   "metadata": {
    "id": "nzinR-n0dwcv"
   },
   "source": [
    "We have provided a script that sets up and runs the DAgger algorithm for you. You can choose between four different environments and expert policies: 'Ant-v4', 'Walker2d-v4', 'HalfCheetah-v4', and 'Hopper-v4'. Additionally, you have the flexibility to explore the results by tweaking various parameters such as the number of training steps, batch sizes, network configurations, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TOaW1-rpPUjC",
   "metadata": {
    "cellView": "form",
    "id": "TOaW1-rpPUjC"
   },
   "outputs": [],
   "source": [
    "#@title imports\n",
    "\n",
    "import time\n",
    "\n",
    "from cs236018.scripts.run import run_training_loop\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DZ-nQ2Qq4Bi5",
   "metadata": {
    "id": "DZ-nQ2Qq4Bi5"
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/?size=50&id=ndnNDCLXM-H6&format=png&color=000000\" style=\"height:50px;display:inline\"> Task 3: Explore the DAgger Algorithm with Different Arguments\n",
    "\n",
    "Change the values of various arguments such as `ep_len`, `n_iter`, `batch_size`, `n_layers`, `size`, and `learning_rate`, then run the algorithm with these new settings. Observe the results and record the performance metrics and behavior changes for each set of arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imnAkQ6jryL7",
   "metadata": {
    "cellView": "form",
    "id": "imnAkQ6jryL7"
   },
   "outputs": [],
   "source": [
    "#@title runtime arguments\n",
    "\n",
    "class Args:\n",
    "\n",
    "  def __getitem__(self, key):\n",
    "    return getattr(self, key)\n",
    "\n",
    "  def __setitem__(self, key, val):\n",
    "    setattr(self, key, val)\n",
    "\n",
    "  #@markdown expert data\n",
    "  env_name = 'Walker2d-v4' #@param ['Ant-v4', 'Walker2d-v4', 'HalfCheetah-v4', 'Hopper-v4']\n",
    "  expert_policy_file = 'cs236018/policies/experts/' + env_name.split('-')[0] + '.pkl'\n",
    "  expert_data = 'cs236018/expert_data/expert_data_' + env_name + '.pkl'\n",
    "  exp_name = 'dagger_' + env_name.split('-')[0]\n",
    "  do_dagger = True\n",
    "  ep_len = 1000 #@param {type: \"integer\"}\n",
    "  save_params = False\n",
    "\n",
    "  num_agent_train_steps_per_iter = 1000 #@param {type: \"integer\"})\n",
    "  n_iter = 10 #@param {type: \"integer\"})\n",
    "\n",
    "  #@markdown batches & buffers\n",
    "  batch_size_initial = 2000\n",
    "  batch_size = 1000 #@param {type: \"integer\"})\n",
    "  eval_batch_size = 1000 #@param {type: \"integer\"}\n",
    "  train_batch_size = 100 #@param {type: \"integer\"}\n",
    "  max_replay_buffer_size = 1000000\n",
    "\n",
    "  #@markdown network\n",
    "  n_layers = 2 #@param {type: \"integer\"}\n",
    "  size = 64 #@param {type: \"integer\"}\n",
    "  learning_rate = 5e-3 #@param {type: \"number\"}\n",
    "\n",
    "  video_log_freq = 5\n",
    "  scalar_log_freq = 1\n",
    "\n",
    "  #@markdown gpu & run-time settings\n",
    "  no_gpu = False\n",
    "  which_gpu = 0\n",
    "  seed = 1 #@param {type: \"integer\"}\n",
    "\n",
    "args = Args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7UkzHBfxsxH8",
   "metadata": {
    "cellView": "form",
    "id": "7UkzHBfxsxH8"
   },
   "outputs": [],
   "source": [
    "#@title create directory for logging\n",
    "import os\n",
    "\n",
    "data_path ='/content/cs236018_S24/tut03/data'\n",
    "if not (os.path.exists(data_path)):\n",
    "    os.makedirs(data_path)\n",
    "logdir = args.exp_name + '_' + args.env_name + \\\n",
    "         '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "logdir = os.path.join(data_path, logdir)\n",
    "args['logdir'] = logdir\n",
    "if not(os.path.exists(logdir)):\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_qQb789_syt0",
   "metadata": {
    "id": "_qQb789_syt0"
   },
   "outputs": [],
   "source": [
    "## run training\n",
    "print(args.logdir)\n",
    "run_training_loop(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zsHmXwVDeXbQ",
   "metadata": {
    "id": "zsHmXwVDeXbQ"
   },
   "source": [
    "After running the DAgger algorithm, we can use TensorBoard to visualize the results and monitor the performance of our agent. TensorBoard is a powerful tool for visualizing machine learning experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75M0MlR5tUIb",
   "metadata": {
    "id": "75M0MlR5tUIb"
   },
   "outputs": [],
   "source": [
    "#@markdown You can visualize your runs with tensorboard from within the notebook\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/cs236018_W24/tut03/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7yDZo7me4h7d",
   "metadata": {
    "id": "7yDZo7me4h7d"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/?size=50&id=103792&format=png&color=000000\" style=\"height:50px;display:inline\"> **Discussion**\n",
    "\n",
    "How does episode length (`ep_len`) influence learning and performance? What impact do varying training iterations (`n_iter`) and steps per iteration (`num_agent_train_steps_per_iter`) have? How do different batch sizes affect training stability and accuracy? How do changes in network architecture (`n_layers`, `size`) impact learning? How do different learning rates (`learning_rate`) affect convergence and stability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175852a-f29b-430a-97b8-6565bc3a503f",
   "metadata": {
    "id": "f175852a-f29b-430a-97b8-6565bc3a503f"
   },
   "source": [
    "# <img src=\"https://img.icons8.com/?size=100&id=46509&format=png&color=000000\" style=\"height:50px;display:inline\"> Conclusion\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "IL, and specifically BC, is a simple solution to solving basic RL problems. Although it is not perfect, it can achieve some impressive results in expectation. However, we notice two massive issues that manage to cripple even as simple a domain as single-taxi.\n",
    "\n",
    "Our learner is unable to generalize to unseen states and so one mistake can lead to complete failure. A straight forward solution is to collect more data. Due to the small number of states, we will most likely collect optimal actions for all possible states if we collect enough trajectories. Our deep classifier is able to completely fit the training data, and so in this case our classifier correctly chooses the optimal action (as would the expert) at every state.\n",
    "\n",
    "The above solution is only relevant for a simple domain such as this, and is not feasible in much larger state spaces or continuous spaces, in which our algorithms must be able to generalize to unseen observations if they are to be useful. In such cases, it is worth considering other approaches. (For example, A* can be practical if you have a very informative heuristic, but it may fail on large domains without one).\n",
    "\n",
    "There are other forms of imitation learning (see [this blog bost](https://smartlabai.medium.com/a-brief-overview-of-imitation-learning-8a8a75c44a9c)) besides BC that were not discussed in this presentation. These techniques are aimed at solving issues in BC.\n",
    "\n",
    "The limitations of imitation learning highlight the need for methods that can autonomously collect data and improve policies without extensive human involvement. In future lectures, we will explore reinforcement learning (RL) techniques that address these challenges by allowing agents to learn from their own experiences, aiming for behaviors that surpass human performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s_Ndmee2iL02",
   "metadata": {
    "id": "s_Ndmee2iL02"
   },
   "source": [
    "# <img src=\"https://img.icons8.com/dusk/64/000000/plus-2-math.png\" style=\"height:50px;display:inline\"> Further Reading on Imitation Learning\n",
    "---\n",
    "\n",
    "* [A brief overview of Imitation Learning](https://smartlabai.medium.com/a-brief-overview-of-imitation-learning-8a8a75c44a9c)\n",
    "\n",
    "* [Generative Adversarial Imitation Learning Paper](https://arxiv.org/abs/1606.03476)\n",
    "\n",
    "* [DAGGER Algorithm](https://arxiv.org/abs/1011.0686)\n",
    "\n",
    "\n",
    "\n",
    "* [Challenges of Imitation Learning](https://arxiv.org/abs/1811.06711)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fxLYbw1wj9wS",
   "metadata": {
    "id": "fxLYbw1wj9wS"
   },
   "source": [
    "# <img src=\"https://img.icons8.com/?size=100&id=46756&format=png&color=000000\" style=\"height:50px;display:inline\"> Credits\n",
    "---\n",
    "* This tutorial is based on a previous one written by Guy Azran.\n",
    "* Examples and code snippets were taken from <a href=\"https://rail.eecs.berkeley.edu/deeprlcourse/\">CS285 - Deep Reinforcement Learning Course at UC Berkeley</a>\n",
    "* Icons from <a href=\"https://icons8.com/\">Icons8.com"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "d0ac9c3e-8ed2-4274-bfb1-dc5e68d561fe"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

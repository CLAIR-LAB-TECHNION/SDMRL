{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c79b2ed6"
      },
      "source": [
        "<div style=\"text-align: left\">\n",
        "    <img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/logo.png?raw=true' width=800/>  \n",
        "</div>\n",
        "\n",
        "Author: Itay Segev\n",
        "\n",
        "E-mail: [itaysegev@campus.technion.ac.il](mailto:itaysegev@campus.technion.ac.il)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img src=\"https://img.icons8.com/?size=50&id=ZCDMsVkk9six&format=png&color=000000\" style=\"height:50px;display:inline\"> Model-based RL\n",
        "\n"
      ],
      "metadata": {
        "id": "axRG7x9zmYu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://news.ubc.ca/wp-content/uploads/2023/08/AdobeStock_559145847.jpeg' width=900/>"
      ],
      "metadata": {
        "id": "GKYSuKP7rVxP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8c04ad1-e844-4836-9de1-206dc454d870",
        "tags": []
      },
      "source": [
        "\n",
        "<a id=\"section:intro\"></a>\n",
        "\n",
        "# <img src=\"https://img.icons8.com/?size=50&id=55412&format=png&color=000000\" style=\"height:50px;display:inline\"> Introduction\n",
        "---\n",
        "\n",
        "The algorithms introduced in the previous tutorials are all model-free, as they do not require a model to use or control behavior. In this section, we will study a different class of algorithms called model-based. In contrast to model-free RL, **model-based methods use a model to build a policy**.\n",
        "\n",
        "We will cover several key concepts, including the importance of learning a model, addressing the challenges of distributional shift, and the benefits of using short model-based rollouts. You will also explore the Dyna algorithm and its modern variants, and understand how model-based acceleration can enhance learning efficiency. Throughout this tutorial, you'll implement the main components of **Q-Dyna**, incorporating synthetic data generation, model training, and policy updates. These steps will help you build a more efficient and robust RL agent by leveraging both real and simulated experiences.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oid7uNNGKvv"
      },
      "source": [
        "# <img src=\"https://img.icons8.com/?size=50&id=43171&format=png&color=000000\" style=\"height:30px;display:inline\"> Setup\n",
        "\n",
        "\n",
        "You will need to make a copy of this notebook in your Google Drive before you can edit the notebook. You can do so with **File &rarr; Save a copy in Drive**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSNObDRJgu8b",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title mount your Google Drive\n",
        "import os\n",
        "connect_drive = False #@param {type: \"boolean\"}\n",
        "if connect_drive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "  # set up mount symlink\n",
        "  DRIVE_PATH = '/content/gdrive/My\\ Drive/cs236203_s24'\n",
        "  DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
        "  if not os.path.exists(DRIVE_PYTHON_PATH):\n",
        "    %mkdir $DRIVE_PATH\n",
        "\n",
        "## the space in `My Drive` causes some issues,\n",
        "## make a symlink to avoid this\n",
        "SYM_PATH = '/content/cs236203_s24'\n",
        "if not os.path.exists(SYM_PATH) and connect_drive:\n",
        "  !ln -s $DRIVE_PATH $SYM_PATH\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bcb5e2f-b9e0-4354-a24a-2f893b1313e6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import convolve as conv\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "T4UEa1B5Ar9L"
      },
      "outputs": [],
      "source": [
        "# @title Figure Settings\n",
        "import logging\n",
        "logging.getLogger('matplotlib.font_manager').disabled = True\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "CX0uaPJjAr9M"
      },
      "outputs": [],
      "source": [
        "#@title Plotting Functions\n",
        "\n",
        "def plot_state_action_values(env, value, ax=None):\n",
        "  \"\"\"\n",
        "  Generate plot showing value of each action at each state.\n",
        "  \"\"\"\n",
        "  if ax is None:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "  for a in range(env.n_actions):\n",
        "    ax.plot(range(env.n_states), value[:, a], marker='o', linestyle='--')\n",
        "  ax.set(xlabel='States', ylabel='Values')\n",
        "  ax.legend(['R','U','L','D'], loc='lower right')\n",
        "\n",
        "\n",
        "def plot_quiver_max_action(env, value, ax=None):\n",
        "  \"\"\"\n",
        "  Generate plot showing action of maximum value or maximum probability at\n",
        "    each state (not for n-armed bandit or cheese_world).\n",
        "  \"\"\"\n",
        "  if ax is None:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "  X = np.tile(np.arange(env.dim_x), [env.dim_y,1]) + 0.5\n",
        "  Y = np.tile(np.arange(env.dim_y)[::-1][:,np.newaxis], [1,env.dim_x]) + 0.5\n",
        "  which_max = np.reshape(value.argmax(axis=1), (env.dim_y,env.dim_x))\n",
        "  which_max = which_max[::-1,:]\n",
        "  U = np.zeros(X.shape)\n",
        "  V = np.zeros(X.shape)\n",
        "  U[which_max == 0] = 1\n",
        "  V[which_max == 1] = 1\n",
        "  U[which_max == 2] = -1\n",
        "  V[which_max == 3] = -1\n",
        "\n",
        "  ax.quiver(X, Y, U, V)\n",
        "  ax.set(\n",
        "      title='Maximum value/probability actions',\n",
        "      xlim=[-0.5, env.dim_x+0.5],\n",
        "      ylim=[-0.5, env.dim_y+0.5],\n",
        "  )\n",
        "  ax.set_xticks(np.linspace(0.5, env.dim_x-0.5, num=env.dim_x))\n",
        "  ax.set_xticklabels([\"%d\" % x for x in np.arange(env.dim_x)])\n",
        "  ax.set_xticks(np.arange(env.dim_x+1), minor=True)\n",
        "  ax.set_yticks(np.linspace(0.5, env.dim_y-0.5, num=env.dim_y))\n",
        "  ax.set_yticklabels([\"%d\" % y for y in np.arange(0, env.dim_y*env.dim_x, env.dim_x)])\n",
        "  ax.set_yticks(np.arange(env.dim_y+1), minor=True)\n",
        "  ax.grid(which='minor',linestyle='-')\n",
        "\n",
        "\n",
        "def plot_heatmap_max_val(env, value, ax=None):\n",
        "  \"\"\"\n",
        "  Generate heatmap showing maximum value at each state\n",
        "  \"\"\"\n",
        "  if ax is None:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "  if value.ndim == 1:\n",
        "      value_max = np.reshape(value, (env.dim_y,env.dim_x))\n",
        "  else:\n",
        "      value_max = np.reshape(value.max(axis=1), (env.dim_y,env.dim_x))\n",
        "  value_max = value_max[::-1,:]\n",
        "\n",
        "  im = ax.imshow(value_max, aspect='auto', interpolation='none', cmap='afmhot')\n",
        "  ax.set(title='Maximum value per state')\n",
        "  ax.set_xticks(np.linspace(0, env.dim_x-1, num=env.dim_x))\n",
        "  ax.set_xticklabels([\"%d\" % x for x in np.arange(env.dim_x)])\n",
        "  ax.set_yticks(np.linspace(0, env.dim_y-1, num=env.dim_y))\n",
        "  if env.name != 'windy_cliff_grid':\n",
        "      ax.set_yticklabels(\n",
        "          [\"%d\" % y for y in np.arange(\n",
        "              0, env.dim_y*env.dim_x, env.dim_x)][::-1])\n",
        "  return im\n",
        "\n",
        "\n",
        "def plot_rewards(n_episodes, rewards, average_range=10, ax=None):\n",
        "  \"\"\"\n",
        "  Generate plot showing total reward accumulated in each episode.\n",
        "  \"\"\"\n",
        "  if ax is None:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "  smoothed_rewards = (conv(rewards, np.ones(average_range), mode='same')\n",
        "                      / average_range)\n",
        "\n",
        "  ax.plot(range(0, n_episodes, average_range),\n",
        "          smoothed_rewards[0:n_episodes:average_range],\n",
        "          marker='o', linestyle='--')\n",
        "  ax.set(xlabel='Episodes', ylabel='Total reward')\n",
        "\n",
        "\n",
        "def plot_performance(env, value, reward_sums):\n",
        "  fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16, 12))\n",
        "  plot_state_action_values(env, value, ax=axes[0,0])\n",
        "  plot_quiver_max_action(env, value, ax=axes[0,1])\n",
        "  plot_rewards(n_episodes, reward_sums, ax=axes[1,0])\n",
        "  im = plot_heatmap_max_val(env, value, ax=axes[1,1])\n",
        "  fig.colorbar(im)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img src=\"https://img.icons8.com/?size=50&id=104313&format=png&color=000000\" style=\"height:50px;display:inline\"> Quentin's World Environment\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tzq_UAm3l1L1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, our RL agent will act in the Quentin's world, a 10x10 grid world.\n",
        "\n",
        "<img alt=\"QuentinsWorld\" width=\"560\" height=\"560\" src=\"https://github.com/NeuromatchAcademy/course-content/blob/main/tutorials/static/W3D4_Tutorial4_QuentinsWorld.png?raw=true\">\n",
        "\n",
        "In this environment, there are 100 states and 4 possible actions: right, up, left, and down. The goal of the agent is to move, via a series of steps, from the start (green) location to the goal (yellow) region, while avoiding the red walls. More specifically:\n",
        "* The agent starts in the green state,\n",
        "* Moving into one of the red states incurs a reward of -1,\n",
        "* Moving into the world borders stays in the same place,\n",
        "* Moving into the goal state (yellow square in the upper right corner) gives you a reward of 1, and\n",
        "* Moving anywhere from the goal state ends the episode.\n",
        "\n",
        "Now that we have our environment and task defined, how can we solve this using a model-based RL agent?"
      ],
      "metadata": {
        "id": "U3gwcIbymgyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class world(object):\n",
        "    def __init__(self):\n",
        "        return\n",
        "\n",
        "    def get_outcome(self):\n",
        "        print(\"Abstract method, not implemented\")\n",
        "        return\n",
        "\n",
        "    def get_all_outcomes(self):\n",
        "        outcomes = {}\n",
        "        for state in range(self.n_states):\n",
        "            for action in range(self.n_actions):\n",
        "                next_state, reward = self.get_outcome(state, action)\n",
        "                outcomes[state, action] = [(1, next_state, reward)]\n",
        "        return outcomes"
      ],
      "metadata": {
        "id": "CMYtIseHmlQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuentinsWorld(world):\n",
        "    \"\"\"\n",
        "    World: Quentin's world.\n",
        "    100 states (10-by-10 grid world).\n",
        "    The mapping from state to the grid is as follows:\n",
        "    90 ...       99\n",
        "    ...\n",
        "    40 ...       49\n",
        "    30 ...       39\n",
        "    20 21 22 ... 29\n",
        "    10 11 12 ... 19\n",
        "    0  1  2  ...  9\n",
        "    54 is the start state.\n",
        "    Actions 0, 1, 2, 3 correspond to right, up, left, down.\n",
        "    Moving anywhere from state 99 (goal state) will end the session.\n",
        "    Landing in red states incurs a reward of -1.\n",
        "    Landing in the goal state (99) gets a reward of 1.\n",
        "    Going towards the border when already at the border will stay in the same\n",
        "        place.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.name = \"QuentinsWorld\"\n",
        "        self.n_states = 100\n",
        "        self.n_actions = 4\n",
        "        self.dim_x = 10\n",
        "        self.dim_y = 10\n",
        "        self.init_state = 54\n",
        "        self.shortcut_state = 64\n",
        "\n",
        "    def toggle_shortcut(self):\n",
        "      if self.shortcut_state == 64:\n",
        "        self.shortcut_state = 2\n",
        "      else:\n",
        "        self.shortcut_state = 64\n",
        "\n",
        "    def get_outcome(self, state, action):\n",
        "        if state == 99:  # goal state\n",
        "            reward = 0\n",
        "            next_state = None\n",
        "            return next_state, reward\n",
        "        reward = 0  # default reward value\n",
        "        if action == 0:  # move right\n",
        "            next_state = state + 1\n",
        "            if state == 98:  # next state is goal state\n",
        "                reward = 1\n",
        "            elif state % 10 == 9:  # right border\n",
        "                next_state = state\n",
        "            elif state in [11, 21, 31, 41, 51, 61, 71,\n",
        "                           12, 72,\n",
        "                           73,\n",
        "                           14, 74,\n",
        "                           15, 25, 35, 45, 55, 65, 75]:  # next state is red\n",
        "                reward = -1\n",
        "        elif action == 1:  # move up\n",
        "            next_state = state + 10\n",
        "            if state == 89:  # next state is goal state\n",
        "                reward = 1\n",
        "            if state >= 90:  # top border\n",
        "                next_state = state\n",
        "            elif state in [2, 12, 22, 32, 42, 52, 62,\n",
        "                           3, 63,\n",
        "                           self.shortcut_state,\n",
        "                           5, 65,\n",
        "                           6, 16, 26, 36, 46, 56, 66]:  # next state is red\n",
        "                reward = -1\n",
        "        elif action == 2:  # move left\n",
        "            next_state = state - 1\n",
        "            if state % 10 == 0:  # left border\n",
        "                next_state = state\n",
        "            elif state in [17, 27, 37, 47, 57, 67, 77,\n",
        "                           16, 76,\n",
        "                           75,\n",
        "                           14, 74,\n",
        "                           13, 23, 33, 43, 53, 63, 73]:  # next state is red\n",
        "                reward = -1\n",
        "        elif action == 3:  # move down\n",
        "            next_state = state - 10\n",
        "            if state <= 9:  # bottom border\n",
        "                next_state = state\n",
        "            elif state in [22, 32, 42, 52, 62, 72, 82,\n",
        "                           23, 83,\n",
        "                           84,\n",
        "                           25, 85,\n",
        "                           26, 36, 46, 56, 66, 76, 86]:  # next state is red\n",
        "                reward = -1\n",
        "        else:\n",
        "            print(\"Action must be between 0 and 3.\")\n",
        "            next_state = None\n",
        "            reward = None\n",
        "        return int(next_state) if next_state is not None else None, reward"
      ],
      "metadata": {
        "id": "U_RgdZ8WmqmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img src=\"https://img.icons8.com/?size=50&id=55011&format=png&color=000000\" style=\"height:50px;display:inline\"> Why Learn a Model in Model-Based Reinforcement Learning?\n",
        "\n",
        "Model-based reinforcement learning (RL) is a powerful approach that involves learning a model of the environment's dynamics and using this model to make decisions. This contrasts with model-free RL, where the agent learns to make decisions directly from interactions with the environment without an explicit model. Here, we will explore why learning a model can be advantageous and discuss a basic approach to implementing model-based RL."
      ],
      "metadata": {
        "id": "ReRan4mLRHjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**But what is a model?** A model (sometimes called a world model or internal model) is a representation of how the world will respond to the agent's actions. You can think of it as a representation of how the world works. With such a representation, the agent can simulate new experiences and learn from these simulations. This is advantageous for two reasons. First, acting in the real world can be costly and sometimes even dangerous. Learning from simulated experience can avoid some of these costs or risks. Second, simulations make fuller use of one's limited experience. To see why, imagine an agent interacting with the real world. The information acquired with each individual action can only be assimilated at the moment of the interaction. In contrast, the experiences simulated from a model can be simulated multiple times -- and whenever desired -- allowing for the information to be more fully assimilated."
      ],
      "metadata": {
        "id": "7Qjs2gyVmJch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advantages of Learning a Model\n",
        "\n",
        "1. **Efficiency**: Model-based RL can be more sample-efficient than model-free RL. By learning a model of the environment, the agent can simulate interactions internally, reducing the need for extensive real-world interaction, which is often costly or time-consuming.\n",
        "\n",
        "2. **Control and Planning**: With a model, the agent can plan its actions by predicting future states and rewards. This capability allows for more informed decision-making and the ability to foresee and avoid potential pitfalls.\n",
        "\n",
        "3. **Flexibility**: Models enable the agent to adapt to changes in the environment by reusing the learned model for different tasks or objectives. This flexibility is particularly useful in dynamic or evolving environments."
      ],
      "metadata": {
        "id": "DJiHQ-BWRdnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Approach to Model-Based RL\n",
        "\n",
        "The process of model-based RL involves two main steps: learning the model and using the model for control.\n"
      ],
      "metadata": {
        "id": "Ck1_QS8MRt2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Learning the Model\n",
        "\n",
        "- The model represents the dynamics of the environment. In the deterministic case, it can be expressed as a function $ f(s_t, a_t) = s_{t+1} $, where $ s_t $ and $ s_{t+1} $ are the states at time $ t $ and $ t+1 $, respectively, and $ a_t $ is the action taken at time $ t $.\n",
        "- In a stochastic environment, the model might represent a probability distribution over the next state, $ P(s_{t+1} \\mid s_t, a_t) $.\n",
        "- The model is typically learned using supervised learning techniques from a dataset of transitions collected by the agent.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "adIeOTj0RvKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using the Model for Control\n",
        "\n",
        "- Once the model is learned, it can be used to plan actions. The agent uses the model to simulate future states and evaluate the potential outcomes of different actions, allowing it to select the optimal action."
      ],
      "metadata": {
        "id": "VDm5Bq9wSSXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example: A Simple Model-Based RL Algorithm\n",
        "\n",
        "Here’s a prototype of a basic model-based RL algorithm, referred to as version 0.5:\n",
        "\n",
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/MB_model-based-0.5.png?raw=true' width=900/>"
      ],
      "metadata": {
        "id": "_m-RDkNGSvfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Challenges and Considerations\n",
        "\n",
        "1. **Distributional Shift**: One of the main challenges in model-based RL is the distributional shift, where the model is trained on data from a certain distribution of states and actions but is later used to predict outcomes in potentially different distributions. This can lead to inaccuracies in the model’s predictions.\n",
        "\n",
        "2. **Uncertainty**: Accurately estimating uncertainty in the model’s predictions can significantly improve the performance of model-based RL algorithms. By considering the uncertainty, the agent can avoid over-reliance on potentially inaccurate predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "rHBYtF7KSy51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img src=\"https://img.icons8.com/?size=50&id=yg0Xl3Bazd07&format=png&color=000000\" style=\"height:50px;display:inline\"> Distributional Shift in Model-Based RL\n",
        "\n",
        "\n",
        "\n",
        "One of the significant challenges in model-based reinforcement learning (RL) is handling distributional shift. This problem arises when the distribution of states encountered during policy execution differs from the distribution of states used to train the model. Let's delve into why this occurs and how it affects the performance of model-based RL algorithms.\n"
      ],
      "metadata": {
        "id": "p0mZaKoGUwJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Distributional Shift\n",
        "\n",
        "To understand distributional shift, consider a simple example of an agent trying to reach the top of a mountain. The agent starts by running a base policy ($\\pi_0$), such as a random policy, to collect data. This initial policy explores the environment and generates a dataset of state transitions. The agent then uses this dataset to learn a dynamics model $ f(s_t, a_t) = s_{t+1} $.\n",
        "\n",
        "With this learned model, the agent plans its actions to maximize its altitude, expecting that moving to the right will lead it to higher ground, based on the initial random data. However, the agent might end up falling off the mountain because it hasn't seen all possible states and transitions during its random walk. This scenario illustrates the core issue of distributional shift.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aTj0W7iPU7pv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Distributional Shift Occurs\n",
        "\n",
        "1. **Training Data Distribution**: The model is trained on data collected under the initial policy $\\pi_0$. The state distribution under this policy is denoted as $ P_{\\pi_0}(s_t) $. This distribution represents the states the agent encountered while following $\\pi_0$.\n",
        "\n",
        "2. **Planning with the Model**: When the agent uses the learned model to plan its actions, it effectively follows a new policy, denoted as $\\pi_f$, which is induced by the model. The state distribution under this new policy is $ P_{\\pi_f}(s_t) $.\n",
        "\n",
        "The key issue is that $ P_{\\pi_f}(s_t) $ is generally not the same as $ P_{\\pi_0}(s_t) $. This discrepancy means that the model, which is accurate for the states seen under $\\pi_0$, may make poor predictions for the states encountered under $\\pi_f$. This leads to erroneous predictions and suboptimal actions, exacerbating the problem as the agent continues to plan and execute actions based on these flawed predictions."
      ],
      "metadata": {
        "id": "cXPWG2KGU-Fk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example of Distributional Shift\n",
        "\n",
        "Consider the agent's goal of climbing a mountain:\n",
        "\n",
        "1. **Data Collection**: The agent performs a random walk ($\\pi_0$) and collects data, learning that moving to the right generally increases altitude.\n",
        "2. **Model Learning**: The agent uses this data to train a model $ f $ that predicts higher altitudes to the right.\n",
        "3. **Planning**: Using $ f $, the agent plans to keep moving right to reach higher altitudes.\n",
        "4. **Execution**: The agent follows $\\pi_f$ (the policy induced by $ f $), but this policy takes the agent to regions not covered by the initial random walk, leading to a fall off the mountain.\n",
        "\n",
        "This example highlights how the model's predictions can fail outside the distribution of the training data, causing significant performance issues."
      ],
      "metadata": {
        "id": "qGPZlR36VVS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why It Becomes a Problem with High-Capacity Models\n",
        "\n",
        "1. **Expressive Models**: High-capacity models like deep neural networks can fit the training data very tightly. While this can improve performance on the training distribution, it makes the models more sensitive to distributional shifts.\n",
        "2. **Overfitting**: These models can overfit to the specific distribution of states seen during training, leading to poor generalization to new states encountered during planning.\n",
        "\n",
        "In contrast, simpler models with fewer parameters (e.g., fitting a few coefficients in a known physics model) are less prone to overfitting because they have less flexibility. This is why system identification often works well in robotics: the models are simpler, and the parameter space is smaller, reducing the risk of significant distributional shift.\n"
      ],
      "metadata": {
        "id": "FrUtx9d1VkZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Addressing Distributional Shift\n",
        "\n",
        "To mitigate the effects of distributional shift, several strategies can be employed:\n",
        "\n",
        "1. **Regularization**: Apply regularization techniques to prevent the model from overfitting to the training data.\n",
        "2. **Data Augmentation**: Collect more diverse data to better cover the state space.\n",
        "3. **Model Uncertainty**: Incorporate model uncertainty into planning, using methods such as ensemble models or Bayesian approaches to account for the model's confidence in its predictions.\n"
      ],
      "metadata": {
        "id": "ZgU-v3hZVqyE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=55162&format=png&color=000000\" style=\"height:50px;display:inline\"> Improving Model-Based Reinforcement Learning\n",
        "\n",
        "\n",
        "\n",
        "The distributional shift problem discussed earlier is a significant challenge for model-based reinforcement learning (RL). However, there are strategies to mitigate this issue and improve the performance of model-based RL algorithms. Here, we will discuss how to enhance the basic model-based RL algorithm to address distributional shift and introduce more robust methods like Model Predictive Control (MPC).\n",
        "\n"
      ],
      "metadata": {
        "id": "nmaGNsqFW5fL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improving the Basic Model-Based RL Algorithm\n",
        "\n",
        "To address distributional shift, we can enhance the basic algorithm by iteratively collecting data, training the model, and planning actions. This approach is conceptually similar to the DAgger (Dataset Aggregation) method used in imitation learning, where we iteratively collect new data to match the state distribution of a learned policy."
      ],
      "metadata": {
        "id": "pXt3o9d1W-XE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This iterative loop helps the model adapt to new states encountered during execution, reducing the impact of distributional shift. However, while this method conceptually mitigates distributional shift, it still has limitations in practice, such as requiring extensive data collection and retraining."
      ],
      "metadata": {
        "id": "6Ea85eneXGFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To further improve, we can adopt **Model Predictive Control (MPC)**, a more advanced approach that involves frequent re-planning to correct mistakes as soon as they occur. This method, referred to as Model-Based RL Version 1.5, enhances the robustness of the algorithm by planning actions at each time step based on the latest state information.\n",
        "\n",
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/MB_model-based-1.5.png?raw=true' width=900/>"
      ],
      "metadata": {
        "id": "kqFaLnFAXJfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Advantages of MPC:\n",
        "\n",
        "1. **Immediate Correction**: By re-planning at each time step, the agent can immediately correct any mistakes, making the system more robust to model errors.\n",
        "2. **Reduced Dependency on Perfect Models**: Since the agent frequently re-plans, it can handle less accurate models better than the naive approach.\n",
        "3. **Shorter Planning Horizons**: Frequent re-planning allows the use of shorter planning horizons, reducing computational complexity and enabling faster adaptation."
      ],
      "metadata": {
        "id": "jgq9inteXY5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example: Driving a Car\n",
        "\n",
        "Consider a scenario where the agent is driving a car. The initial model predicts that steering slightly to the left will keep the car going straight. However, in reality, this causes the car to veer left. With MPC, as soon as the agent observes the deviation, it re-plans to correct the steering, ensuring the car stays on course.\n"
      ],
      "metadata": {
        "id": "AQ4Rg0ugXu9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limitations of Model Predictive Control (MPC) and Open-Loop Control\n",
        "\n",
        "### Open-Loop Control\n",
        "\n",
        "Open-loop control refers to planning and committing to a fixed sequence of actions without adjusting based on new observations. The control strategy is predetermined and does not adapt to the changing state of the environment.\n",
        "\n",
        "#### Suboptimality of Open-Loop Control\n",
        "\n",
        "1. **Lack of Adaptability**: Since the actions are predetermined, the strategy cannot respond to new information or changes in the environment. This can lead to suboptimal decisions when unexpected situations arise.\n",
        "2. **Illustrative Example**: Consider an agent faced with a math test. If the agent must decide whether to take the test and answer it without seeing the actual test questions, it has to commit to an action without sufficient information. The optimal strategy would involve first observing the test questions and then deciding how to proceed. An open-loop strategy would fail in such scenarios.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IdaZt5YpmWNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specific Open-Loop Planning Methods"
      ],
      "metadata": {
        "id": "uEPgK1vNoOKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Random Shooting\n",
        "\n",
        "- **Method**: Generate a large number of random action sequences, simulate the outcomes using the model, and select the sequence that yields the highest expected reward.\n",
        "- **Limitations**: While simple, random shooting can be computationally expensive and inefficient, especially in high-dimensional action spaces. It also does not account for future state observations, making it inherently open-loop.\n",
        "\n"
      ],
      "metadata": {
        "id": "0-Dmn9I1m6hR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cross-Entropy Method\n",
        "\n",
        "- **Method**: This is an optimization technique where multiple action sequences are sampled, and the top-performing sequences are used to update the distribution from which new sequences are sampled. The process is repeated until convergence.\n",
        "- **Limitations**: Although more efficient than random shooting, the cross-entropy method still plans in an open-loop manner, optimizing a fixed sequence of actions without adapting to new observations during execution.\n",
        "\n",
        "Both of these methods optimize for the expected reward given a sequence of actions but do not adjust based on real-time state changes. This lack of adaptability is a fundamental limitation, particularly in dynamic environments where the optimal action depends on the latest state information."
      ],
      "metadata": {
        "id": "xp5SyDN0m9sA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=oNOWJS4XHflp&format=png&color=000000\" style=\"height:50px;display:inline\"> Transitioning to Closed-Loop Control\n",
        "\n",
        "\n",
        "To overcome the limitations of open-loop control, we need to shift to closed-loop control, where the agent continuously observes the state and updates its policy accordingly. Closed-loop control strategies, unlike open-loop strategies, adapt to new information in real-time, leading to more optimal decision-making.\n",
        "\n",
        "### Closed-Loop Control\n",
        "\n",
        "In closed-loop control, the agent’s actions are determined by a policy that takes the current state as input and outputs the next action. This allows the agent to adjust its actions based on the latest observations, making it more responsive and adaptable to changes in the environment.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2GT_xmTAoAOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By transitioning from open-loop to closed-loop control, model-based RL can achieve more adaptive and effective policies. This approach aligns closely with the original reinforcement learning problem, where the objective is to develop a policy that optimizes actions for any given state. Leveraging learned models provides a powerful tool for enhancing policy learning and achieving better performance in complex environments."
      ],
      "metadata": {
        "id": "iTsMGplqpIAd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advantages of Closed-Loop Control\n",
        "\n",
        "1. **Real-Time Adaptation**: The agent can adapt its actions based on the current state, allowing it to handle unexpected situations more effectively.\n",
        "2. **Improved Performance**: By continuously updating the policy, the agent can make more informed decisions, leading to better overall performance compared to open-loop control.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZjztuNMwoFAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Concepts in Closed-Loop Control\n",
        "\n",
        "1. **Policy-Based Methods**: Instead of planning a fixed sequence of actions, the agent develops a policy that dictates actions based on the current state.\n",
        "2. **Model Integration**: The agent models the environment’s dynamics explicitly, which aids in policy learning and decision-making.\n",
        "3. **Global Policies**: Using highly expressive function approximators like neural networks, the agent can learn global policies that perform well across various states.\n",
        "\n"
      ],
      "metadata": {
        "id": "v2QSWrpdoGjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example: Math Test Problem Resolved\n",
        "\n",
        "In a closed-loop control scenario, the agent would first observe the test before deciding to answer. This adaptive strategy ensures the agent can choose the optimal action based on the current state, leading to better overall performance."
      ],
      "metadata": {
        "id": "i6hu6FuooHu_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model-Based RL with Backpropagation\n",
        "\n",
        "In model-based reinforcement learning (RL), we aim to learn policies that maximize reward by leveraging models of the environment's dynamics. A natural idea is to apply the tools of deep learning, such as backpropagation and gradient descent, to optimize policies. This involves setting up a computation graph that allows us to compute the total reward for a given policy and use gradient-based methods to optimize this policy."
      ],
      "metadata": {
        "id": "5t-se4UIqfBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up the Computation Graph\n",
        "\n",
        "To maximize the total reward, we set up a computation graph with the following components:\n",
        "\n",
        "1. **Policies**: Functions that take states as input and produce actions.\n",
        "2. **Dynamics**: Functions that take states and actions as input and produce the next state.\n",
        "3. **Rewards**: Functions that take states and actions as input and produce scalar reward values.\n"
      ],
      "metadata": {
        "id": "-05ZG59trQwH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/MB_backprop.png?raw=true' width=900/>\n",
        "\n",
        "This computation graph allows us to compute the sum of rewards for a given policy. By leveraging automatic differentiation software, we can compute gradients and perform gradient ascent to optimize the policy parameters.\n"
      ],
      "metadata": {
        "id": "Qeux0JzyrU7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why This Approach Might Not Work\n",
        "\n",
        "While setting up the computation graph and applying backpropagation seems straightforward, this approach often fails in practice due to several reasons:\n",
        "\n"
      ],
      "metadata": {
        "id": "x9ofY4Jvrv0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Temporal Structure and Compounding Effects\n",
        "\n",
        "- In trajectory optimization, actions taken earlier in a trajectory have compounding effects on later states and rewards. This results in large gradients for early actions and smaller gradients for later actions.\n",
        "- This situation leads to an ill-conditioned optimization problem, where some parameters receive very large gradient updates while others receive very small updates.\n",
        "\n"
      ],
      "metadata": {
        "id": "zVfcP68oryP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Parameter Sensitivity\n",
        "\n",
        "- Small changes in actions at the beginning of a trajectory can lead to significant changes in the trajectory's outcome. This sensitivity makes it challenging to optimize policies effectively.\n",
        "\n"
      ],
      "metadata": {
        "id": "82oPq1Bmrz-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vanishing and Exploding Gradients\n",
        "\n",
        "- The problems faced when optimizing policies through backpropagation are similar to those encountered in training Recurrent Neural Networks (RNNs) naively. The derivatives of later rewards with respect to earlier policy parameters involve the product of many Jacobians.\n",
        "- If these Jacobians have eigenvalues significantly different from one, the gradients can either explode (if eigenvalues are larger than one) or vanish (if eigenvalues are smaller than one)."
      ],
      "metadata": {
        "id": "ZWmoqItvr1ql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution: Leveraging Model-Based Acceleration for Policy Learning\n",
        "\n",
        "Given the challenges associated with backpropagation in model-based RL, a practical solution involves using the learned model to generate synthetic samples to accelerate model-free RL algorithms. This approach may initially seem counterintuitive, as it essentially treats the learned model as a simulator rather than directly exploiting its known derivatives. However, this method, known as model-based acceleration, can significantly enhance the efficiency and effectiveness of model-free RL training.\n"
      ],
      "metadata": {
        "id": "ZIvRqPwcsoN8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img src=\"https://img.icons8.com/?size=50&id=46554&format=png&color=000000\" style=\"height:50px;display:inline\"> Model-Free Learning with a Model\n"
      ],
      "metadata": {
        "id": "r0S2vJuX3YSv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building on the previous discussion, we now introduce a refined approach to model-based reinforcement learning (RL), referred to as Model-Based RL Version 2.5. This method combines elements of model-free learning with the benefits of having a learned model, addressing some of the challenges encountered with direct backpropagation through the model.\n",
        "\n",
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/MB_model-based-2.5.png?raw=true' width=900/>"
      ],
      "metadata": {
        "id": "qTiFrXGM3bPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model-Based RL Version 2.5 aims to improve the policy by using a learned dynamics model to generate synthetic experiences. Instead of relying on backpropagation through the model, this approach uses policy gradient methods, thereby mitigating some of the difficulties associated with optimizing policies directly via backpropagation."
      ],
      "metadata": {
        "id": "7hSYXNF23fWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steps Involved\n",
        "\n",
        "### Data Collection\n",
        "\n",
        "- Run an initial policy to collect a dataset of state transitions.\n",
        "\n",
        "### Model Learning\n",
        "\n",
        "- Train a dynamics model using the collected data.\n",
        "\n",
        "### Trajectory Sampling\n",
        "\n",
        "- Use the learned dynamics model to generate a large number of synthetic trajectories with the current policy.\n",
        "\n",
        "### Policy Improvement\n",
        "\n",
        "- Apply policy gradient methods to the sampled trajectories to improve the policy. Techniques such as actor-critic methods can be used to enhance this step.\n",
        "\n",
        "### Iteration\n",
        "\n",
        "- Repeat the trajectory sampling and policy improvement steps several times, using the model to simulate new trajectories without generating additional real data.\n",
        "\n",
        "### Data Augmentation\n",
        "\n",
        "- Once the policy has improved sufficiently, run it in the real environment to collect more data. Append this new data to the dataset and retrain the dynamics model with the expanded dataset.\n",
        "\n",
        "This iterative process helps refine the policy using both real and synthetic data, enhancing the agent’s performance over time.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rF8hV7RB4ajA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benefits and Challenges\n",
        "\n",
        "### Benefits\n",
        "\n",
        "1. **Avoiding Backpropagation Issues**: By using policy gradient methods instead of direct backpropagation through the model, this approach avoids problems like vanishing and exploding gradients, making the optimization process more stable.\n",
        "2. **Data Efficiency**: The use of synthetic trajectories generated by the learned model allows for more efficient use of data, reducing the need for extensive real-world interactions.\n",
        "\n"
      ],
      "metadata": {
        "id": "j_fb6fFb4cn9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Challenges\n",
        "\n",
        "1. **Model Accuracy**: The effectiveness of this approach heavily depends on the accuracy of the learned dynamics model. If the model's predictions are inaccurate, the synthetic trajectories might not represent the real environment accurately, leading to suboptimal policy updates.\n",
        "2. **Exploration**: Since the model generates synthetic data based on the current policy, it might not explore the state space as thoroughly as needed, potentially missing critical areas that require real-world data collection."
      ],
      "metadata": {
        "id": "JoeL8z6M4eOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=FXi8HsuaMBHf&format=png&color=000000\" style=\"height:50px;display:inline\"> The Curse of Long Model-Based Rollouts\n",
        "\n",
        "\n",
        "One of the significant challenges in model-based reinforcement learning (RL) is dealing with the inaccuracies that arise from using learned models over long horizons. This issue is commonly referred to as the \"curse of long model-based rollouts.\" To understand this problem, let's revisit some key concepts and explore how errors accumulate during extended model-based rollouts.\n"
      ],
      "metadata": {
        "id": "vSvtTmXS5-IK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Imitation Learning and Distributional Shift\n",
        "\n",
        "In imitation learning, a policy trained via supervised learning may make small mistakes when executed, leading to deviations from the states observed during training. These deviations place the policy in unfamiliar situations, causing compounding errors. This phenomenon is known as distributional shift.\n",
        "\n",
        "### Similar Issues in Model-Based RL\n",
        "\n",
        "In model-based RL, the learned dynamics model is used to simulate the environment. However, like the learned policy in imitation learning, the learned model can make mistakes. These mistakes cause the simulated states to diverge from the real states, and the errors compound over time.\n",
        "\n",
        "When the policy is optimized using the learned model, any inaccuracies in the model can lead to suboptimal policy updates. This is particularly problematic when long rollouts are used, as the cumulative errors can significantly distort the policy's performance.\n"
      ],
      "metadata": {
        "id": "SF_yhfLm6FOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exacerbation by Policy Updates\n",
        "\n",
        "The issue is further exacerbated when the policy is improved based on the learned model:\n",
        "\n",
        "1. **Changing Policy**: In Model-Based RL Version 2.5, the policy is iteratively improved using synthetic data generated by the learned model. Each update to the policy can introduce new states that the model hasn't accurately learned, leading to greater distributional shift.\n",
        "2. **Distributional Shift**: The distribution of states encountered by the updated policy differs even more from those seen during the initial data collection, worsening the model's accuracy and further compounding errors in long rollouts.\n"
      ],
      "metadata": {
        "id": "3qAaPLrD7NZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=cGIDLkSsAuf3&format=png&color=000000\" style=\"height:50px;display:inline\"> Model-Based RL with Short Rollouts\n",
        "\n",
        "\n",
        "\n",
        "Given the challenges of long model-based rollouts due to accumulating errors, one effective strategy is to use short rollouts. This approach can mitigate the error accumulation problem while still benefiting from model-based learning."
      ],
      "metadata": {
        "id": "8Z11vl6N9HTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**By limiting the length of model-based rollouts, we can reduce the accumulated error**. For example, if the task has a horizon of 1000 steps, using rollouts of only 50 steps can significantly lower the error. However, simply reducing the rollout length changes the nature of the problem since tasks with long horizons might have critical events occurring later in the trajectory that short rollouts would miss. For instance, a robot cooking a meal might need more than five minutes to complete significant actions."
      ],
      "metadata": {
        "id": "NnskC6CU9x98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hybrid Approach\n",
        "\n",
        "A hybrid approach involves collecting full-length trajectories from the real environment infrequently and using the real-world states as starting points for short model-based rollouts. This ensures that the model still encounters later stages of the task without relying on long rollouts. The trade-offs include having much lower error due to the short rollouts while ensuring comprehensive state coverage by sampling states uniformly from real-world trajectories. This way, the agent can still see states from all time steps."
      ],
      "metadata": {
        "id": "_lVmE7b696bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### State Distribution Mismatch\n",
        "\n",
        "One issue with this approach is the state distribution mismatch. **When the policy is updated, the new policy will encounter different states than the ones seen during data collection**. If short rollouts are started from real-world states, the initial policy collected the data, and the new policy is being tested on these states, leading to a mismatch. The resulting state distribution is a mix of the distribution from the data-collecting policy and the new policy, which can be problematic. If the policy changes are small, the impact of the mismatch is minimal, and advanced policy gradient methods can still be effective. However, when significant policy changes are made between data collection rounds, the mismatch can degrade the performance of on-policy methods like policy gradient algorithms."
      ],
      "metadata": {
        "id": "R3EFZdVo-Cxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Practical Implementation: Model-Based RL Version 3.0\n",
        "\n",
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/MB_model-based_3.0.png?raw=true' width=900/>\n",
        "\n",
        "Model-Based RL Version 3.0, which uses short rollouts, aligns more closely with practical methods used in the field. The process typically involves:\n",
        "\n",
        "#### Data Collection\n",
        "- Collect real-world data and use it to train a dynamics model.\n",
        "\n",
        "#### State Sampling\n",
        "- Sample states from the real-world data, possibly uniformly at random across the trajectory.\n",
        "\n",
        "#### Short Model-Based Rollouts\n",
        "- Perform short rollouts from these sampled states using the learned model. These rollouts can be as short as one time step but are typically around 10 time steps.\n",
        "\n",
        "#### Policy Improvement\n",
        "- Use both real data and synthetic data from the model to improve the policy. Off-policy algorithms like Q-learning or actor-critic methods are often employed.\n",
        "\n",
        "#### Iteration\n",
        "- Generate more data from the model, then run the improved policy in the real environment to collect additional real data. Append this new data to the dataset and retrain the model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OxzXzFa8-ZjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Design Considerations\n",
        "\n",
        "1. **Balance of Real and Synthetic Data**: The method involves delicate decisions regarding the proportion of real versus synthetic data used for training.\n",
        "2. **Frequency of Policy Updates**: The frequency and extent of policy updates between data collection episodes need careful tuning to maintain performance."
      ],
      "metadata": {
        "id": "KNGpvh6f-blE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img src=\"https://img.icons8.com/?size=50&id=lWvRA05d4v6N&format=png&color=000000\" style=\"height:50px;display:inline\"> Dyna: Practical Model-Based RL Algorithms\n",
        "\n",
        "\n",
        "In this section, we explore practical model-based reinforcement learning (RL) algorithms that build upon the framework of Model-Based RL Version 3.0. Specifically, we focus on the Dyna algorithm, a classic approach that effectively integrates model-based and model-free techniques to improve learning efficiency.\n",
        "\n",
        "Dyna, introduced by Richard Sutton in the 1990s, exemplifies an approach that enhances online Q-learning with model-based rollouts. The algorithm uses very short rollouts, often just one time step, to leverage the learned model and improve data efficiency. Despite its simplicity, Dyna can provide significant benefits if a good model is learned."
      ],
      "metadata": {
        "id": "SyTROhVYjH8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Steps in Dyna\n",
        "\n",
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/MB_dyna.png?raw=true' width=900/>\n",
        "\n",
        "### Action Selection\n",
        "\n",
        "- In the current state, pick an action $a$ using an exploration policy. This step is identical to standard online Q-learning.\n",
        "\n",
        "### Transition Observation\n",
        "\n",
        "- Observe the resulting next state $s'$ and the reward $r$, forming a transition tuple $(s,a,s')$.\n",
        "\n",
        "### Model and Reward Function Update\n",
        "\n",
        "- Update the dynamics model and reward function using the observed transition. In the original Dyna, this was done online with a single step of gradient descent or by mixing old values in a tabular model with new observations.\n",
        "\n",
        "### Q-Learning Update\n",
        "\n",
        "- Perform a standard Q-learning update using the observed transition.\n",
        "\n",
        "### Model-Based Rollouts\n",
        "\n",
        "- Repeat a model-based procedure $K$ times (where $K$ is a hyperparameter):\n",
        "  1. Sample a state-action pair from the buffer of previous experiences.\n",
        "  2. Use the learned model to simulate the next state and reward for this pair.\n",
        "  3. Perform Q-learning updates using these simulated transitions.\n",
        "\n",
        "This procedure allows Dyna to incorporate simulated experiences, generated by the learned model, into the Q-learning process, thereby improving the policy without relying solely on real-world interactions.\n"
      ],
      "metadata": {
        "id": "8yBOXq2jjo_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Design Choices and Variations\n",
        "\n",
        "Dyna makes several design choices that can be adjusted for different applications. The original Dyna samples state-action pairs from a buffer of previous experiences, but an alternative is to sample actions according to the latest policy, such as the argmax policy for the Q function. While Dyna performs a single update step for the model, multiple steps could be used to improve model accuracy, especially in deterministic systems where the same state-action pair should always produce the same next state. Dyna's design is optimized for highly stochastic systems. By using the state-action pairs directly from the buffer, it avoids distributional shift issues, making it statistically safer. These design choices are not rigid and can be adapted based on the specific characteristics of the environment and the goals of the learning task."
      ],
      "metadata": {
        "id": "QNr2aH3Xjjo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benefits of Dyna\n",
        "\n",
        "### Data Efficiency\n",
        "\n",
        "- By incorporating model-based rollouts, Dyna significantly reduces the number of real-world interactions needed to improve the policy, enhancing data efficiency.\n",
        "\n",
        "### Flexibility\n",
        "\n",
        "- Dyna can be implemented with various off-policy RL methods, such as Q-learning and Q-function actor-critic methods, providing flexibility in its application.\n",
        "\n",
        "### Robustness\n",
        "\n",
        "- The use of short rollouts minimizes error accumulation, making the algorithm more robust to inaccuracies in the learned model.\n"
      ],
      "metadata": {
        "id": "0wjLfYOMkQbi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=_lFKH2HByz22&format=png&color=000000\" style=\"height:50px;display:inline\"> Dyna-Q\n",
        "\n",
        "In this section, we will implement Dyna-Q, one of the simplest model-based reinforcement learning algorithms. A Dyna-Q agent combines acting, learning, and planning. The first two components -- acting and learning -- are just like what we have studied previously. Q-learning, for example, learns by acting in the world, and therefore combines acting and learning. But a Dyna-Q agent also implements planning, or simulating experiences from a model--and learns from them."
      ],
      "metadata": {
        "id": "UyDzEkmKnYEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most common way in which the Dyna-Q agent is implemented is by adding a planning routine to a Q-learning agent: after the agent acts in the real world and learns from the observed experience, the agent is allowed a series of $k$ *planning steps*. At each one of those $k$ planning steps, the model generates a simulated experience by randomly sampling from the history of all previously experienced state-action pairs. The agent then learns from this simulated experience, again using the same Q-learning rule that you implemented for learning from real experience. This simulated experience is simply a one-step transition, i.e., a state, an action, and the resulting state and reward. So, in practice, a Dyna-Q agent learns (via Q-learning) from one step of **real** experience during acting, and then from k steps of **simulated** experience during planning."
      ],
      "metadata": {
        "id": "AhVie_Rrnguj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "YRWHYwlrAr9Q"
      },
      "source": [
        "---\n",
        "**TABULAR DYNA-Q**\n",
        "\n",
        "Initialize $Q(s,a)$ and $Model(s,a)$ for all $s \\in S$ and $a \\in A$.\n",
        "\n",
        "Loop forever:\n",
        "\n",
        "> (a) $S$ &larr; current (nonterminal) state <br>\n",
        "> (b) $A$ &larr; $\\epsilon$-greedy$(S,Q)$ <br>\n",
        "> (c) Take action $A$; observe resultant reward, $R$, and state, $S'$ <br>\n",
        "> (d) $Q(S,A)$ &larr; $Q(S,A) + \\alpha \\left[R + \\gamma \\max_{a} Q(S',a) - Q(S,A)\\right]$ <br>\n",
        "> (e) $Model(S,A)$ &larr; $R,S'$ (assuming deterministic environment) <br>\n",
        "> (f) Loop repeat $k$ times: <br>\n",
        ">> $S$ &larr; random previously observed state <br>\n",
        ">> $A$ &larr; random action previously taken in $S$ <br>\n",
        ">> $R,S'$ &larr; $Model(S,A)$ <br>\n",
        ">> $Q(S,A)$ &larr; $Q(S,A) + \\alpha \\left[R + \\gamma \\max_{a} Q(S',a) - Q(S,A)\\right]$ <br>\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's one final detail about this algorithm: where does the simulated experiences come from or, in other words, what is the \"model\"? In Dyna-Q, as the agent interacts with the environment, the agent also learns the model. For simplicity, Dyna-Q implements model-learning in an almost trivial way, as simply caching the results of each transition. Thus, after each one-step transition in the environment, the agent saves the results of this transition in a big matrix, and consults that matrix during each of the planning steps. Obviously, this model-learning strategy only makes sense if the world is deterministic (so that each state-action pair always leads to the same state and reward), and this is the setting of the exercise below. However, even this simple setting can already highlight one of Dyna-Q major strengths: the fact that the planning is done at the same time as the agent interacts with the environment, which means that new information gained from the interaction may change the model and thereby interact with planning in potentially interesting ways."
      ],
      "metadata": {
        "id": "40RgaoJSn7BG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since you already implemented Q-learning in the previous tutorial, we will focus here on the extensions new to Dyna-Q: the model update step and the planning step."
      ],
      "metadata": {
        "id": "zgc1XJuroDCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy(q, epsilon):\n",
        "  \"\"\"Epsilon-greedy policy: selects the maximum value action with probabilty\n",
        "  (1-epsilon) and selects randomly with epsilon probability.\n",
        "\n",
        "  Args:\n",
        "    q (ndarray): an array of action values\n",
        "    epsilon (float): probability of selecting an action randomly\n",
        "\n",
        "  Returns:\n",
        "    int: the chosen action\n",
        "  \"\"\"\n",
        "  be_greedy = np.random.random() > epsilon\n",
        "  if be_greedy:\n",
        "    action = np.argmax(q)\n",
        "  else:\n",
        "    action = np.random.choice(len(q))\n",
        "\n",
        "  return action\n",
        "\n",
        "\n",
        "def q_learning(state, action, reward, next_state, value, params):\n",
        "  \"\"\"Q-learning: updates the value function and returns it.\n",
        "\n",
        "  Args:\n",
        "    state (int): the current state identifier\n",
        "    action (int): the action taken\n",
        "    reward (float): the reward received\n",
        "    next_state (int): the transitioned to state identifier\n",
        "    value (ndarray): current value function of shape (n_states, n_actions)\n",
        "    params (dict): a dictionary containing the default parameters\n",
        "\n",
        "  Returns:\n",
        "    ndarray: the updated value function of shape (n_states, n_actions)\n",
        "  \"\"\"\n",
        "  # value of previous state-action pair\n",
        "  prev_value = value[int(state), int(action)]\n",
        "\n",
        "  # maximum Q-value at current state\n",
        "  if next_state is None or np.isnan(next_state):\n",
        "      max_value = 0\n",
        "  else:\n",
        "      max_value = np.max(value[int(next_state)])\n",
        "\n",
        "  # reward prediction error\n",
        "  delta = reward + params['gamma'] * max_value - prev_value\n",
        "\n",
        "  # update value of previous state-action pair\n",
        "  value[int(state), int(action)] = prev_value + params['alpha'] * delta\n",
        "\n",
        "  return value"
      ],
      "metadata": {
        "id": "LJrf-kymoDyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/task_sign.png?raw=true' width=800/>"
      ],
      "metadata": {
        "id": "d1Hytj2Poo08"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <img src=\"https://img.icons8.com/?size=50&id=46589&format=png&color=000000\" style=\"height:30px;display:inline\"> Task: Dyna-Q Model Update\n",
        "\n",
        "In this exercise you will implement the model update portion of the Dyna-Q algorithm. More specifically, after each action that the agent executes in the world, we need to update our model to remember what reward and next state we last experienced for the given state-action pair."
      ],
      "metadata": {
        "id": "jKsTJurEoTtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dyna_q_model_update(model, state, action, reward, next_state):\n",
        "  \"\"\" Dyna-Q model update\n",
        "\n",
        "  Args:\n",
        "    model (ndarray): An array of shape (n_states, n_actions, 2) that represents\n",
        "                     the model of the world i.e. what reward and next state do\n",
        "                     we expect from taking an action in a state.\n",
        "    state (int): the current state identifier\n",
        "    action (int): the action taken\n",
        "    reward (float): the reward received\n",
        "    next_state (int): the transitioned to state identifier\n",
        "\n",
        "  Returns:\n",
        "    ndarray: the updated model\n",
        "  \"\"\"\n",
        "  ###############################################################\n",
        "  ## TODO for students: implement the model update step of Dyna-Q\n",
        "  # Fill out function and remove\n",
        "  raise NotImplementedError(\"Student exercise: implement the model update step of Dyna-Q\")\n",
        "  ###############################################################\n",
        "\n",
        "  # Update our model with the observed reward and next state\n",
        "  model[...] = ...\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "bTC-BVz-ox1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <img src=\"https://img.icons8.com/?size=50&id=42816&format=png&color=000000\" style=\"height:30px;display:inline\">  Solution\n",
        "\n"
      ],
      "metadata": {
        "id": "y3sRnN289aRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dyna_q_model_update(model, state, action, reward, next_state):\n",
        "  \"\"\" Dyna-Q model update\n",
        "\n",
        "  Args:\n",
        "    model (ndarray): An array of shape (n_states, n_actions, 2) that represents\n",
        "                     the model of the world i.e. what reward and next state do\n",
        "                     we expect from taking an action in a state.\n",
        "    state (int): the current state identifier\n",
        "    action (int): the action taken\n",
        "    reward (float): the reward received\n",
        "    next_state (int): the transitioned to state identifier\n",
        "\n",
        "  Returns:\n",
        "    ndarray: the updated model\n",
        "  \"\"\"\n",
        "  # Update our model with the observed reward and next state\n",
        "  model[state, action] = reward, next_state\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "0OlffUD4ozcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "4_AuDDdXAr9R"
      },
      "source": [
        "Now that we have a way to update our model, we can use it in the planning phase of Dyna-Q to simulate past experiences."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def learn_environment(env, model_updater, planner, params, max_steps,\n",
        "                      n_episodes, shortcut_episode=None):\n",
        "  # Start with a uniform value function\n",
        "  value = np.ones((env.n_states, env.n_actions))\n",
        "\n",
        "  # Run learning\n",
        "  reward_sums = np.zeros(n_episodes)\n",
        "  episode_steps = np.zeros(n_episodes)\n",
        "\n",
        "  # Dyna-Q state\n",
        "  model = np.nan*np.zeros((env.n_states, env.n_actions, 2))\n",
        "\n",
        "  # Loop over episodes\n",
        "  for episode in range(n_episodes):\n",
        "    if shortcut_episode is not None and episode == shortcut_episode:\n",
        "      env.toggle_shortcut()\n",
        "      state = 64\n",
        "      action = 1\n",
        "      next_state, reward = env.get_outcome(state, action)\n",
        "      model[state, action] = reward, next_state\n",
        "      value = q_learning(state, action, reward, next_state, value, params)\n",
        "\n",
        "\n",
        "    state = env.init_state  # initialize state\n",
        "    reward_sum = 0\n",
        "\n",
        "    for t in range(max_steps):\n",
        "      # choose next action\n",
        "      action = epsilon_greedy(value[state], params['epsilon'])\n",
        "\n",
        "      # observe outcome of action on environment\n",
        "      next_state, reward = env.get_outcome(state, action)\n",
        "\n",
        "      # sum rewards obtained\n",
        "      reward_sum += reward\n",
        "\n",
        "      # update value function\n",
        "      value = q_learning(state, action, reward, next_state, value, params)\n",
        "\n",
        "      # update model\n",
        "      model = model_updater(model, state, action, reward, next_state)\n",
        "\n",
        "      # execute planner\n",
        "      value = planner(model, value, params)\n",
        "\n",
        "      if next_state is None:\n",
        "        break  # episode ends\n",
        "      state = next_state\n",
        "\n",
        "    reward_sums[episode] = reward_sum\n",
        "    episode_steps[episode] = t+1\n",
        "\n",
        "  return value, reward_sums, episode_steps"
      ],
      "metadata": {
        "id": "uBx5p_L_pgsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <img src=\"https://img.icons8.com/?size=50&id=46589&format=png&color=000000\" style=\"height:30px;display:inline\"> Task: Dyna-Q Planning\n",
        "\n",
        "In this exercise you will implement the other key part of Dyna-Q: planning. We will sample a random state-action pair from those we've experienced, use our model to simulate the experience of taking that action in that state, and update our value function using Q-learning with these simulated state, action, reward, and next state outcomes. Furthermore, we want to run this planning step $k$ times, which can be obtained from `params['k']`.\n",
        "\n",
        "For this exercise, you may use the `q_learning` function to handle the Q-learning value function update. Recall that the method signature is `q_learning(state, action, reward, next_state, value, params)` and it returns the updated `value` table.\n",
        "\n",
        "After completing this function, we have a way to update our model and a means to use it in planning so we will see it in action. The code sets up our agent parameters and learning environment, then passes your model update and planning methods to the agent to try and solve Quentin's World. Notice that we set the number of planning steps $k=10$."
      ],
      "metadata": {
        "id": "5QO30oOzo-Em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dyna_q_planning(model, value, params):\n",
        "  \"\"\" Dyna-Q planning\n",
        "\n",
        "  Args:\n",
        "    model (ndarray): An array of shape (n_states, n_actions, 2) that represents\n",
        "                     the model of the world i.e. what reward and next state do\n",
        "                     we expect from taking an action in a state.\n",
        "    value (ndarray): current value function of shape (n_states, n_actions)\n",
        "    params (dict): a dictionary containing learning parameters\n",
        "\n",
        "  Returns:\n",
        "    ndarray: the updated value function of shape (n_states, n_actions)\n",
        "  \"\"\"\n",
        "  ############################################################\n",
        "  ## TODO for students: implement the planning step of Dyna-Q\n",
        "  # Fill out function and remove\n",
        "  raise NotImplementedError(\"Student exercise: implement the planning step of Dyna-Q\")\n",
        "  #############################################################\n",
        "  # Perform k additional updates at random (planning)\n",
        "  for _ in range(...):\n",
        "    # Find state-action combinations for which we've experienced a reward i.e.\n",
        "    # the reward value is not NaN. The outcome of this expression is an Nx2\n",
        "    # matrix, where each row is a state and action value, respectively.\n",
        "    candidates = np.array(np.where(~np.isnan(model[:,:,0]))).T\n",
        "\n",
        "    # Write an expression for selecting a random row index from our candidates\n",
        "    idx = ...\n",
        "\n",
        "    # Obtain the randomly selected state and action values from the candidates\n",
        "    state, action = ...\n",
        "\n",
        "    # Obtain the expected reward and next state from the model\n",
        "    reward, next_state = ...\n",
        "\n",
        "    # Update the value function using Q-learning\n",
        "    value = ...\n",
        "\n",
        "  return value\n",
        "\n",
        "\n",
        "# set for reproducibility, comment out / change seed value for different results\n",
        "np.random.seed(1)\n",
        "\n",
        "# parameters needed by our policy and learning rule\n",
        "params = {\n",
        "  'epsilon': 0.05,  # epsilon-greedy policy\n",
        "  'alpha': 0.5,  # learning rate\n",
        "  'gamma': 0.8,  # temporal discount factor\n",
        "  'k': 10,  # number of Dyna-Q planning steps\n",
        "}\n",
        "\n",
        "# episodes/trials\n",
        "n_episodes = 500\n",
        "max_steps = 1000\n",
        "\n",
        "# environment initialization\n",
        "env = QuentinsWorld()\n",
        "\n",
        "# solve Quentin's World using Dyna-Q\n",
        "results = learn_environment(env, dyna_q_model_update, dyna_q_planning,\n",
        "                            params, max_steps, n_episodes)\n",
        "value, reward_sums, episode_steps = results\n",
        "\n",
        "# Plot the results\n",
        "plot_performance(env, value, reward_sums)"
      ],
      "metadata": {
        "id": "fFYonmQLpPf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "_gXyhy0WAr9S"
      },
      "source": [
        "After an initial warm-up phase of the first 20 episodes, we should see that the number of planning steps has a noticeable impact on our agent's ability to rapidly solve the environment. We should also notice that after a certain value of $k$ our relative utility goes down, so it's important to balance a large enough value of $k$ that helps us learn quickly without wasting too much time in planning."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <img src=\"https://img.icons8.com/?size=50&id=42816&format=png&color=000000\" style=\"height:30px;display:inline\">  Solution\n",
        "\n"
      ],
      "metadata": {
        "id": "bVHKPYRDpRv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def dyna_q_planning(model, value, params):\n",
        "  \"\"\" Dyna-Q planning\n",
        "\n",
        "  Args:\n",
        "    model (ndarray): An array of shape (n_states, n_actions, 2) that represents\n",
        "                     the model of the world i.e. what reward and next state do\n",
        "                     we expect from taking an action in a state.\n",
        "    value (ndarray): current value function of shape (n_states, n_actions)\n",
        "    params (dict): a dictionary containing learning parameters\n",
        "\n",
        "  Returns:\n",
        "    ndarray: the updated value function of shape (n_states, n_actions)\n",
        "  \"\"\"\n",
        "  # Perform k additional updates at random (planning)\n",
        "  for _ in range(params['k']):\n",
        "    # Find state-action combinations for which we've experienced a reward i.e.\n",
        "    # the reward value is not NaN. The outcome of this expression is an Nx2\n",
        "    # matrix, where each row is a state and action value, respectively.\n",
        "    candidates = np.array(np.where(~np.isnan(model[:,:,0]))).T\n",
        "\n",
        "    # Write an expression for selecting a random row index from our candidates\n",
        "    idx = np.random.choice(len(candidates))\n",
        "\n",
        "    # Obtain the randomly selected state and action values from the candidates\n",
        "    state, action = candidates[idx]\n",
        "\n",
        "    # Obtain the expected reward and next state from the model\n",
        "    reward, next_state = model[state, action]\n",
        "\n",
        "    # Update the value function using Q-learning\n",
        "    value = q_learning(state, action, reward, next_state, value, params)\n",
        "\n",
        "  return value\n",
        "\n",
        "\n",
        "# set for reproducibility, comment out / change seed value for different results\n",
        "np.random.seed(1)\n",
        "\n",
        "# parameters needed by our policy and learning rule\n",
        "params = {\n",
        "  'epsilon': 0.05,  # epsilon-greedy policy\n",
        "  'alpha': 0.5,  # learning rate\n",
        "  'gamma': 0.8,  # temporal discount factor\n",
        "  'k': 10,  # number of Dyna-Q planning steps\n",
        "}\n",
        "\n",
        "# episodes/trials\n",
        "n_episodes = 500\n",
        "max_steps = 1000\n",
        "\n",
        "# environment initialization\n",
        "env = QuentinsWorld()\n",
        "\n",
        "# solve Quentin's World using Dyna-Q\n",
        "results = learn_environment(env, dyna_q_model_update, dyna_q_planning,\n",
        "                            params, max_steps, n_episodes)\n",
        "value, reward_sums, episode_steps = results\n",
        "\n",
        "# Plot the results\n",
        "with plt.xkcd():\n",
        "  plot_performance(env, value, reward_sums)"
      ],
      "metadata": {
        "id": "iWrM8GdkpWoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "B-aAO1fAAr9S"
      },
      "source": [
        "### When the world changes...\n",
        "\n",
        "In addition to speeding up learning about a new environment, planning can also help the agent to quickly incorporate new information about the environment into its policy. Thus, if the environment changes (e.g. the rules governing the transitions between states, or the rewards associated with each state/action), the agent doesn't need to experience that change *repeatedly* (as would be required in a Q-learning agent) in real experience. Instead, planning allows that change to be incorporated quickly into the agent's policy, without the need to experience the change more than once.\n",
        "\n",
        "In this final section, we will again have our agents attempt to solve Quentin's World. However, after 200 episodes, a shortcut will appear in the environment.  We will test how a model-free agent using Q-learning and a Dyna-Q agent adapt to this change in the environment.\n",
        "\n",
        "<img alt=\"QuentinsWorldShortcut\" width=\"560\" height=\"560\" src=\"https://github.com/NeuromatchAcademy/course-content/blob/main/tutorials/static/W3D4_Tutorial4_QuentinsWorldShortcut.png?raw=true\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "iRmnLyyYAr9T"
      },
      "source": [
        "The following code again looks similar to what we've run previously. Just as above we will have multiple values for $k$, with $k=0$ representing our Q-learning agent and $k=10$ for our Dyna-Q agent with 10 planning steps. The main difference is we now add in an indicator as to when the shortcut appears. In particular, we will run the agents for 400 episodes, with the shortcut appearing in the middle after episode #200.\n",
        "\n",
        "When this shortcut appears we will also let each agent experience this change once i.e. we will evaluate the act of moving upwards when in the state that is below the now-open shortcut. After this single demonstration, the agents will continue on interacting in the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "TUojM2cSAr9T"
      },
      "outputs": [],
      "source": [
        "# set for reproducibility, comment out / change seed value for different results\n",
        "np.random.seed(1)\n",
        "\n",
        "# parameters needed by our policy and learning rule\n",
        "params = {\n",
        "  'epsilon': 0.05,  # epsilon-greedy policy\n",
        "  'alpha': 0.5,  # learning rate\n",
        "  'gamma': 0.8,  # temporal discount factor\n",
        "}\n",
        "\n",
        "# episodes/trials\n",
        "n_episodes = 400\n",
        "max_steps = 1000\n",
        "shortcut_episode = 200  # when we introduce the shortcut\n",
        "\n",
        "# number of planning steps\n",
        "planning_steps = np.array([0, 10]) # Q-learning, Dyna-Q (k=10)\n",
        "\n",
        "# environment initialization\n",
        "steps_per_episode = np.zeros((len(planning_steps), n_episodes))\n",
        "\n",
        "# Solve Quentin's World using Q-learning and Dyna-Q\n",
        "for i, k in enumerate(planning_steps):\n",
        "  env = QuentinsWorld()\n",
        "  params['k'] = k\n",
        "  results = learn_environment(env, dyna_q_model_update, dyna_q_planning,\n",
        "                              params, max_steps, n_episodes,\n",
        "                              shortcut_episode=shortcut_episode)\n",
        "  steps_per_episode[i] = results[2]\n",
        "\n",
        "\n",
        "# Plot results\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(steps_per_episode.T)\n",
        "ax.set(xlabel='Episode', ylabel='Steps per Episode',\n",
        "       xlim=[20,None], ylim=[0, 160])\n",
        "ax.axvline(shortcut_episode, linestyle=\"--\", color='gray', label=\"Shortcut appears\")\n",
        "ax.legend(('Q-learning', 'Dyna-Q', 'Shortcut appears'),\n",
        "          loc='upper right');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "AYJOMJ4pAr9U"
      },
      "source": [
        "If all went well, we should see the Dyna-Q agent having already achieved near optimal performance before the appearance of the shortcut and then immediately incorporating this new information to further improve. In this case, the Q-learning agent takes much longer to fully incorporate the new shortcut."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img src=\"https://img.icons8.com/?size=50&id=55422&format=png&color=000000\" style=\"height:50px;display:inline\"> Variants of Generalized Dyna Algorithms\n",
        "\n",
        "Several algorithms in the literature leverage the generalized Dyna approach, each with unique design decisions regarding the use of data for Q-learning and the integration of model-based rollouts. For instance, **Model-Based Policy Optimization (MBPO)** closely follows the procedure described, while **Model-Based Value Expansion (MBVE)** uses model rollouts to improve target value estimates without directly training the Q function. These methods share a fundamental recipe: collect transitions, update the model, perform model-based rollouts, and use the transitions to update the Q function. The primary advantage of these approaches is their sample efficiency, as they generate additional synthetic data to augment the real-world dataset, leading to faster learning. However, they also introduce potential biases, especially if the model is inaccurate or the state distribution becomes skewed. To mitigate these issues, techniques like model ensembles and frequent real-world data collection are used. Despite these challenges, model-based approaches typically achieve faster initial learning, though they may eventually plateau at a lower performance level due to model inaccuracies."
      ],
      "metadata": {
        "id": "ds6yURJUr5_V"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f175852a-f29b-430a-97b8-6565bc3a503f"
      },
      "source": [
        "# <img src=\"https://img.icons8.com/?size=100&id=46509&format=png&color=000000\" style=\"height:50px;display:inline\"> Conclusion\n",
        "---\n",
        "\n",
        "In this tutorial, we have learned about model-based reinforcement learning and implemented one of the simplest architectures of this type, Dyna-Q. Dyna-Q is very much like Q-learning, but instead of learning only from real experience, you also learn from **simulated** experience. This small difference, however, can have huge benefits! Planning *frees* the agent from the limitation of its own environment, and this in turn allows the agent to speed-up learning -- for instance, effectively incorporating environmental changes into one's policy.\n",
        "\n",
        "Not surprisingly, model-based RL is an active area of research. Some of the exciting topics in the frontier of the field involve (i) learning and representing a complex world model (i.e., beyond the tabular and deterministic case above), and (ii) what to simulate -- also known as search control -- (i.e., beyond the random selection of experiences implemented above).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_Ndmee2iL02"
      },
      "source": [
        "# <img src=\"https://img.icons8.com/dusk/64/000000/plus-2-math.png\" style=\"height:50px;display:inline\"> Further Reading\n",
        "---\n",
        "\n",
        "\n",
        "For those interested in diving deeper into model-based reinforcement learning, the following papers provide a comprehensive exploration of various approaches and innovations in the field:\n",
        "\n",
        "### Core Papers\n",
        "\n",
        "- **Deisenroth et al.**: [PILCO: A Model-Based and Data-Efficient Approach to Policy Search](https://link.springer.com/article/10.1007/s10994-013-9332-0). This paper presents a pioneering approach to policy search using probabilistic models to achieve data efficiency.\n",
        "- **Nagabandi et al.**: [Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning](https://arxiv.org/abs/1708.02596). This work explores the combination of neural network dynamics models with model-free fine-tuning to enhance performance.\n",
        "- **Chua et al.**: [Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models](https://arxiv.org/abs/1805.12114). This paper discusses the use of probabilistic dynamics models to achieve sample-efficient learning in complex environments.\n",
        "- **Feinberg et al.**: [Model-Based Value Expansion for Efficient Model-Free Reinforcement Learning](https://arxiv.org/abs/1803.00101). This study introduces Model-Based Value Expansion (MBVE) to improve the efficiency of model-free RL.\n",
        "- **Buckman et al.**: [Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion](https://arxiv.org/abs/1807.01675). This paper explores the use of ensemble methods to enhance the efficiency and robustness of model-based RL.\n",
        "\n",
        "### Additional Seminal Works\n",
        "\n",
        "- **Gu et al.**: [Continuous Deep Q-Learning with Model-Based Acceleration (2016)](https://arxiv.org/abs/1603.00748). This paper presents a method for accelerating deep Q-learning using model-based techniques.\n",
        "- **Feinberg et al.**: [Model-Based Value Expansion (2018)](https://arxiv.org/abs/1803.00101). This study further elaborates on the MBVE approach for efficient RL.\n",
        "- **Janner et al.**: [When to Trust Your Model: Model-Based Policy Optimization (2019)](https://arxiv.org/abs/1906.08253). This paper discusses strategies for determining the reliability of models in model-based policy optimization.\n",
        "\n",
        "These readings provide a solid foundation and advanced insights into the development and application of model-based reinforcement learning algorithms.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxLYbw1wj9wS"
      },
      "source": [
        "# <img src=\"https://img.icons8.com/?size=100&id=46756&format=png&color=000000\" style=\"height:50px;display:inline\"> Credits\n",
        "---\n",
        "* Examples and code snippets were taken from <a href=\"https://neuromatch.io/neuroscience/\"> RNeuromatch Academy </a>\n",
        "* Examples and explanations were taken from <a href=\"https://rail.eecs.berkeley.edu/deeprlcourse/\">CS285 - Deep Reinforcement Learning Course at UC Berkeley</a>\n",
        "* Icons from <a href=\"https://icons8.com/\">Icons8.com"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "12px",
        "width": "186px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "410.667px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "-oid7uNNGKvv",
        "UoXl-zQ5bxxG",
        "r4P79ZolWypl",
        "pbIQ15HYzdLu",
        "eypSYcGfzdLu",
        "S34VKH9x2Fy4",
        "MVqn4Kc8Aomo",
        "sJBNkA_h64g1"
      ],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c79b2ed6"
      },
      "source": [
        "<div style=\"text-align: left\">\n",
        "    <img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/logo.png?raw=true' width=800/>  \n",
        "</div>\n",
        "\n",
        "Author: Itay Segev\n",
        "\n",
        "E-mail: [itaysegev@campus.technion.ac.il](mailto:itaysegev@campus.technion.ac.il)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIdLEDqyzdLk"
      },
      "source": [
        "# From Tabular Q-Learning to DQN\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp1H54SQLRUU"
      },
      "source": [
        "\n",
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/presentations/dqn-tutorial/images/dqn_nature.png?raw=true' width=900/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8c04ad1-e844-4836-9de1-206dc454d870",
        "tags": []
      },
      "source": [
        "\n",
        "<a id=\"section:intro\"></a>\n",
        "\n",
        "# <img src=\"https://img.icons8.com/?size=50&id=55412&format=png&color=000000\" style=\"height:50px;display:inline\"> Introduction\n",
        "---\n",
        "\n",
        "In this tutorial, you'll learn how to implement the **Fitted Q-Iteration** (FQI) algorithm, which is the predecessor of the [Deep Q-Network (DQN)](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) algorithm. To address the limitations of FQI in dealing with large and complex state spaces, we transition to **DQN**, which enhances FQI by using neural networks and introducing several key components.\n",
        "\n",
        "You will then learn how to implement step-by-step the FQI algorithm. In this notebook, you will implement the main components of the Deep Q-Network (DQN) algorithm: a replay buffer, epsilon-greedy exploration strategy, and a Q-network. These components are crucial for stabilizing the learning process and improving the efficiency of the algorithm.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oid7uNNGKvv"
      },
      "source": [
        "# <img src=\"https://img.icons8.com/?size=50&id=43171&format=png&color=000000\" style=\"height:30px;display:inline\"> Setup\n",
        "\n",
        "\n",
        "You will need to make a copy of this notebook in your Google Drive before you can edit the notebook. You can do so with **File &rarr; Save a copy in Drive**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DSNObDRJgu8b"
      },
      "outputs": [],
      "source": [
        "#@title mount your Google Drive\n",
        "import os\n",
        "connect_drive = False #@param {type: \"boolean\"}\n",
        "if connect_drive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "  # set up mount symlink\n",
        "  DRIVE_PATH = '/content/gdrive/My\\ Drive/cs236203_s24'\n",
        "  DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
        "  if not os.path.exists(DRIVE_PYTHON_PATH):\n",
        "    %mkdir $DRIVE_PATH\n",
        "\n",
        "## the space in `My Drive` causes some issues,\n",
        "## make a symlink to avoid this\n",
        "SYM_PATH = '/content/cs236203_s24'\n",
        "if not os.path.exists(SYM_PATH) and connect_drive:\n",
        "  !ln -s $DRIVE_PATH $SYM_PATH\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ft-NJZ29L_p9"
      },
      "outputs": [],
      "source": [
        "#@title apt install requirements\n",
        "\n",
        "#@markdown Run each section with Shift+Enter\n",
        "\n",
        "#@markdown Double-click on section headers to show code.\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y python3-opengl\n",
        "!apt install ffmpeg xvfb\n",
        "!pip3 install pyvirtualdisplay\n",
        "!pip install gymnasium\n",
        "\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApXAKq1CNbCs"
      },
      "outputs": [],
      "source": [
        "#@title clone course repo\n",
        "\n",
        "%cd $SYM_PATH\n",
        "# !git clone {repo_url}\n",
        "!git clone --single-branch --branch main https://github.com/CLAIR-LAB-TECHNION/SDMRL.git\n",
        "%cd SDMRL/tutorials/notebooks/deep-Qlearning\n",
        "\n",
        "from fqi.collect_data import collect_data, load_data, save_data\n",
        "from dqn.replay_buffer import ReplayBufferSamples\n",
        "from dqn.evaluation import evaluate_policy\n",
        "from notebook_utils import show_videos\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B8_R2qRiG2xA"
      },
      "outputs": [],
      "source": [
        "#@title set up virtual display\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-7f-Swax_9x"
      },
      "source": [
        "## Import the packages\n",
        "\n",
        "In addition to the installed libraries, we also use:\n",
        "\n",
        "- `random`: To generate random numbers (that will be useful for epsilon-greedy policy).\n",
        "- `imageio`: To generate a replay video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bcb5e2f-b9e0-4354-a24a-2f893b1313e6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import imageio\n",
        "import os\n",
        "import tqdm\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgydhMV1q8DV"
      },
      "source": [
        "# <img src=\"https://img.icons8.com/?size=50&id=42948&format=png&color=000000\" style=\"height:30px;display:inline\"> The CartPole-v1 Environment\n",
        "\n",
        "\n",
        "\n",
        "The [CartPole-v1 environment](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py) is a classic control task in reinforcement learning. It is designed to test the basic capabilities of an RL algorithm in a simple yet challenging setting. Here’s a detailed description of the environment:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUkOD7xsq8Dd"
      },
      "source": [
        "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart, and the goal is to balance the pole by applying forces in the left and right direction on the cart.\n",
        "\n",
        "The agent must learn to push the cart left or right **to keep the pole balanced and prevent it from falling over**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnSBVFDMq8Dd"
      },
      "source": [
        "<img src='https://gymnasium.farama.org/_images/cart_pole.gif' width=800/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-xMxm468-sh"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=46589&format=png&color=000000\" style=\"height:30px;display:inline\"> Task 1: Getting Familiar with the CartPole Environment\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA86BbfNJ8Ga"
      },
      "source": [
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/task_sign.png?raw=true' width=800/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41wcOSe9PSJ"
      },
      "source": [
        "In this task, you will explore the CartPole-v1 environment. Your objective is to understand the basic operations of this environment, such as resetting the environment, taking actions, and observing the outcomes.\n",
        "\n",
        "Questions:\n",
        "- What are the dimensions of the state space?\n",
        "- What are the possible actions in the action space?\n",
        "- What triggers the termination of an episode in CartPole-v1?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wi_HrvKaq8Dd"
      },
      "outputs": [],
      "source": [
        "env_id = \"CartPole-v1\"\n",
        "# Create the env\n",
        "env = gym.make(env_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3sRnN289aRW"
      },
      "source": [
        "#### <img src=\"https://img.icons8.com/?size=50&id=42816&format=png&color=000000\" style=\"height:30px;display:inline\">  Solution\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcL2dJeu9VYk"
      },
      "outputs": [],
      "source": [
        "# Get the state space and action space\n",
        "s_size = env.observation_space.shape[0]\n",
        "a_size = env.action_space.n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-FxXlkiq8Dd"
      },
      "source": [
        "The state space of the CartPole-v1 environment is represented by a 4-dimensional vector:\n",
        "\n",
        "1. **Cart Position**: The position of the cart along the track.\n",
        "2. **Cart Velocity**: The velocity of the cart.\n",
        "3. **Pole Angle**: The angle of the pole with respect to the vertical position.\n",
        "4. **Pole Angular Velocity**: The rate of change of the pole's angle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zetjSAONq8Dd"
      },
      "outputs": [],
      "source": [
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"The State Space is: \", s_size)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nzCU0NJq8Dd"
      },
      "source": [
        "The action space is **discrete**, consisting of two possible actions:\n",
        "\n",
        "1. **Push Left**: Apply a force to the left.\n",
        "2. **Push Right**: Apply a force to the right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-EjKUnkq8De"
      },
      "outputs": [],
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"The Action Space is: \", a_size)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vka-_FOkq8De"
      },
      "source": [
        "The agent receives a reward 💰 of +1 for every timestep that the pole remains upright. The goal is to maximize the total reward over an episode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpCN_y5-q8De"
      },
      "source": [
        "An episode ends if any of the following conditions are met:\n",
        "\n",
        "- The pole angle exceeds ±12 degrees.\n",
        "- The cart position exceeds ±2.4 units from the center.\n",
        "- The episode length exceeds 500 timesteps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAW-ywho51wU"
      },
      "source": [
        "# Reminder About Tabular Q-Learning\n",
        "\n",
        "Before diving into Fitted Q-Iteration (FQI), let's briefly revisit Tabular Q-learning, which we covered in the last tutorial. Tabular Q-learning is a fundamental reinforcement learning algorithm that aims to learn the optimal action-value function, $Q^*$, which gives the maximum expected reward for each state-action pair.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZphya_VAMEr"
      },
      "source": [
        "### Core Concepts:\n",
        "\n",
        "- **Q-Table**: In Tabular Q-learning, we maintain a Q-table, a 2D array where each cell $(𝑠,𝑎)$ contains the Q-value for state $𝑠$ and action $𝑎$.\n",
        "\n",
        "- **Bellman Equation**: The Q-values are updated iteratively based on the Bellman equation:\n",
        "  \n",
        "  $$\n",
        "  Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s',a') - Q(s,a) \\right]\n",
        "  $$\n",
        "\n",
        "- **Policy Improvement**: The policy is derived from the Q-table by selecting the action with the highest Q-value in each state, known as the 𝜖-greedy policy to balance exploration and exploitation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Loudo_ZFAk_7"
      },
      "source": [
        "### Steps in Tabular Q-Learning:\n",
        "\n",
        "1. **Initialize Q-Table**: Start with an initial Q-table with all zeros or random values.\n",
        "\n",
        "2. **For each episode**:\n",
        "   - **Initialize the starting state**.\n",
        "   - **For each step within the episode**:\n",
        "     - Choose an action 𝑎 using the 𝜖-greedy policy.\n",
        "     - Take action 𝑎, observe the reward 𝑟 and the next state 𝑠′.\n",
        "     - Update the Q-value 𝑄(𝑠,𝑎) using the Bellman equation.\n",
        "     - Transition to the next state 𝑠′.\n",
        "     \n",
        "3. **Repeat until convergence**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs3Sq0oBAgDg"
      },
      "source": [
        "Tabular Q-learning is simple and effective for environments with a small state and action space. However, it becomes impractical when dealing with large or continuous state-action spaces, which leads us to Fitted Q-Iteration (FQI), an approach to handle such scenarios using function approximation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ilq6KbG57ta"
      },
      "source": [
        ";;;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c09ada11-d3a6-4837-b4d0-9657106a54b5"
      },
      "source": [
        "# <img src=\"https://img.icons8.com/?size=50&id=aauVMeTJQzjy&format=png&color=000000\" style=\"height:30px;display:inline\"> Fitted Q Iteration (FQI)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogLWnhjSyZqk"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ff52b1a-6eb1-4284-b25f-8d59ab005ed5"
      },
      "source": [
        "\n",
        "\n",
        "Fitted Q Iteration (FQI) is an algorithm that extends q-learning to continuous state space using function estimators.\n",
        "\n",
        "It iteratively approximates the q-values for a given dataset of transitions (state, action, reward, next_state) by iteratively solving a regression problem.\n",
        "\n",
        "Compared to later algorithms like DQN, FQI uses the whole dataset at every iteration (working on the whole batch instead of using minibatches).\n",
        "\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://araffin.github.io/slides/dqn-tutorial/images/fqi/tabular_limit_2.png\" width=\"500\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90fbec50-1da0-4bc4-8b88-79fa38480bfe"
      },
      "source": [
        "###  Collect the Dataset\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkMoruluPBlv"
      },
      "source": [
        "The first step in fitted Q-iteration is to collect a dataset consisting of tuples $(s_i, a_i, s_i', r_i)$\n",
        "using some policy. The algorithm works for a wide range of different policies, though not all policy choices are equally good. The principles will apply to any policy, and it certainly doesn't have to be the latest policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrRhzf_OPgjR"
      },
      "source": [
        "This function collects an offline dataset of transitions that will be used to train a model using the FQI algorithm.\n",
        "\n",
        "See docstring of the function for what is expected as input/output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f1b3ff1-911b-4582-91f7-49420380eda3"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "from gymnasium import spaces\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class OfflineData:\n",
        "    \"\"\"\n",
        "    A class to store transitions.\n",
        "    \"\"\"\n",
        "\n",
        "    observations: np.ndarray  # same as \"state\" in the theory\n",
        "    next_observations: np.ndarray\n",
        "    actions: np.ndarray\n",
        "    rewards: np.ndarray\n",
        "    terminateds: np.ndarray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9f753ed-5f5d-4133-ab82-c84b3cfe2eef"
      },
      "outputs": [],
      "source": [
        "def collect_data(env_id: str, n_steps: int = 50_000) -> OfflineData:\n",
        "    \"\"\"\n",
        "    Collect transitions using a random agent (sample action randomly).\n",
        "\n",
        "    :param env_id: The name of the environment.\n",
        "    :param n_steps: Number of steps to perform in the env.\n",
        "    :return: The collected transitions.\n",
        "    \"\"\"\n",
        "    # Create the Gym env\n",
        "    env = gym.make(env_id)\n",
        "\n",
        "    assert isinstance(env.observation_space, spaces.Box)\n",
        "    # Numpy arrays (buffers) to collect the data\n",
        "    observations = np.zeros((n_steps, *env.observation_space.shape))\n",
        "    next_observations = np.zeros((n_steps, *env.observation_space.shape))\n",
        "    # Discrete actions\n",
        "    actions = np.zeros((n_steps, 1))\n",
        "    rewards = np.zeros((n_steps,))\n",
        "    terminateds = np.zeros((n_steps,))\n",
        "\n",
        "    # Variable to know if the episode is over (done = terminated or truncated)\n",
        "    done = False\n",
        "\n",
        "    # Start the first episode\n",
        "    obs, _ = env.reset()\n",
        "\n",
        "    for idx in range(n_steps):\n",
        "        # Sample a random action\n",
        "        action = env.action_space.sample()\n",
        "        # Step in the environment\n",
        "        next_obs, reward, terminated, truncated, info_ = env.step(action)\n",
        "        # Store the transition\n",
        "        # Note: we only record true termination (timeouts/truncations are artificial terminations)\n",
        "        observations[idx, :] = obs\n",
        "        next_observations[idx, :] = next_obs\n",
        "        actions[idx, :] = action\n",
        "        rewards[idx] = reward\n",
        "        terminateds[idx] = terminated\n",
        "\n",
        "        # Update current observation\n",
        "        obs = next_obs\n",
        "        # Check if the episode is over\n",
        "        done = terminated or truncated\n",
        "        # Don't forget to reset the env at the end of an episode\n",
        "        if done:\n",
        "            obs, _ = env.reset()\n",
        "\n",
        "\n",
        "    ### END OF YOUR CODE\n",
        "\n",
        "    return OfflineData(\n",
        "        observations,\n",
        "        next_observations,\n",
        "        actions,\n",
        "        rewards,\n",
        "        terminateds,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbe5d4b3-d3d6-4f82-9337-a8566c1fc707"
      },
      "source": [
        "Let's try the collect data method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f499a940-a35f-4b8c-86bb-94231c41a87e"
      },
      "outputs": [],
      "source": [
        "env_id = \"CartPole-v1\"\n",
        "n_steps = 50_000\n",
        "# Collect transitions for n_steps\n",
        "data = collect_data(env_id=env_id, n_steps=n_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ae0713b-63f3-40f0-8471-f3f7e3c53f88"
      },
      "outputs": [],
      "source": [
        "# Check the length of the collected data\n",
        "assert len(data.observations) == n_steps\n",
        "assert len(data.actions) == n_steps\n",
        "# Check that there are multiple episodes in the data\n",
        "assert not np.all(data.terminateds)\n",
        "assert np.any(data.terminateds)\n",
        "# Check the shape of the collected data\n",
        "if env_id == \"CartPole-v1\":\n",
        "    assert data.observations.shape == (n_steps, 4)\n",
        "    assert data.next_observations.shape == (n_steps, 4)\n",
        "assert data.actions.shape == (n_steps, 1)\n",
        "assert data.rewards.shape == (n_steps,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f815ff42-3c8e-4ab5-835f-70438171751b"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "output_filename = Path(\"../data\") / f\"{env_id}_data\"\n",
        "# Create folder if it doesn't exist\n",
        "output_filename.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save collected data using numpy\n",
        "save_data(data, output_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "045b251a-9ace-450b-afa9-7ee4128cdef6"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from gymnasium import spaces\n",
        "from sklearn import tree\n",
        "from sklearn.base import RegressorMixin\n",
        "from sklearn.exceptions import NotFittedError\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.neighbors import KNeighborsRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "239ad0de-bec9-4918-a691-4322d062c45a"
      },
      "source": [
        "### Choosing a Model\n",
        "\n",
        "With FQI, you can use any regression model.\n",
        "\n",
        "Here we are choosing a [k-nearest neighbors regressor](https://scikit-learn.org/stable/modules/neighbors.html#regression), but one could choose a linear model, a decision tree, a neural network, ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d8d873e-bc40-4a35-b98b-4fe9ce916aab"
      },
      "outputs": [],
      "source": [
        "# First choose the regressor\n",
        "model_class = partial(KNeighborsRegressor, n_neighbors=30)  # LinearRegression, GradientBoostingRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be7a24b4-a416-4607-af3a-9dfdbc203fc1"
      },
      "source": [
        "### Loading offline dataset\n",
        "\n",
        "The offline dataset contains the transitions collected using a random agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fe29bd2-ea95-4778-b466-537a088615b0"
      },
      "outputs": [],
      "source": [
        "\n",
        "env_id = \"CartPole-v1\"\n",
        "output_filename = Path(\"../data\") / f\"{env_id}_data.npz\"\n",
        "render_mode = \"rgb_array\"\n",
        "\n",
        "# Create test environment\n",
        "env = gym.make(env_id, render_mode=render_mode)\n",
        "\n",
        "# Load saved transitions\n",
        "data = load_data(output_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bqq3PbpZP8Zm"
      },
      "source": [
        "### Calculate Target Values\n",
        "\n",
        "For every transition that you sampled, calculate a target value. This involves taking the reward from the transition plus the discounted maximum Q-value of the next state. The formula for the target value $y_i$ is:\n",
        "\n",
        "$$\n",
        "y_i = R(s_i, a_i) + \\gamma \\max_{a_i'} Q_{\\phi}(s_i', a_i')\n",
        "$$\n",
        "\n",
        "Here, $Q_{\\phi}$ is your previous Q-function estimator parameterized by $\\phi$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKhaPw4OxIVh"
      },
      "source": [
        "<div>\n",
        "    <img src=\"https://araffin.github.io/slides/dqn-tutorial/images/fqi/q_value_fqi.png\" width=\"300\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7c0f461-9c04-48f2-ad57-21b555a7c041"
      },
      "source": [
        "### First Iteration of FQI\n",
        "\n",
        "For $n = 0$, the initial training set is defined as:\n",
        "\n",
        "- $x = (s_t, a_t)$\n",
        "- $y = r_t$\n",
        "\n",
        "We fit a regression model $f_\\theta(x) = y$ to obtain $ Q^{n=0}_\\theta(s, a) $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26940b3e-4c9a-4632-a198-2e0ce7b12b88"
      },
      "outputs": [],
      "source": [
        "# First iteration:\n",
        "# The target q-value is the reward obtained\n",
        "targets = data.rewards.copy()\n",
        "# Create input for current observations and actions\n",
        "# Concatenate the observations and actions\n",
        "# so we can predict qf(s_t, a_t)\n",
        "current_obs_input = np.concatenate((data.observations, data.actions), axis=1)\n",
        "# Fit the estimator for the current target\n",
        "model = model_class().fit(current_obs_input, targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bf9544c-87f4-4872-9ae9-eef49c3a040a"
      },
      "source": [
        "### Function to predict Q-Values\n",
        " For efficiency, we are not working with one transition at a time but on a batch of transitions/observations. The resulting `q_values` are therefore a matrix of shape `(batch_size, n_actions)` where `batch_size` is the size of the batch of observations (aka number of observations used at once) and `n_actions` is the number of available actions (`n_actions=2` for CartPole, as we can only go left or right)\n",
        "\n",
        "Below is a diagram explaining how the predicted `q_values` will be stored.\n",
        "\n",
        "Row-wise, we get the q-values estimates for all possible actions but for one single observation (state):\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://araffin.github.io/slides/dqn-tutorial/images/sklearn/q_value_batch.png\" width=\"500\"/>\n",
        "    <br>\n",
        "</div>\n",
        "\n",
        "Column-wise, we get the q-values estimates for a given action but for a batch of observations (states):\n",
        "<div>\n",
        "    <img src=\"https://araffin.github.io/slides/dqn-tutorial/images/sklearn/q_value_batch_2.png\" width=\"400\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d1c52d1-4336-4caa-9d91-cfb641e7972a"
      },
      "outputs": [],
      "source": [
        "def get_q_values(\n",
        "    model: RegressorMixin,\n",
        "    obs: np.ndarray,\n",
        "    n_actions: int,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Retrieve the q-values for a set of observations(=states in the theory).\n",
        "    qf(s_t, action) for all possible actions.\n",
        "\n",
        "    :param model: Q-value estimator\n",
        "    :param obs: A batch of observations\n",
        "    :param n_actions: Number of discrete actions.\n",
        "    :return: The predicted q-values for the given observations\n",
        "        (batch_size, n_actions)\n",
        "    \"\"\"\n",
        "    batch_size = len(obs)\n",
        "    q_values = np.zeros((batch_size, n_actions))\n",
        "\n",
        "    # TODO: for every possible actions a:\n",
        "    # 1. Create the regression model input $(s, a)$ for the action a\n",
        "    # and states s (here a batch of observations)\n",
        "    # 2. Predict the q-values for the batch of states\n",
        "    # 3. Update q-values array for the current action a\n",
        "\n",
        "    # Predict q-value for each action\n",
        "    for action_idx in range(n_actions):\n",
        "        # Note: we should do one hot encoding if not using CartPole (n_actions > 2)\n",
        "        # Create a vector of size (batch_size, 1) for the current action\n",
        "        # This allows to do batch prediction for all the provided observations\n",
        "        actions = action_idx * np.ones((batch_size, 1))\n",
        "        # Concatenate the observations and the actions to obtain\n",
        "        # the input to the q-value estimator\n",
        "        # you can use `np.concatenate()`\n",
        "        model_input = np.concatenate((obs, actions), axis=1)\n",
        "        # Predict q-values for the given observation/action combination\n",
        "        # shape: (batch_size, 1)\n",
        "        predicted_q_values = model.predict(model_input)\n",
        "        # Update the q-values array for the current action\n",
        "        q_values[:, action_idx] = predicted_q_values\n",
        "\n",
        "    return q_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "353de669-1744-43be-a3dc-5310d23c72a4"
      },
      "source": [
        "Let's test it with a subset of the collected data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6910968b-16db-41fe-a6d9-ff8c65322c85"
      },
      "outputs": [],
      "source": [
        "n_observations = 2\n",
        "n_actions = int(env.action_space.n)\n",
        "\n",
        "q_values = get_q_values(model, data.observations[:n_observations], n_actions)\n",
        "\n",
        "assert q_values.shape == (n_observations, n_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a883ff38-48e9-4f7c-8692-33a9d1b325d2"
      },
      "source": [
        "### Function to evaluate a model\n",
        "\n",
        "A greedy policy $\\pi(s)$ can be defined using the q-value:\n",
        "\n",
        "$\\pi(s) = argmax_{a \\in A} Q(s, a)$.\n",
        "\n",
        "It is the policy that takes the action with the highest q-value for a given state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b1f4b18-a811-43fa-925d-20ad2f4d52d7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    model: RegressorMixin,\n",
        "    env: gym.Env,\n",
        "    n_eval_episodes: int = 10,\n",
        "    video_name: Optional[str] = None,\n",
        ") -> None:\n",
        "    episode_returns, episode_reward = [], 0.0\n",
        "    total_episodes = 0\n",
        "    done = False\n",
        "\n",
        "    # Setup video recorder\n",
        "    video_recorder = None\n",
        "    if video_name is not None and env.render_mode == \"rgb_array\":\n",
        "        os.makedirs(\"../logs/videos/\", exist_ok=True)\n",
        "\n",
        "        video_recorder = VideoRecorder(\n",
        "            env=env,\n",
        "            base_path=f\"../logs/videos/{video_name}\",\n",
        "        )\n",
        "\n",
        "    obs, _ = env.reset()\n",
        "    # Number of discrete actions\n",
        "    n_actions = int(env.action_space.n)\n",
        "    assert isinstance(env.action_space, spaces.Discrete), \"FQI only support discrete actions\"\n",
        "\n",
        "    while total_episodes < n_eval_episodes:\n",
        "        # Record video\n",
        "        if video_recorder is not None:\n",
        "            video_recorder.capture_frame()\n",
        "\n",
        "\n",
        "        # Retrieve the q-values for the current observation\n",
        "        # you need to re-use `get_q_values()`\n",
        "        # Note: you need to add a batch dimension to the observation\n",
        "        # you can use `obs[np.newaxis, ...]` for that: (obs_dim,) -> (batch_size=1, obs_dim)\n",
        "        q_values = get_q_values(\n",
        "            model,\n",
        "            obs[np.newaxis, ...],\n",
        "            n_actions,\n",
        "        )\n",
        "        # Select the action that maximizes the q-value for each state\n",
        "        # Don't forget to remove the batch dimension, you can `.item()` for that\n",
        "        best_action = int(np.argmax(q_values, axis=1).item())\n",
        "\n",
        "        # Send the action to the env\n",
        "        obs, reward, terminated, truncated, _ = env.step(best_action)\n",
        "\n",
        "\n",
        "        episode_reward += float(reward)\n",
        "\n",
        "        done = terminated or truncated\n",
        "        if done:\n",
        "            episode_returns.append(episode_reward)\n",
        "            episode_reward = 0.0\n",
        "            total_episodes += 1\n",
        "            current_obs, _ = env.reset()\n",
        "\n",
        "    if video_recorder is not None:\n",
        "        print(f\"Saving video to {video_recorder.path}\")\n",
        "        video_recorder.close()\n",
        "\n",
        "    print(f\"Total reward = {np.mean(episode_returns):.2f} +/- {np.std(episode_returns):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23e326d9-1fa7-4efd-a841-b1d9e5ef9484"
      },
      "outputs": [],
      "source": [
        "# Evaluate the first iteration\n",
        "evaluate(model, env, n_eval_episodes=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4jVaNk_RBCk"
      },
      "source": [
        "### Step 3: Train a New Q-Function\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZq7-HCMRICl"
      },
      "source": [
        "Train a new Q-function by finding a new parameter vector $\\phi$ that minimizes the difference between the Q-values and the corresponding target values  $y_i$. The Q-function takes as input the state $s$ and action $a$ and outputs a scalar value.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBTPTnjIRb_V"
      },
      "source": [
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/task_sign.png?raw=true' width=800/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eced782e-653d-4041-8473-2c39d12ba9a9"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/?size=50&id=46589&format=png&color=000000\" style=\"height:30px;display:inline\"> Task: The Fitted Q Iterations\n",
        "\n",
        "1. Create the training set based on the previous iteration $ Q^{n-1}_\\theta(s, a) $ and the transitions:\n",
        "- input: $x = (s_t, a_t)$\n",
        "- if $s_{t+1}$ is non-terminal: $y = r_t + \\gamma \\cdot \\max_{a' \\in A}(Q^{n-1}_\\theta(s_{t+1}, a'))$\n",
        "- if $s_{t+1}$ is terminal, do not bootstrap: $y = r_t$\n",
        "\n",
        "2. Fit a model $f_\\theta$ using a regression algorithm to obtain $ Q^{n}_\\theta(s, a)$\n",
        "\n",
        "\\begin{aligned}\n",
        " f_\\theta(x) = y\n",
        "\\end{aligned}\n",
        "\n",
        "4. Repeat, $n = n + 1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d824b98-6360-4d4e-a3e2-584c0c36665e"
      },
      "source": [
        "First, let's define some constants:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cbff0d8-eab0-4cec-847e-23bc1c509359"
      },
      "outputs": [],
      "source": [
        "# Max number of iterations\n",
        "n_iterations = 20\n",
        "# How often do we evaluate the learned model\n",
        "eval_freq = 2\n",
        "# How many episodes to evaluate every eval-freq\n",
        "n_eval_episodes = 10\n",
        "# discount factor\n",
        "gamma = 0.99\n",
        "# Number of discrete actions\n",
        "n_actions = int(env.action_space.n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18d564da-7170-4bf8-8259-f19d0b12bcbd"
      },
      "source": [
        "Then do several iteration of the FQI algorithm\n",
        "\n",
        "**HINT**: For that exercise, you should take a look at the first iteration of FQI.\n",
        "\n",
        "**HINT**: The`data` variable (containing the offline dataset) is an instance of `OfflineData`:\n",
        "\n",
        "```python\n",
        "@dataclass\n",
        "class OfflineData:\n",
        "    \"\"\"\n",
        "    A class to store transitions.\n",
        "    \"\"\"\n",
        "\n",
        "    observations: np.ndarray  # same as \"state\" in the theory\n",
        "    next_observations: np.ndarray\n",
        "    actions: np.ndarray\n",
        "    rewards: np.ndarray\n",
        "    terminateds: np.ndarray\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45dbbc13-5332-4f2b-8cfe-0b7d5464e853"
      },
      "outputs": [],
      "source": [
        "# Load saved transitions\n",
        "data = load_data(output_filename)\n",
        "\n",
        "print(type(data))\n",
        "print(data.observations.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acb8ef52-67c8-4290-b679-67b40025caed"
      },
      "source": [
        " We are working here with a batch of data. Thanks to numpy, you can efficiently compute the max over a batch:\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://araffin.github.io/slides/dqn-tutorial/images/sklearn/q_values_max.png\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "**HINT**: Using masking, you can elegantly handle the two cases for the regression target. The value of $y$ depends on whether the episode is terminated or not (in one case we bootstrap using the q-value, in the other case, we do not bootstrap and use the terminal reward instead).\n",
        "\n",
        "Visually, masking is an element-wise operation:\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://araffin.github.io/slides/dqn-tutorial/images/sklearn/masking.png\" width=\"500\"/>\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21c99207-4a80-4263-8b13-1818f65632e1"
      },
      "outputs": [],
      "source": [
        "%%script true\n",
        "for iter_idx in range(n_iterations):\n",
        "    ### YOUR CODE HERE\n",
        "    # TODO:\n",
        "    # 1. Compute the q values for the next states using\n",
        "    # the previous regression model\n",
        "    # 2. Keep only the next q values that correspond\n",
        "    # to the greedy-policy\n",
        "    # 3. Construct the regression target (TD(0) target)\n",
        "    # 4. Fit a new regression model with this new target\n",
        "\n",
        "    # First, retrieve the q-values for the next states (data.next_observations)\n",
        "    # for all possible actions\n",
        "    # you need to use `get_q_values()` method\n",
        "\n",
        "    # Follow-greedy policy: use the action with the highest q-value\n",
        "    # to compute the next q-values\n",
        "\n",
        "    # The new target is the reward + what our agent expect to get\n",
        "    # if it follows a greedy policy (follow action with the highest q-value)\n",
        "    # Reminder: you should not bootstrap if terminated=True\n",
        "    # (you can mask the next q values for that using `np.logical_not`)\n",
        "\n",
        "\n",
        "    # Update the q-value estimate for the current (observations, actions) pair\n",
        "    # (current_obs_input)\n",
        "    # with the current target,\n",
        "    # i.e., fit a regression model using the new target\n",
        "    # Note: you can use `model_class()` to instantiate a new model\n",
        "    model =\n",
        "\n",
        "\n",
        "    ### END OF YOUR CODE\n",
        "\n",
        "    if (iter_idx + 1) % eval_freq == 0:\n",
        "        print(f\"Iter {iter_idx + 1}\")\n",
        "        print(f\"Score: {model.score(current_obs_input, targets):.2f}\")\n",
        "        evaluate(model, env, n_eval_episodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3477740-bfe4-4e0b-8a5d-cd0206786c8b"
      },
      "source": [
        "### Record a video of the trained agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da8d90cc-2d77-4682-a97c-a3dacd081f38"
      },
      "outputs": [],
      "source": [
        "eval_env = gym.make(env_id, render_mode=\"rgb_array\")\n",
        "video_name = f\"FQI_{env_id}\"\n",
        "n_eval_episodes = 3\n",
        "\n",
        "evaluate(model, eval_env, n_eval_episodes, video_name=video_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfc66910-de41-4622-9811-96addf8ba8d3"
      },
      "outputs": [],
      "source": [
        "print(f\"FQI agent on {env_id} after {n_iterations} iterations:\")\n",
        "show_videos(\"../logs/videos/\", prefix=video_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uNUzj8hQqxD"
      },
      "source": [
        "# <img src=\"https://img.icons8.com/?size=50&id=66365&format=png&color=000000\" style=\"height:30px;display:inline\"> Deep Q-Network (DQN)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjUe7TlAYnXx"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this section, we'll introduce the transition from Fitted Q Iteration (FQI) to Deep Q-Networks (DQN), highlighting the limitations of FQI and how DQN addresses these challenges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxEBQtRBeHVN"
      },
      "source": [
        "Fitted Q-Iteration (FQI) generally uses regression models to approximate the Q-value function. However, Deep Q-Learning (DQN) leverages neural networks for several compelling reasons:\n",
        "\n",
        "1. **Scalability**: Neural networks can handle large and continuous state spaces, making them suitable for complex environments where traditional regression models and tabular methods are impractical.\n",
        "2. **Generalization**: Neural networks can generalize from seen states to unseen states, allowing the agent to make informed decisions even in states it has not encountered before.\n",
        "3. **Feature Extraction**: Especially in environments with raw sensory inputs (e.g., images), neural networks can learn useful features from raw data, improving the agent's decision-making process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY9cp7ZrZLBT"
      },
      "source": [
        "### Limitations of FQI\n",
        "\n",
        "While FQI provides a solid foundation for value-based RL, it suffers from several limitations:\n",
        "\n",
        "1. **Offline Reinforcement Learning**: FQI operates in an offline manner, meaning it uses a fixed dataset for training and does not continuously interact with the environment. This limits the ability of the algorithm to adapt to new data and learn incrementally.\n",
        "2. **Loop Over All Actions**: To compute the target values, FQI needs to loop over all possible actions to find the maximum Q-value for the next state. This becomes computationally expensive in environments with large or continuous action spaces.\n",
        "3. **Instability**: FQI can suffer from instability in learning because the target values used for training the Q-function are based on the same Q-function being learned. This can lead to oscillations and divergence in the Q-value estimates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tj3fjqoaj3J"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=46457&format=png&color=000000\" style=\"height:30px;display:inline\"> Offline vs. Online Reinforcement Learning Trade-offs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4rilgS4ak32"
      },
      "source": [
        "Offline RL, also known as batch RL, involves learning a policy from a fixed dataset of experiences collected beforehand. The agent does not interact with the environment during training; instead, it learns solely from the provided dataset.\n",
        "\n",
        "### Advantages:\n",
        "\n",
        "- **Safety**: Offline RL is suitable for scenarios where interacting with the environment is costly, dangerous, or impractical (e.g., healthcare, autonomous driving).\n",
        "- **Reproducibility**: Since the dataset is fixed, experiments are more reproducible, making it easier to benchmark different algorithms.\n",
        "\n",
        "### Disadvantages:\n",
        "\n",
        "- **Limited Exploration**: The agent can only learn from the experiences present in the dataset. If the dataset does not cover diverse scenarios, the learned policy may be suboptimal.\n",
        "- **Overfitting**: There is a risk of overfitting to the fixed dataset, especially if the dataset is not representative of the entire state-action space.\n",
        "\n",
        "Online RL, in contrast, involves continuous interaction with the environment. The agent collects new experiences while learning, allowing it to adapt and improve its policy over time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebiJMIB6aqSR"
      },
      "source": [
        "### Online Q-Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUHfBIgaa0ef"
      },
      "source": [
        "We can easily make our Fitted Q-Iteration (FQI) online by integrating it with real-time interaction with the environment. In online Q iterations, the agent continually updates its Q-values based on the latest interactions. The process can be described as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i145lN5bqk4"
      },
      "source": [
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/online_q_learing.png?raw=true' width=800/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1qoX4eNcyMO"
      },
      "source": [
        "### Problems in the Online Version Without Replay Buffer and Target Network\n",
        "\n",
        "Despite the potential of online Q-learning, it faces significant challenges when implemented without additional mechanisms such as a replay buffer and a target network:\n",
        "\n",
        "1. **Correlated Samples**: Sequential experiences are often highly correlated, violating the assumption of independent and identically distributed (i.i.d.) samples necessary for effective stochastic gradient descent.\n",
        "2. **Instability**: The targets used for updates are constantly changing because the Q-values depend on the current estimates. This can cause the learning process to become unstable and may prevent convergence.\n",
        "3. **Overestimation Bias**: The Q-learning algorithm tends to overestimate Q-values due to the maximization step, leading to overly optimistic value estimates.\n",
        "\n",
        "To address these issues, we need to introduce additional components that can help stabilize and improve the learning process. In the following sections, we will discuss these components in detail.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d41f978-b6dd-47aa-baa6-04514109cd0c"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=46958&format=png&color=000000\" style=\"height:30px;display:inline\"> DQN Components\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRFvI9baQqxJ"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=46683&format=png&color=000000\" style=\"height:30px;display:inline\">  Replay Buffer\n",
        "\n",
        "\n",
        "\n",
        "The replay buffer is one of the main component of DQN. It contains a collection of transitions, the same way we had a fixed dataset of transitions with FQI. However, compared to FQI, this ring buffer is consistently updated with new experience (and old transitions are dropped when the max capicity is reached).\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://araffin.github.io/slides/dqn-tutorial/images/dqn/replay_buffer.png\" width=\"800\"/>\n",
        "</div>\n",
        "\n",
        "To update the Q-Network, DQN samples mini-batches from the replay buffer (vs the whole dataset for FQI).\n",
        "\n",
        "Each mini-batch can be represented using this structure:\n",
        "\n",
        "```python\n",
        "@dataclass\n",
        "class ReplayBufferSamples:\n",
        "    \"\"\"\n",
        "    A dataclass containing transitions from the replay buffer.\n",
        "    \"\"\"\n",
        "\n",
        "    observations: np.ndarray  # same as states in the theory\n",
        "    next_observations: np.ndarray\n",
        "    actions: np.ndarray\n",
        "    rewards: np.ndarray\n",
        "    terminateds: np.ndarray\n",
        "```\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://araffin.github.io/slides/dqn-tutorial/images/dqn/replay_buffer_sampling.png\" width=\"600\"/>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J25Ix7IXjfpm"
      },
      "source": [
        "### Why Use a Replay Buffer?\n",
        "\n",
        "1. **Breaking Correlations**: By storing experiences in a buffer and sampling them randomly, the replay buffer helps to break the temporal correlations between consecutive experiences. This decorrelation is vital for stabilizing the training of neural networks.\n",
        "\n",
        "2. **Improving Data Efficiency**: The replay buffer allows the algorithm to reuse past experiences multiple times. This reusability is crucial in reinforcement learning where collecting new data can be expensive or time-consuming.\n",
        "\n",
        "3. **Smoothing Training Data Distribution**: Sampling from a buffer ensures that the distribution of training data remains relatively stable over time, which helps in preventing the neural network from overfitting to recent experiences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34722639-6c01-4c5b-ab65-c8d5cb2adaad"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch as th\n",
        "from gymnasium import spaces\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rru7VZU8j-Mc"
      },
      "source": [
        "### Trade-offs about the Size of the Replay Buffer\n",
        "\n",
        "The size of the replay buffer presents a trade-off between memory usage and the diversity of experiences stored. A larger buffer can store a wider variety of experiences, improving the generalization capability of the neural network and reducing the likelihood of overfitting to specific patterns. However, it requires more memory and might retain older experiences that become less relevant as the agent learns and the environment dynamics change. On the other hand, a smaller buffer requires less memory and is more likely to sample recent experiences, which can be more relevant to the current policy. However, it captures a less diverse range of experiences, potentially limiting the neural network's ability to generalize and increasing the risk of sampling correlated experiences, which can destabilize training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQH175RDkXMZ"
      },
      "source": [
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/task_sign.png?raw=true' width=800/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d1b07ed-cde5-41ac-adfc-e1733dfb21bd"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/?size=50&id=46589&format=png&color=000000\" style=\"height:30px;display:inline\"> Task: Write the replay buffer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aafc38e2-db83-4cd9-aa26-57a04896023f"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    A simple replay buffer class to store and sample transitions.\n",
        "\n",
        "    :param buffer_size: Max number of transitions to store\n",
        "    :param observation_space: Observation space of the env,\n",
        "        contains information about the observation type and shape.\n",
        "    :param action_space: Action space of the env,\n",
        "        contains information about the number of actions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        buffer_size: int,\n",
        "        observation_space: spaces.Box,\n",
        "        action_space: spaces.Discrete,\n",
        "    ) -> None:\n",
        "        # Current position in the ring buffer\n",
        "        self.current_idx = 0\n",
        "        # Replay buffer max capacity\n",
        "        self.buffer_size = buffer_size\n",
        "        # Boolean flag to know when the buffer has reached its maximal capacity\n",
        "        self.is_full = False\n",
        "\n",
        "        self.observation_space = observation_space\n",
        "        self.action_space = action_space\n",
        "        # Create the different buffers\n",
        "        self.observations = np.zeros((buffer_size, *observation_space.shape), dtype=observation_space.dtype)\n",
        "        self.next_observations = np.zeros((buffer_size, *observation_space.shape), dtype=observation_space.dtype)\n",
        "        # The action is an integer\n",
        "        action_dim = 1\n",
        "        self.actions = np.zeros((buffer_size, action_dim), dtype=action_space.dtype)\n",
        "\n",
        "        ### YOUR CODE HERE\n",
        "\n",
        "        # TODO: create the buffers (numpy arrays) for the rewards (dtype=np.float32)\n",
        "        # and the terminated signals (dtype=bool)\n",
        "        self.rewards = ...\n",
        "        self.terminateds = ...\n",
        "\n",
        "        ### END OF YOUR CODE\n",
        "\n",
        "    def store_transition(\n",
        "        self,\n",
        "        obs: np.ndarray,\n",
        "        next_obs: np.ndarray,\n",
        "        action: int,\n",
        "        reward: float,\n",
        "        terminated: bool,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Store one transition in the buffer.\n",
        "\n",
        "        :param obs: Current observation\n",
        "        :param next_obs: Next observation\n",
        "        :param action: Action taken for the current observation\n",
        "        :param reward: Reward received after taking the action\n",
        "        :param terminated: Whether it is the end of an episode or not\n",
        "            (discarding episode truncation like timeout)\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE\n",
        "\n",
        "        # TODO:\n",
        "        # 1. Update the different buffers defined in the __init__\n",
        "        # 2. Update the pointer (`self.current_idx`). Be careful,\n",
        "        #  the pointer needs to be set to zero when reaching the end of the ring buffer.\n",
        "\n",
        "        # Update the buffers to store the new transition\n",
        "\n",
        "\n",
        "        # Update the pointer\n",
        "\n",
        "        # If the buffer is full, we start from zero again, this is a ring buffer\n",
        "        # you also need to set the flag `is_full` to True (so we know the buffer has reached its max capacity)\n",
        "\n",
        "\n",
        "        ### END OF YOUR CODE\n",
        "\n",
        "    def sample(self, batch_size: int) -> ReplayBufferSamples:\n",
        "        \"\"\"\n",
        "        Sample with replacement `batch_size` transitions from the buffer.\n",
        "\n",
        "        :param batch_size: How many transitions to sample.\n",
        "        :return: Samples from the replay buffer\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE\n",
        "\n",
        "        # TODO:\n",
        "        # 1. Retrieve the upper bound (max index that can be sampled)\n",
        "        #  it corresponds to `self.buffer_size` when the ring buffer is full (we can samples all indices)\n",
        "        # 2. Sample `batch_size` indices with replacement from the buffer\n",
        "        # (in the range [0, upper_bound[ ), numpy has a method `np.random.randint` for that ;)\n",
        "        upper_bound = ...\n",
        "        batch_indices = ...\n",
        "\n",
        "        ### END OF YOUR CODE\n",
        "\n",
        "        return ReplayBufferSamples(\n",
        "            self.observations[batch_indices],\n",
        "            self.next_observations[batch_indices],\n",
        "            self.actions[batch_indices],\n",
        "            self.rewards[batch_indices],\n",
        "            self.terminateds[batch_indices],\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7f4c768-2c3b-4eac-b025-5ed04eaa241c"
      },
      "source": [
        "testing the replay buffer, without reaching max capacity:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syNzo29DQqxJ"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "buffer_size = 1000\n",
        "buffer = ReplayBuffer(buffer_size, env.observation_space, env.action_space)\n",
        "obs, _ = env.reset()\n",
        "# Fill the buffer\n",
        "for _ in range(500):\n",
        "    action = int(env.action_space.sample())\n",
        "    next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "    # Store new transition in the replay buffer\n",
        "    buffer.store_transition(obs, next_obs, action, float(reward), terminated)\n",
        "    # Update current observation\n",
        "    obs = next_obs\n",
        "\n",
        "    done = terminated or truncated\n",
        "    if done:\n",
        "        obs, _ = env.reset()\n",
        "\n",
        "assert not buffer.is_full\n",
        "assert buffer.current_idx == 500\n",
        "samples = buffer.sample(batch_size=10)\n",
        "assert len(samples.observations) == 10\n",
        "assert samples.actions.shape == (10, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc01ab42-cc8f-4ae3-90de-3cd99de3db31"
      },
      "source": [
        "Testing the replay buffer when reaching max capacity:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94e9196c-bf2a-4433-bc36-3100a145a318"
      },
      "outputs": [],
      "source": [
        "# Fill the buffer completely\n",
        "for _ in range(1000):\n",
        "    action = int(env.action_space.sample())\n",
        "    next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "    buffer.store_transition(obs, next_obs, action, float(reward), terminated)\n",
        "    # Update current observation\n",
        "    obs = next_obs\n",
        "\n",
        "    done = terminated or truncated\n",
        "    if done:\n",
        "        obs, _ = env.reset()\n",
        "\n",
        "assert buffer.is_full\n",
        "# We did a full loop\n",
        "assert buffer.current_idx == 500\n",
        "# Check sampling with replacement\n",
        "# we should be able to sample more transitions\n",
        "# than the capacity of the ring buffer\n",
        "samples = buffer.sample(batch_size=1001)\n",
        "assert len(samples.observations) == 1001\n",
        "assert samples.actions.shape == (1001, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34619317-64d8-4872-a25e-750caa0f1588"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=110865&format=png&color=000000\" style=\"height:30px;display:inline\">  Q-Network\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://araffin.github.io/slides/dqn-tutorial/images/dqn/q_network.png\" width=\"600\"/>\n",
        "</div>\n",
        "\n",
        "Similiar to FQI, the Q-Network estimates $Q_\\pi(s_t, a_t)$, however, instead of taking the action as input, it outputs q-values for all possible actions as output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWBKbuj5kZEv"
      },
      "source": [
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/task_sign.png?raw=true' width=800/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a0de4e1-c605-4b8e-bc79-585de22dd4e6"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/?size=50&id=46589&format=png&color=000000\" style=\"height:30px;display:inline\"> Task: Write the Q-Network using PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7210df0c-4e0c-4dc0-a893-b5a1ccb815d9"
      },
      "outputs": [],
      "source": [
        "from typing import Type\n",
        "\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "from gymnasium import spaces\n",
        "\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    A Q-Network for the DQN algorithm\n",
        "    to estimate the q-value for a given observation.\n",
        "\n",
        "    :param observation_space: Observation space of the env,\n",
        "        contains information about the observation type and shape.\n",
        "    :param action_space: Action space of the env,\n",
        "        contains information about the number of actions.\n",
        "    :param n_hidden_units: Number of units for each hidden layer.\n",
        "    :param activation_fn: Activation function (ReLU by default)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_space: spaces.Box,\n",
        "        action_space: spaces.Discrete,\n",
        "        n_hidden_units: int = 64,\n",
        "        activation_fn: Type[nn.Module] = nn.ReLU,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        # Assume 1d space\n",
        "        obs_dim = observation_space.shape[0]\n",
        "\n",
        "        ### YOUR CODE HERE\n",
        "        # TODO:\n",
        "        # 1. Retrieve the number of discrete actions,\n",
        "        # that will be the number of ouputs of the q-network\n",
        "        # 2. Create the q-network, it will be a two layers fully-connected\n",
        "        # neural network which take the state (observation) as input\n",
        "        # and outputs the q-values for all possible actions\n",
        "\n",
        "        # Retrieve the number of discrete actions (using attribute `n` from `action_space`)\n",
        "\n",
        "\n",
        "        # Create the q network: a 2 fully connected hidden layers with `n_hidden_units` each\n",
        "        # with `activation_fn` for the activation function after each hidden layer.\n",
        "        # You should use `nn.Sequential` (combine several layers to create a network)\n",
        "        # `nn.Linear` (fully connected layer) from PyTorch.\n",
        "        self.q_net = ...\n",
        "\n",
        "        ### END OF YOUR CODE\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        \"\"\"\n",
        "        :param observations: A batch of observation (batch_size, obs_dim)\n",
        "        :return: The Q-values for the given observations\n",
        "            for all the action (batch_size, n_actions)\n",
        "        \"\"\"\n",
        "        return self.q_net(observations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d2d22ad-4a38-49ab-849d-276b233c972a"
      },
      "source": [
        "To test your Q-Network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6384a47-4f34-4ae6-96c7-38107d097e50"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "q_net = QNetwork(env.observation_space, env.action_space)\n",
        "\n",
        "print(q_net)\n",
        "assert isinstance(q_net.q_net, nn.Sequential)\n",
        "assert len(q_net.q_net) == 5\n",
        "assert isinstance(q_net.q_net[0], nn.Linear)\n",
        "assert isinstance(q_net.q_net[1], nn.ReLU)\n",
        "assert isinstance(q_net.q_net[-1], nn.Linear)\n",
        "\n",
        "obs, _ = env.reset()\n",
        "\n",
        "with th.no_grad():\n",
        "    # Create the input to the Q-Network\n",
        "    obs_tensor = th.as_tensor(obs[np.newaxis, ...])\n",
        "    q_values = q_net(obs_tensor)\n",
        "\n",
        "    assert q_values.shape == (1, 2)\n",
        "\n",
        "    best_action = q_values.argmax().item()\n",
        "\n",
        "    assert isinstance(best_action, int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61ba21ac-b2c3-4d23-bf5a-6ea1ffd62ec7"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=42832&format=png&color=000000\" style=\"height:30px;display:inline\"> Epsilon-greedy data collection\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99rj7QranRNc"
      },
      "source": [
        "Effective exploration strategies are critical for the agent to learn an optimal policy, especially in complex environments with large state and action spaces. By managing the exploration-exploitation trade-off appropriately, DQN can achieve robust learning and better performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nebJ3PZrlzKW"
      },
      "source": [
        "To handle the exploration-exploitation trade-off in DQN, a common strategy is the use of epsilon-greedy action selection. This strategy involves the following steps:\n",
        "\n",
        "1. **Epsilon-Greedy Policy**: The agent selects a random action with probability epsilon ($ϵ$) and chooses the action that maximizes the Q-value with probability (1−$ϵ$). This ensures a balance between exploration and exploitation.\n",
        "\n",
        "2. **Epsilon ($ϵ$)**: The exploration rate, which starts high to encourage exploration and is gradually reduced over time to favor exploitation as the agent becomes more knowledgeable about the environment.\n",
        "\n",
        "3. **Decay of Epsilon**: Over the course of training, epsilon is decayed according to a schedule, reducing the exploration rate. Initially, the agent explores more to gather diverse experiences, and as learning progresses, it exploits the learned policy more frequently. A common approach is to linearly decay epsilon from a high value (e.g., 1.0) to a low value (e.g., 0.1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lImjS8r6kdgJ"
      },
      "source": [
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/task_sign.png?raw=true' width=800/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a3fae51-6330-495c-96dd-46616d3c71ed"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/?size=50&id=46589&format=png&color=000000\" style=\"height:30px;display:inline\"> Task: Write the epsilon-greedy action selection strategy\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://araffin.github.io/slides/dqn-tutorial/images/dqn/epsilon_greedy.png\" width=\"600\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f66aafe-b4eb-4878-af9a-b83b29e0d78a"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy_action_selection(\n",
        "    q_net: QNetwork,\n",
        "    observation: np.ndarray,\n",
        "    exploration_rate: float,\n",
        "    action_space: spaces.Discrete,\n",
        "    device: str = \"cpu\",\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Select an action according to an espilon-greedy policy:\n",
        "    with a probability of epsilon (`exploration_rate`),\n",
        "    sample a random action, otherwise follow the best known action\n",
        "    according to the q-value.\n",
        "\n",
        "    :param observation: A single observation.\n",
        "    :param q_net: Q-network for estimating the q value\n",
        "    :param exploration_rate: Current rate of exploration (in [0, 1], 0 means no exploration),\n",
        "        probability to select a random action,\n",
        "        this is \"epsilon\".\n",
        "    :param action_space: Action space of the env,\n",
        "        contains information about the number of actions.\n",
        "    :param device: PyTorch device\n",
        "    :return: An action selected according to the epsilon-greedy policy.\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE\n",
        "    # TODO:\n",
        "    # 1. Toss a biased coin (you can use `np.random.rand()`)\n",
        "    # to decide if the agent should take a random action or not\n",
        "    # (with probability p=`exploration_rate`)\n",
        "    # 2. Either take a random action (sample the action space)\n",
        "    # or follow the greedy policy (take the action with the highest q-value)\n",
        "\n",
        "    if ...:\n",
        "        # Random action\n",
        "        action = ...\n",
        "    else:\n",
        "        # Greedy action\n",
        "        # We do not need to compute the gradient, so we use `with th.no_grad():`\n",
        "        with th.no_grad():\n",
        "            # Convert the input to PyTorch tensor and add a batch dimension (obs_dim,) -> (1, obs_dim)\n",
        "            # you can use `th.as_tensor` and numpy `[np.newaxis, ...]`\n",
        "\n",
        "            # Compute q values for all actions\n",
        "\n",
        "            # Greedy policy: select action with the highest q value\n",
        "            # you can use PyTorch `.argmax()` for that\n",
        "            action = ...\n",
        "\n",
        "    ### END OF YOUR CODE\n",
        "\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1c6823ed-8f88-48dc-a5a9-4e9d260603ea"
      },
      "outputs": [],
      "source": [
        "def collect_one_step(\n",
        "    env: gym.Env,\n",
        "    q_net: QNetwork,\n",
        "    replay_buffer: ReplayBuffer,\n",
        "    obs: np.ndarray,\n",
        "    exploration_rate: float = 0.1,\n",
        "    verbose: int = 0,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Collect one transition and fill the replay buffer following an epsilon greedy policy.\n",
        "\n",
        "    :param env: The environment object.\n",
        "    :param q_net: Q-network for estimating the q value\n",
        "    :param replay_buffer: Replay buffer to store the new transitions.\n",
        "    :param obs: The current observation.\n",
        "    :param exploration_rate: Current rate of exploration (in [0, 1], 0 means no exploration),\n",
        "        probability to select a random action,\n",
        "        this is \"epsilon\".\n",
        "    :param verbose: The verbosity level (1 to print some info).\n",
        "    :return: The last observation (important when collecting data multiple times).\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    # Select an action following an epsilon-greedy policy\n",
        "    # you should use `epsilon_greedy_action_selection()`\n",
        "    action = epsilon_greedy_action_selection(q_net, obs, exploration_rate, env.action_space)\n",
        "    # Step in the env\n",
        "    next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "    # Store the transition in the replay buffer\n",
        "    replay_buffer.store_transition(obs, next_obs, action, float(reward), terminated)\n",
        "    # Update current observation\n",
        "    obs = next_obs\n",
        "\n",
        "    ### END OF YOUR CODE\n",
        "\n",
        "    if \"episode\" in info and verbose >= 1:\n",
        "        print(f\"Episode return={float(info['episode']['r']):.2f} length={int(info['episode']['l'])}\")\n",
        "\n",
        "    done = terminated or truncated\n",
        "    if done:\n",
        "        # Don't forget to reset the env at the end of an episode\n",
        "        obs, _ = env.reset()\n",
        "    return obs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e72093a8-b227-40e5-a104-191b086e33c2"
      },
      "source": [
        "Let's test the data collection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paLkz7fhQqxK"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "q_net = QNetwork(env.observation_space, env.action_space)\n",
        "buffer = ReplayBuffer(2000, env.observation_space, env.action_space)\n",
        "\n",
        "obs, _ = env.reset()\n",
        "for _ in range(1000):\n",
        "    obs = collect_one_step(env, q_net, buffer, obs, exploration_rate=0.1)\n",
        "\n",
        "# Check current buffer position\n",
        "assert buffer.current_idx == 1000\n",
        "\n",
        "# Collect more data\n",
        "for _ in range(1000):\n",
        "    obs = collect_one_step(env, q_net, buffer, obs, exploration_rate=0.1)\n",
        "\n",
        "# Buffer is full\n",
        "assert buffer.current_idx == 0\n",
        "assert buffer.is_full"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb9bf659-e0e3-42f7-b774-1c989bfc6c8f"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/?size=50&id=49296&format=png&color=000000\" style=\"height:30px;display:inline\"> Exploration Schedule\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://araffin.github.io/slides/dqn-tutorial/images/dqn/linear_schedule.png\" width=\"600\"/>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "251042a1-0250-4559-8f75-db5c7fd67d2a"
      },
      "outputs": [],
      "source": [
        "def linear_schedule(initial_value: float, final_value: float, current_step: int, max_steps: int) -> float:\n",
        "    \"\"\"\n",
        "    Linear schedule for the exploration rate (epsilon).\n",
        "    Note: we clip the value so the schedule is constant after reaching the final value\n",
        "    at `max_steps`.\n",
        "\n",
        "    :param initial_value: Initial value of the schedule.\n",
        "    :param final_value: Final value of the schedule.\n",
        "    :param current_step: Current step of the schedule.\n",
        "    :param max_steps: Maximum number of steps of the schedule.\n",
        "    :return: The current value of the schedule.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Compute current progress (in [0, 1], 0 being the start)\n",
        "    progress = current_step / max_steps\n",
        "    # Clip the progress so the schedule is constant after reaching the final value\n",
        "    progress = min(progress, 1.0)\n",
        "    current_value = initial_value + progress * (final_value - initial_value)\n",
        "\n",
        "\n",
        "\n",
        "    return current_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59df7864-f45f-4503-9d21-b07dc22feece"
      },
      "source": [
        "To test the linear schedule:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6f95256-6a76-4323-849e-886741da0653"
      },
      "outputs": [],
      "source": [
        "# Linear schedule\n",
        "exploration_initial_eps = 1.0\n",
        "exploration_final_eps = 0.01\n",
        "exploration_rate = exploration_initial_eps\n",
        "n_steps = 100\n",
        "for step in range(n_steps + 1):\n",
        "    exploration_rate = linear_schedule(exploration_initial_eps, exploration_final_eps, step, n_steps)\n",
        "    if step == 0:\n",
        "        assert exploration_rate == exploration_initial_eps\n",
        "\n",
        "    obs = collect_one_step(env, q_net, buffer, obs, exploration_rate=exploration_rate)\n",
        "\n",
        "assert np.allclose(exploration_rate, exploration_final_eps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82559d16-3fc6-4d47-a373-f94adcab7102"
      },
      "source": [
        "## DQN Update rule (no target network)\n",
        "<div>\n",
        "    <img src=\"https://araffin.github.io/slides/dqn-tutorial/images/dqn/annotated_dqn.png\" width=\"1000\"/>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukAAyrK4oQtm"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=YgRtmfcjZcf8&format=png&color=000000\" style=\"height:30px;display:inline\"> Target Network\n",
        "\n",
        "In Deep Q-Learning, the Target Q Network is a crucial component designed to stabilize the training process. It addresses the instability issues that arise due to the moving target problem when updating Q-values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv5xvK6woW6t"
      },
      "source": [
        "A Target Q Network is a separate neural network used to generate target Q-values for the training updates. It is a copy of the primary Q Network, but its parameters are updated less frequently. This means that while the primary Q Network is updated at each training step, the Target Q Network remains fixed for a certain number of steps, providing a stable target for the Q-value updates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6L5b2pJoaw2"
      },
      "source": [
        "<div>\n",
        "    <img src=\"https://araffin.github.io/slides/dqn-tutorial/images/dqn/target_q_network.png\" width=\"1000\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b269a237-a0db-40a4-9621-449538c410b7"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "The only things that is changing is when predicting the next q value.\n",
        "\n",
        "In DQN without target, the online network with weights **$\\theta$** is used:\n",
        "\n",
        "$y = r_t + \\gamma \\cdot \\max_{a \\in A}(\\hat{Q}_{\\pi}(s_{t+1}, a; \\theta))$\n",
        "\n",
        "\n",
        "whereas with DQN with target network, the target q-network (a delayed copy of the q-network) with weights **$\\theta^\\prime$** is used instead:\n",
        "\n",
        "$y = r_t + \\gamma \\cdot \\max_{a \\in A}(\\hat{Q}_{\\pi}(s_{t+1}, a; \\theta^\\prime))$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbg3TeTEoe02"
      },
      "source": [
        "## Why Use a Target Q Network?\n",
        "\n",
        "1. **Stabilizing Training**: One of the main reasons to use a Target Q Network is to stabilize training. In Q-learning, the target for the Q-value update is derived from the same Q Network being updated, leading to a moving target problem. This can cause instability and divergence during training. By using a separate Target Q Network, the target values remain more stable over short periods, allowing for more reliable updates.\n",
        "\n",
        "2. **Reducing Oscillations**: Without a Target Q Network, the learning process can oscillate or even diverge due to the constantly changing Q-values. The Target Q Network helps in smoothing these changes, providing a more consistent learning signal.\n",
        "\n",
        "3. **Improving Convergence**: By decoupling the target Q-value generation from the learning Q-values, the agent can converge to a more optimal policy. The periodic updates to the Target Q Network ensure that the target values gradually follow the improvements in the Q Network, promoting stable learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lykDAdmkn2w"
      },
      "source": [
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/task_sign.png?raw=true' width=800/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "781e7e78-b156-43f0-aa4d-f7c842d3d837"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/?size=50&id=46589&format=png&color=000000\" style=\"height:30px;display:inline\">Task: Write the DQN update with target network\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27f17152-59cf-495a-bde7-73b6f219e779"
      },
      "outputs": [],
      "source": [
        "%%script ture\n",
        "def dqn_update(\n",
        "    q_net: QNetwork,\n",
        "    q_target_net: QNetwork,\n",
        "    optimizer: th.optim.Optimizer,\n",
        "    replay_buffer: ReplayBuffer,\n",
        "    batch_size: int,\n",
        "    gamma: float,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Perform one gradient step on the Q-network\n",
        "    using the data from the replay buffer.\n",
        "\n",
        "    :param q_net: The Q-network to update\n",
        "    :param q_target_net: The target Q-network, to compute the td-target.\n",
        "    :param optimizer: The optimizer to use\n",
        "    :param replay_buffer: The replay buffer containing the transitions\n",
        "    :param batch_size: The minibatch size, how many transitions to sample\n",
        "    :param gamma: The discount factor\n",
        "    \"\"\"\n",
        "\n",
        "    # Sample the replay buffer and convert them to PyTorch tensors\n",
        "    replay_data = replay_buffer.sample(batch_size).to_torch()\n",
        "\n",
        "    with th.no_grad():\n",
        "        ### YOUR CODE HERE\n",
        "        # TODO: use the target q-network instead of the online q-network\n",
        "        # to compute the next values\n",
        "\n",
        "        # Compute the Q-values for the next observations (batch_size, n_actions)\n",
        "        # using the target network\n",
        "\n",
        "        # Follow greedy policy: use the one with the highest value\n",
        "        # (batch_size,)\n",
        "\n",
        "        # If the episode is terminated, set the target to the reward\n",
        "\n",
        "        # 1-step TD target\n",
        "\n",
        "        ### END OF YOUR CODE\n",
        "\n",
        "    # Get current Q-values estimates for the replay_data (batch_size, n_actions)\n",
        "    q_values = q_net(replay_data.observations)\n",
        "    # Select the Q-values corresponding to the actions that were selected\n",
        "    # during data collection\n",
        "    current_q_values = th.gather(q_values, dim=1, index=replay_data.actions)\n",
        "    # Reshape from (batch_size, 1) to (batch_size,) to avoid broadcast error\n",
        "    current_q_values = current_q_values.squeeze(dim=1)\n",
        "\n",
        "    # Check for any shape/broadcast error\n",
        "    # Current q-values must have the same shape as the TD target\n",
        "    assert current_q_values.shape == (batch_size,), f\"{current_q_values.shape} != {(batch_size,)}\"\n",
        "    assert current_q_values.shape == td_target.shape, f\"{current_q_values.shape} != {td_target.shape}\"\n",
        "\n",
        "    # Compute the Mean Squared Error (MSE) loss\n",
        "    # Optionally, one can use a Huber loss instead of the MSE loss\n",
        "    loss = ((current_q_values - td_target) ** 2).mean()\n",
        "    # Huber loss\n",
        "    # loss = th.nn.functional.smooth_l1_loss(current_q_values, td_target)\n",
        "\n",
        "    # Reset gradients\n",
        "    optimizer.zero_grad()\n",
        "    # Compute the gradients\n",
        "    loss.backward()\n",
        "    # Update the parameters of the q-network\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85c74b0e-1923-4c6e-9a58-f83a97435f0d"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=46621&format=png&color=000000\" style=\"height:30px;display:inline\"> Updated training loop\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZQzQvV9qdsa"
      },
      "source": [
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/fixed-q-target-pseudocode.jpg?raw=true' width=800/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtI-M-vzRQc3"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_dqn(\n",
        "    env_id: str = \"CartPole-v1\",\n",
        "    replay_buffer_size: int = 50_000,\n",
        "    # How often do we copy the parameters from the Q-network to the target network\n",
        "    target_network_update_interval: int = 1000,\n",
        "    # Warmup phase\n",
        "    learning_starts: int = 100,\n",
        "    # Exploration schedule\n",
        "    # (for the epsilon-greedy data collection)\n",
        "    exploration_initial_eps: float = 1.0,\n",
        "    exploration_final_eps: float = 0.01,\n",
        "    exploration_fraction: float = 0.1,\n",
        "    n_timesteps: int = 20_000,\n",
        "    update_interval: int = 2,\n",
        "    learning_rate: float = 3e-4,\n",
        "    batch_size: int = 64,\n",
        "    gamma: float = 0.99,\n",
        "    n_hidden_units: int = 64,\n",
        "    n_eval_episodes: int = 10,\n",
        "    evaluation_interval: int = 1000,\n",
        "    eval_exploration_rate: float = 0.0,\n",
        "    seed: int = 2023,\n",
        "    # device: Union[th.device, str] = \"cpu\",\n",
        "    eval_render_mode: Optional[str] = None,  # \"human\", \"rgb_array\", None\n",
        ") -> QNetwork:\n",
        "    \"\"\"\n",
        "    Run Deep Q-Learning (DQN) on a given environment.\n",
        "    (with a target network)\n",
        "\n",
        "    :param env_id: Name of the environment\n",
        "    :param replay_buffer_size: Max capacity of the replay buffer\n",
        "    :param target_network_update_interval: How often do we copy the parameters\n",
        "         to the target network\n",
        "    :param learning_starts: Warmup phase to fill the replay buffer\n",
        "        before starting the optimization.\n",
        "    :param exploration_initial_eps: The initial exploration rate\n",
        "    :param exploration_final_eps: The final exploration rate\n",
        "    :param exploration_fraction: The fraction of the number of steps\n",
        "        during which the exploration rate is annealed from\n",
        "        initial_eps to final_eps.\n",
        "        After this many steps, the exploration rate remains constant.\n",
        "    :param n_timesteps: Number of timesteps in total\n",
        "    :param update_interval: How often to update the Q-network\n",
        "        (every update_interval steps)\n",
        "    :param learning_rate: The learning rate to use for the optimizer\n",
        "    :param batch_size: The minibatch size\n",
        "    :param gamma: The discount factor\n",
        "    :param n_hidden_units: Number of units for each hidden layer\n",
        "        of the Q-Network.\n",
        "    :param n_eval_episodes: The number of episodes to evaluate the policy on\n",
        "    :param evaluation_interval: How often to evaluate the policy\n",
        "    :param eval_exploration_rate: The exploration rate to use during evaluation\n",
        "    :param seed: Random seed for the pseudo random generator\n",
        "    :param eval_render_mode: The render mode to use for evaluation\n",
        "    \"\"\"\n",
        "    # Set seed for reproducibility\n",
        "    # Seed Numpy as PyTorch pseudo random generators\n",
        "    # Seed Numpy RNG\n",
        "    np.random.seed(seed)\n",
        "    # seed the RNG for all devices (both CPU and CUDA)\n",
        "    th.manual_seed(seed)\n",
        "\n",
        "    # Create the environment\n",
        "    env = gym.make(env_id)\n",
        "    # For highway env\n",
        "    env = gym.wrappers.FlattenObservation(env)\n",
        "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "    assert isinstance(env.observation_space, spaces.Box)\n",
        "    assert isinstance(env.action_space, spaces.Discrete)\n",
        "    env.action_space.seed(seed)\n",
        "\n",
        "    # Create the evaluation environment\n",
        "    eval_env = gym.make(env_id, render_mode=eval_render_mode)\n",
        "    eval_env = gym.wrappers.FlattenObservation(eval_env)\n",
        "    eval_env.reset(seed=seed)\n",
        "    eval_env.action_space.seed(seed)\n",
        "\n",
        "    # Create the q-network\n",
        "    q_net = QNetwork(env.observation_space, env.action_space, n_hidden_units=n_hidden_units)\n",
        "    # Create the target network\n",
        "    q_target_net = QNetwork(env.observation_space, env.action_space, n_hidden_units=n_hidden_units)\n",
        "    # Copy the parameters of the q-network to the target network\n",
        "    q_target_net.load_state_dict(q_net.state_dict())\n",
        "\n",
        "    # For flappy bird\n",
        "    if env.observation_space.dtype == np.float64:\n",
        "        q_net.double()\n",
        "        q_target_net.double()\n",
        "\n",
        "    # Create the optimizer, we only optimize the parameters of the q-network\n",
        "    optimizer = th.optim.Adam(q_net.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Create the Replay buffer\n",
        "    replay_buffer = ReplayBuffer(replay_buffer_size, env.observation_space, env.action_space)\n",
        "    # Reset the env\n",
        "    obs, _ = env.reset(seed=seed)\n",
        "    for current_step in range(1, n_timesteps + 1):\n",
        "        # Update the current exploration schedule (update the value of epsilon)\n",
        "        exploration_rate = linear_schedule(\n",
        "            exploration_initial_eps,\n",
        "            exploration_final_eps,\n",
        "            current_step,\n",
        "            int(exploration_fraction * n_timesteps),\n",
        "        )\n",
        "        # Do one step in the environment following an epsilon-greedy policy\n",
        "        # and store the transition in the replay buffer\n",
        "        obs = collect_one_step(\n",
        "            env,\n",
        "            q_net,\n",
        "            replay_buffer,\n",
        "            obs,\n",
        "            exploration_rate=exploration_rate,\n",
        "            verbose=0,\n",
        "        )\n",
        "\n",
        "        # Update the target network\n",
        "        # by copying the parameters from the Q-network every target_network_update_interval steps\n",
        "        if (current_step % target_network_update_interval) == 0:\n",
        "            q_target_net.load_state_dict(q_net.state_dict())\n",
        "\n",
        "        # Update the Q-network every update_interval steps\n",
        "        # after learning_starts steps have passed (warmup phase)\n",
        "        if (current_step % update_interval) == 0 and current_step > learning_starts:\n",
        "            # Do one gradient step\n",
        "            dqn_update(q_net, q_target_net, optimizer, replay_buffer, batch_size, gamma=gamma)\n",
        "\n",
        "        if (current_step % evaluation_interval) == 0:\n",
        "            print()\n",
        "            print(f\"Evaluation at step {current_step}:\")\n",
        "            print(f\"exploration_rate={exploration_rate:.2f}\")\n",
        "            # Evaluate the current greedy policy (deterministic policy)\n",
        "            evaluate_policy(eval_env, q_net, n_eval_episodes, eval_exploration_rate=eval_exploration_rate)\n",
        "            # Save a checkpoint\n",
        "            th.save(q_net.state_dict(), f\"../logs/q_net_checkpoint_{env_id}_{current_step}.pth\")\n",
        "    return q_net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14af9427-8166-40fa-a581-51599d3be306"
      },
      "source": [
        "### Train DQN agent with target network on CartPole env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55922877-c7b1-4772-ad1a-e4c1f3ac67d6"
      },
      "outputs": [],
      "source": [
        "# Tuned hyperparameters from the RL Zoo3 of the Stable Baselines3 library\n",
        "# https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/dqn.yml\n",
        "\n",
        "env_id = \"CartPole-v1\"\n",
        "\n",
        "q_net = run_dqn(\n",
        "    env_id=env_id,\n",
        "    replay_buffer_size=100_000,\n",
        "    # Note: you can remove the target network\n",
        "    # by setting target_network_update_interval=1\n",
        "    target_network_update_interval=10,\n",
        "    learning_starts=1000,\n",
        "    exploration_initial_eps=1.0,\n",
        "    exploration_final_eps=0.04,\n",
        "    exploration_fraction=0.1,\n",
        "    n_timesteps=80_000,\n",
        "    update_interval=2,\n",
        "    learning_rate=1e-3,\n",
        "    batch_size=64,\n",
        "    gamma=0.99,\n",
        "    n_eval_episodes=10,\n",
        "    evaluation_interval=5000,\n",
        "    # No exploration during evaluation\n",
        "    # (deteministic policy)\n",
        "    eval_exploration_rate=0.0,\n",
        "    seed=2022,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52450a1f-60b4-4098-bc64-5860476a50cc"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=46759&format=png&color=000000\" style=\"height:30px;display:inline\"> Visualize the trained agent\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56e18bf9-e3e4-495d-be4b-6acf0ae28ee1"
      },
      "outputs": [],
      "source": [
        "eval_env = gym.make(env_id, render_mode=\"rgb_array\")\n",
        "n_eval_episodes = 3\n",
        "eval_exploration_rate = 0.0\n",
        "video_name = f\"DQN_{env_id}\"\n",
        "\n",
        "# Optional: load checkpoint\n",
        "# q_net = QNetwork(eval_env.observation_space, eval_env.action_space, n_hidden_units=64)\n",
        "# q_net.load_state_dict(th.load(\"../logs/q_net_checkpoint_CartPole-v1_75000.pth\"))\n",
        "\n",
        "evaluate_policy(\n",
        "    eval_env,\n",
        "    q_net,\n",
        "    n_eval_episodes,\n",
        "    eval_exploration_rate=eval_exploration_rate,\n",
        "    video_name=video_name,\n",
        ")\n",
        "\n",
        "show_videos(\"../logs/videos/\", prefix=video_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "642e02d8-bd9c-4a07-9a0d-86748dcdd391"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=y_hu-u4x2vis&format=png&color=000000\" style=\"height:30px;display:inline\"> Training DQN agent on flappy bird:\n",
        "\n",
        "You can go in the [GitHub repo](https://github.com/araffin/flappy-bird-gymnasium/tree/patch-1) to learn more about this environment.\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://raw.githubusercontent.com/markub3327/flappy-bird-gymnasium/main/imgs/dqn.gif\" width=\"300\"/>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75e9703d-7b5b-4188-9737-786476a627ec"
      },
      "outputs": [],
      "source": [
        "!pip install \"flappy-bird-gymnasium @ git+https://github.com/araffin/flappy-bird-gymnasium@patch-1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2a0eef2-d652-4224-86c6-8f669e82ec3e"
      },
      "outputs": [],
      "source": [
        "import flappy_bird_gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf79677e-561c-4bb5-95d2-d4a99eb0579c"
      },
      "outputs": [],
      "source": [
        "env_id = \"FlappyBird-v0\"\n",
        "\n",
        "q_net = run_dqn(\n",
        "    env_id=env_id,\n",
        "    replay_buffer_size=100_000,\n",
        "    # Note: you can remove the target network\n",
        "    # by setting target_network_update_interval=1\n",
        "    target_network_update_interval=250,\n",
        "    learning_starts=10_000,\n",
        "    exploration_initial_eps=1.0,\n",
        "    exploration_final_eps=0.03,\n",
        "    exploration_fraction=0.1,\n",
        "    n_timesteps=500_000,\n",
        "    update_interval=4,\n",
        "    learning_rate=1e-3,\n",
        "    batch_size=128,\n",
        "    gamma=0.98,\n",
        "    n_eval_episodes=5,\n",
        "    evaluation_interval=50000,\n",
        "    n_hidden_units=256,\n",
        "    # No exploration during evaluation\n",
        "    # (deteministic policy)\n",
        "    eval_exploration_rate=0.0,\n",
        "    seed=2023,\n",
        "    eval_render_mode=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a08cc96-2601-49dc-921c-44e2a048df81"
      },
      "source": [
        "### Record a video of the trained agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9e191f2-e3b3-41d0-bff2-50e1b9c9d19c"
      },
      "outputs": [],
      "source": [
        "eval_env = gym.make(env_id, render_mode=\"rgb_array\")\n",
        "n_eval_episodes = 3\n",
        "eval_exploration_rate = 0.00\n",
        "video_name = f\"DQN_{env_id}\"\n",
        "\n",
        "\n",
        "# Optional: load checkpoint\n",
        "q_net = QNetwork(eval_env.observation_space, eval_env.action_space, n_hidden_units=256)\n",
        "# Convert weights from float32 to float64 to match flappy bird obs\n",
        "q_net.double()\n",
        "q_net.load_state_dict(th.load(\"../logs/q_net_checkpoint_FlappyBird-v0_200000.pth\"))\n",
        "\n",
        "evaluate_policy(\n",
        "    eval_env,\n",
        "    q_net,\n",
        "    n_eval_episodes,\n",
        "    eval_exploration_rate=eval_exploration_rate,\n",
        "    video_name=video_name,\n",
        ")\n",
        "\n",
        "show_videos(\"../logs/videos/\", prefix=video_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0to0Fw0to1W2"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=55162&format=png&color=000000\" style=\"height:30px;display:inline\"> Improving Q-Learning\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrDYskAso5_M"
      },
      "source": [
        "### Overestimation in Q-Learning\n",
        "One significant issue in Q-learning is overestimation. When calculating target values, Q-learning uses the maximum Q-value of possible actions, which can lead to overestimation. This occurs because the expected value of the maximum of multiple estimates is generally greater than the maximum of their expected values. Essentially, the noise in the Q-value estimates can lead to selecting actions that appear better than they are due to the variability in the estimates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sLhJDMMo9Cx"
      },
      "source": [
        "### Addressing Overestimation with Double Q-Learning\n",
        "Double Q-learning addresses the overestimation problem by using two separate networks: one to select the action and the other to evaluate the action's value. This method helps decorrelate the noise in the action selection from the noise in the value evaluation. By doing so, it mitigates the overestimation issue, leading to more accurate Q-value predictions and more stable learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EUq5DQPo_gL"
      },
      "source": [
        "### Multi-Step Returns\n",
        "Another improvement in Q-learning is the use of multi-step returns. Instead of relying solely on one-step backups, which are highly biased but have low variance, multi-step returns sum rewards over multiple steps. This approach reduces bias as the target values include more true rewards, thus providing a better learning signal. However, it increases variance since the summation over multiple steps introduces more noise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIPg_jf9pEwW"
      },
      "source": [
        "### Off-Policy Considerations with Multi-Step Returns\n",
        "\n",
        "Multi-step returns are only valid with on-policy data, where the policy used to generate the data matches the current policy being learned. With off-policy data, using multi-step returns can be problematic because the actions in the sampled data might not align with the new policy's actions. Several solutions exist for this issue:\n",
        "\n",
        "1. **Ignoring the Problem**: This often works well in practice despite the theoretical drawbacks.\n",
        "2. **Dynamically Cutting the Trace**: Adjust the number of steps, 𝑛, to match on-policy data, ensuring that the sampled actions align with what the policy would have done.\n",
        "3. **Importance Sampling**: Use a stochastic policy and importance weighting to adjust the multi-step returns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhPZ_J44ppUj"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=Vm9v3PaQFGMr&format=png&color=000000\" style=\"height:30px;display:inline\"> Q-Learning with Continuous Actions\n",
        "\n",
        "\n",
        "\n",
        "So far, when we talked about Q-learning algorithms, we mainly focused on algorithms with discrete action spaces. It is actually possible but somewhat more complicated to extend Q-learning procedures to the case when we have continuous actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4edw2NKpv0G"
      },
      "source": [
        "### The Problem with Continuous Actions\n",
        "\n",
        "The problem is that when you select your actions, you need to perform this argmax. An argmax over discrete actions is pretty straightforward; you simply evaluate the Q-value for every possible action and take the best one. But when you have continuous actions, this is, of course, much harder. This comes up in two places: when evaluating the argmax policy and when computing the target value, which requires the max or, in the case of double Q-learning, also an argmax. The target value max is particularly problematic because that happens in the inner loop of training, so you really want it to be very fast and very efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SurJwGDWp4Vb"
      },
      "source": [
        "#### Continuous Optimization Procedure\n",
        "\n",
        "A simple solution is to approximate the max over a continuous action as the max over a discrete set of actions that are sampled randomly. For instance, you could sample a set of $n$ actions, maybe uniformly at random from the set of valid actions, and then take the Q-value with the largest of those actions. This method is straightforward and efficient to parallelize but might suffer from inaccuracy as the action space dimensionality increases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txb_hv_LqFdw"
      },
      "source": [
        "#### Using a Function Class with Easy Optimization\n",
        "\n",
        "Another option is to use a function class that is inherently easy to optimize. For example, quadratic functions have closed-form solutions for their optima. The [NAF](https://towardsdatascience.com/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095) (Normalized Advantage Function) architecture is one such approach, where a neural network outputs a quadratic function in the action. This method simplifies the maximization operation but reduces the representational capacity of the Q-function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VitawKgSqMaA"
      },
      "source": [
        "#### Learning an Approximate Maximizer\n",
        "\n",
        "The third option is to perform Q-learning with continuous actions by learning an approximate maximizer. This approach involves training a second neural network to perform the maximization. This method is used in algorithms like [DDPG](https://spinningup.openai.com/en/latest/algorithms/ddpg.html) (Deep Deterministic Policy Gradient), where a policy network approximates the argmax of the Q-function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f175852a-f29b-430a-97b8-6565bc3a503f"
      },
      "source": [
        "# <img src=\"https://img.icons8.com/?size=100&id=46509&format=png&color=000000\" style=\"height:50px;display:inline\"> Conclusion\n",
        "---\n",
        "\n",
        "In this tutorial, we covered the transition from Fitted Q-Iteration (FQI) to Deep Q-Networks (DQN). We started by understanding FQI as a regression problem designed to learn Q-values for state-action pairs. Despite its strengths, FQI faces limitations such as challenges in scaling, looping over all possible actions, and instability in target updates.\n",
        "\n",
        "To address these issues, we moved to DQN, which uses neural networks to handle large state spaces, replay buffers to mitigate sample correlation, and target networks to stabilize learning. The trade-offs between offline and online reinforcement learning were also discussed, highlighting the advantages of an online approach for continuous learning.\n",
        "\n",
        "The core components of DQN—Q-network, replay buffer, exploration vs. exploitation strategies, and target Q-network—each play a crucial role in overcoming the limitations of FQI, making DQN a powerful tool for complex reinforcement learning tasks. This tutorial has provided a concise understanding of these components and their importance in creating a stable and efficient reinforcement learning algorithm.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_Ndmee2iL02"
      },
      "source": [
        "# <img src=\"https://img.icons8.com/dusk/64/000000/plus-2-math.png\" style=\"height:50px;display:inline\"> Further Reading\n",
        "---\n",
        "\n",
        "\n",
        "For those interested in diving deeper into Q-learning and its developments, here are some seminal and influential papers in the field:\n",
        "\n",
        "### Classic Papers\n",
        "\n",
        "- **Watkins (1989)**. [\"Learning from Delayed Rewards\"](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf): This paper introduces the Q-learning algorithm, a foundational work in reinforcement learning.\n",
        "- **Riedmiller (2005)**. [\"Neural Fitted Q-Iteration\"](https://www.ias.informatik.tu-darmstadt.de/uploads/Team/JanPeters/riedmiller05nips.pdf): This paper discusses batch-mode Q-learning with neural networks, a significant step towards modern deep reinforcement learning.\n",
        "\n",
        "### Deep Reinforcement Learning Q-Learning Papers\n",
        "\n",
        "- **Lange & Riedmiller (2010)**. [\"Deep Auto-Encoder Neural Networks in Reinforcement Learning\"](https://ml.informatik.uni-freiburg.de/former/_media/publications/lange-2010-icann.pdf): An early image-based Q-learning method using autoencoders to construct embeddings.\n",
        "- **Mnih et al. (2013)**. [\"Human-Level Control through Deep Reinforcement Learning\"](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf): This landmark paper introduces Q-learning with convolutional networks for playing Atari games, marking a significant advancement in deep reinforcement learning.\n",
        "- **Van Hasselt, Guez, & Silver (2015)**. [\"Deep Reinforcement Learning with Double Q-Learning\"](https://arxiv.org/abs/1509.06461): This paper presents a very effective trick to improve the performance of deep Q-learning by addressing overestimation issues.\n",
        "- **Lillicrap et al. (2016)**. [\"Continuous Control with Deep Reinforcement Learning\"](https://arxiv.org/abs/1509.02971): Discusses continuous Q-learning with an actor network for approximate maximization, known as DDPG.\n",
        "- **Gu, Lillicrap, & Stuskever (2016)**. [\"Continuous Deep Q-Learning with Model-Based Acceleration\"](https://arxiv.org/abs/1603.00748): This paper explores continuous Q-learning with action-quadratic value functions, enhancing the efficiency of learning in continuous action spaces.\n",
        "- **Wang et al. (2016)**. [\"Dueling Network Architectures for Deep Reinforcement Learning\"](https://arxiv.org/abs/1511.06581): Introduces an architecture that separates value and advantage estimation in the Q-function, improving stability and performance.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxLYbw1wj9wS"
      },
      "source": [
        "# <img src=\"https://img.icons8.com/?size=100&id=46756&format=png&color=000000\" style=\"height:50px;display:inline\"> Credits\n",
        "---\n",
        "* Examples and code snippets were taken from <a href=\"https://github.com/araffin/rlss23-dqn-tutorial/tree/main\"> Reinforcement Learning Summer School 2023 - DQN Tutorial </a>\n",
        "* Examples and explanations were taken from <a href=\"https://rail.eecs.berkeley.edu/deeprlcourse/\">CS285 - Deep Reinforcement Learning Course at UC Berkeley</a>\n",
        "* Icons from <a href=\"https://icons8.com/\">Icons8.com"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "-oid7uNNGKvv",
        "UoXl-zQ5bxxG",
        "r4P79ZolWypl",
        "pbIQ15HYzdLu",
        "eypSYcGfzdLu",
        "S34VKH9x2Fy4",
        "MVqn4Kc8Aomo",
        "sJBNkA_h64g1"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "12px",
        "width": "186px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "410.667px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
